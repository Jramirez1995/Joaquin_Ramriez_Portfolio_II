[
  {
    "objectID": "time-series/practicum1.html",
    "href": "time-series/practicum1.html",
    "title": "Time Series",
    "section": "",
    "text": "In the field of data analysis, a robust understanding of statistical concepts is essential for accurate and insightful interpretation of data. This section provides a comprehensive overview of fundamental statistical principles and methodologies applied during my practicum in the UTSA Master’s program in Data Analytics. The emphasis is on leveraging these principles to analyze diverse datasets using advanced statistical software and tools, including R Studio, GRETL, Python, and SAS Enterprise.\nThe report begins with an exploration of time series analysis, followed by an examination of the Durbin-Watson statistic, and then proceeds with ARIMA modeling. It also includes examples of code used in SAS 94 New Cluster and concludes with a discussion of key statistical concepts.\nVarious techniques were employed, such as Autocorrelation Function (ACF) plots and formal statistical tests including the KPSS and Augmented Dickey-Fuller (ADF) tests.\n\nTime Series Analysis: Wind Speed in Delhi\nIntroduction: The goal of this section is to analyze time series datasets by investigating whether their mean remains constant over time. This analysis involves importing time series data into GRETL , visualizing the data, and applying statistical tests to determine whether the time series exhibits a constant mean. The methods used include visual inspection through plotting and formal statistical testing via the KPSS and Augmented Dickey-Fuller (ADF) tests.\nDataset: Wind Speed in Delhi The first dataset selected for analysis records wind speed (measured in km/h) in Delhi, India, from January 1, 2013, to April 24, 2017.\nTo begin the analysis, the wind speed data was plotted over time. By visually inspecting the plot, it appeared that the mean wind speed remained relatively constant throughout the time period. However, a subtle trend was observable in certain segments of the data, which warranted further investigation.\n\nAn autocorrelation function (ACF) plot was generated to assess the presence of autocorrelation and trends. The ACF plot exhibited a slight decrease in correlation as the lag increased, indicating autocorrelation in the dataset. This trend suggested that the mean might not be perfectly constant across the entire time series.\n\nTo formally test for a constant mean, two statistical tests were applied to the dataset:\n\nKPSS Test: The null hypothesis (Ho) of the KPSS test states that the mean is constant over time. The test statistic for the wind speed dataset was 0.216261, and with a p-value greater than 0.10, we failed to reject the null hypothesis. Thus, the KPSS test suggested that the mean wind speed is constant over time.\n\nT = 1462\n\nLag truncation parameter = 7\n\nTest statistic = 0.216261\n\n10%      5%      1%\n\nCritical values: 0.348   0.462   0.743\n\nP-value \\&gt; .10\n\nAugmented Dickey-Fuller (ADF) Test: The ADF test’s null hypothesis (Ho) posits that the series has a unit root, implying a non-constant mean. The test statistic for the wind speed data was -4.11449, with a p-value of 0.0009163. This result led to the rejection of the null hypothesis, indicating that the mean wind speed was likely constant over time.\n\n    Augmented Dickey-Fuller test for wind_speed\n\n    testing down from 23 lags, criterion AIC\n\n    sample size 1438\n\n    unit-root null hypothesis: a = 1\n\n    test with constant\n\n    including 23 lags of (1-L)wind_speed\n\n    model: (1-L)y = b0 + (a-1)\\*y(-1) + \\... + e\n\n    estimated value of (a - 1): -0.211152\n\n    test statistic: tau_c(1) = -4.11449\n\n    asymptotic p-value 0.0009163\n\n    1st-order autocorrelation coeff. for e: -0.003\n\n    lagged differences: F(23, 1413) = 3.597 \\[0.0000\\]\nIn conclusion, this analysis applied visual inspection, ACF plots, and statistical tests to assess whether the mean of two time series datasets remained constant over time. The results varied across datasets. For the wind speed data, both visual inspection and statistical testing suggested a constant mean over time. In the case of Microsoft stock volume, the KPSS test indicated a non-constant mean, while the ADF test suggested otherwise, pointing to the importance of using multiple tests in time series analysis. This report demonstrates how combining visual methods with formal statistical testing can provide a robust understanding of the characteristics of time series data.\n\n\nTime Series Analysis: Microsoft Stock\nDataset 2: Microsoft Stock- Time Series Analysis\nThis analysis focuses on the stock trading volume for Microsoft from April 1, 2015, to April 1, 2021. The objective was to examine the behavior of the stock volume over time and determine whether its mean remained constant.\nThe initial visual inspection of the Microsoft stock trading volume plot indicated that the mean volume appeared relatively stable throughout the observation period. There were no noticeable upward or downward trends in the data.\n\nTo gain a deeper understanding, we analyzed the autocorrelation function (ACF) plot. The ACF plot demonstrated a gradual decline in autocorrelation as the lag increased, suggesting some level of autocorrelation in the dataset. However, this plot alone did not provide a clear indication of a trend or non-constant mean.\n\nTo gain a more rigorous understanding of whether the mean stock trading volume remained constant over time, we employed two key statistical tests:\nKPSS Test: For the stock volume data, the KPSS test yielded a test statistic of 0.646783 and a p-value of 0.024. Given that the p-value is below the significance threshold of 0.05, we reject the null hypothesis. This result indicates that the mean stock volume is not constant over the observed period.\nT = 1511\n\nLag truncation parameter = 7\n\nTest statistic = 0.646783\n\n10%      5%      1%\n\nCritical values: 0.348   0.462   0.743\n\nInterpolated p-value 0.024\nFollowing the KPSS test, we also applied the Augmented Dickey-Fuller (ADF) test to further investigate the stability of the stock trading volume:\nAugmented Dickey-Fuller (ADF) Test: The ADF test produced a test statistic of -6.89966 and a p-value of 6.158e-10. Since the p-value is significantly below the significance level, we reject the null hypothesis of a non-constant mean. This result supports the conclusion that the mean trading volume was indeed constant over the observed period.\nAugmented Dickey-Fuller test for Volume\n\ntesting down from 23 lags, criterion AIC\n\nsample size 1502\n\nunit-root null hypothesis: a = 1\n\ntest with constant\n\nincluding 8 lags of (1-L)Volume\n\nmodel: (1-L)y = b0 + (a-1)\\*y(-1) + \\... + e\n\nestimated value of (a - 1): -0.185615\n\ntest statistic: tau_c(1) = -6.89966\n\nasymptotic p-value 6.158e-10\n\n1st-order autocorrelation coeff. for e: -0.001\n\nlagged differences: F(8, 1492) = 13.263 \\[0.0000\\]\nIn conclusion this analysis applied visual inspection, ACF plots, and statistical tests to assess whether the mean of two time series datasets remained constant over time. The results varied across datasets. For the wind speed data, both visual inspection and statistical testing suggested a constant mean over time. In the case of Microsoft stock volume, the KPSS test indicated a non-constant mean, while the ADF test suggested otherwise, pointing to the importance of using multiple tests in time series analysis. This report demonstrates how combining visual methods with formal statistical testing can provide a robust understanding of the characteristics of time series data.\n\n\nTime Series Data Analysis Report 2: Forecasting from Gold Price\nObjective:\nThe aim of this analysis is to apply time series forecasting techniques to historical gold price data. By employing various statistical tests and models, we seek to predict future gold prices and assess the stationarity of the dataset.\nIntroduction:\nGold has long been a valuable commodity, widely used in jewelry and as an investment asset. Prices for gold fluctuate daily due to a variety of economic factors and are actively traded on global commodity exchanges. This project uses historical gold price data to forecast future prices, applying time series forecasting methods to gain insights into market trends.\nDataset:\nFor this analysis, we utilized a dataset from Quandl, which provides extensive financial and economic data. This dataset includes daily gold prices spanning several decades, beginning in the 1970s. You can access the dataset here: Gold Price Time Series\nData Visualization:\nThe first step in our analysis involved plotting the raw time series data of gold prices. The resulting plot shows significant fluctuations over time, suggesting a non-constant mean and variance. These visual indications imply that the time series may be non-stationary.\n\nTo further investigate, we examined the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) to better understand the underlying characteristics of the time series. The ACF plot suggest that no consistent pattern across the data, suggesting a non-constant mean. The lack of clear trends in the ACF supports our earlier conclusion of non-stationarity.\n\nTo confirm the stationarity of the dataset, we conducted two statistical tests, the KPSS test and the Augmented Dickey-Fuller (ADF) test.\n\nKPSS test for Value:\n\nT = 10787\n\nLag truncation parameter = 12\n\nTest statistic = 55.3031\n\n10%      5%      1%\n\nCritical values: 0.347   0.462   0.744\n\nP-value \\&lt; .01\nH0:  no evidence data has more than one mean (fits stationary criteria)\nHa: data has more than one mean (e.g. non-stationary)\nBecause p &lt; .05, we reject H0 and so there is evidence of more than one mean. (It is non-stationary)\n\nAugmented Dickey-Fuller test for Value\n\ntesting down from 38 lags, criterion AIC\n\nsample size 10754\n\nunit-root null hypothesis: a = 1\n\ntest with constant\n\nincluding 32 lags of (1-L)Value\n\nmodel: (1-L)y = b0 + (a-1)\\*y(-1) + \\... + e\n\nestimated value of (a - 1): -3.11791e-005\n\ntest statistic: tau_c(1) = -0.155837\n\nasymptotic p-value 0.9415\n\n1st-order autocorrelation coeff. for e: 0.000\n\nlagged differences: F(32, 10720) = 4.775 \\[0.0000\\]\nH0: data has more than one mean (e.g. non-stationary)\nHa: data has one mean (e.g. stationary)\nSince the p-value is greater than 0.05, we do not reject the null hypothesis, which suggests that the data is non-stationary. Both tests confirm that the gold price dataset exhibits non-stationarity.\nTo address this, we applied differencing to the time series to remove the trend and stabilize the mean. The plot of the differenced data is shown below:\n\nAfter differencing, the data appears more stationary, with the trend and non-constant mean effectively removed. The ACF and PACF plots further indicate that differencing was successful in stabilizing the series, suggesting that the data is now stationary.\n\nTo validate the stationarity of the differenced data, we applied the KPSS and ADF tests again.\n\nKPSS Test Results on Differenced Data\n\nT = 10786\n\nLag truncation parameter = 12\n\nTest statistic = 0.136227\n\n                   10%      5%      1%\n\nCritical values: 0.347   0.462   0.744\n\nP-value \\&gt; .10\nH0:  no evidence data has more than one mean (fits stationary criteria)\nHa: data has more than one mean (e.g. non-stationary)\nSince the p-value is greater than 0.05, we fail to reject the null hypothesis, indicating that the differenced data is stationary.\n\nAugmented Dickey-Fuller test for Differenced Data\n\ntesting down from 38 lags, criterion AIC\n\nsample size 10747\n\nunit-root null hypothesis: a = 1\n\ntest with constant\n\nincluding 38 lags of (1-L)d_Value\n\nmodel: (1-L)y = b0 + (a-1)\\*y(-1) + \\... + e\n\nestimated value of (a - 1): -1.11688\n\ntest statistic: tau_c(1) = -17.5688\n\nasymptotic p-value 3.402e-42\n\n1st-order autocorrelation coeff. for e: 0.000\n\nlagged differences: F(38, 10707) = 4.529 \\[0.0000\\]\nH0: data has more than one mean (e.g. non-stationary)\nHa: data has one mean (e.g. stationary)\nGiven the extremely low p-values from both the KPSS and ADF tests, we reject the null hypothesis (H₀). This confirms that the differenced data is stationary. Consequently, we will proceed with the differenced time series data for further analysis.\nNext, we examined the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots to assess the presence of autoregressive (AR) and moving average (MA) processes. The ACF and PACF plots revealed both negative and positive lags in the data, indicating the presence of autoregressive and moving average components.\nAdditional Resources: Duke University & Penn State\n\nWe then experimented with various ARIMA models and evaluated their performance based on several metrics. The results are summarized in the table below:\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nAdjusted R-squared\nAIC\nBIC (Schwartz)\nHannan-Quinn\nLjung-Box\nARCH p-value\n\n\n\n\n(1,0,1)\n0.0002\n78091.04\n78120.19\n78100.87\n0.001281\n6.90E-211\n\n\n(2,0,1)\n0.0016\n78081.13\n78117.56\n78093.41\n0.02874\n2.30E-209\n\n\n(1,0,2)\n0.001205\n78081.19\n78117.62\n78093.48\n0.02738\n2.39E-209\n\n\n(1,0,3)\n0.001158\n78082.70\n78097.45\n78126.42\n0.09325\n1.36E-207\n\n\n(2,0,2)\n0.004335\n78048.39\n78092.11\n78063.13\n0.1218\n1.96E-201\n\n\n\nAfter reviewing the performance metrics, I selected the (2,0,2) model as the best fit for the data. This model has the highest adjusted R-squared value and the lowest AIC and BIC values, indicating better overall model fit.\nWe plotted the observed versus fitted data for the selected time series models to evaluate their performance:\n\nUsing the selected (2,0,2) model, we forecasted gold prices for six periods ahead. The forecast aligns well with historical data, although further validation over a longer horizon is recommended.\n\nVariance Testing:\nThe ARCH test results indicate significant non-constant variance in the selected model. Despite this, no immediate corrective measures are planned.\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nAdjusted R-squared\nAIC\nBIC (Schwartz)\nHannan-Quinn\nLjung-Box\nARCH p-value\n\n\n\n\n(1,0,1)\n0.0002\n78091.04\n78120.19\n78100.87\n0.001281\n6.90E-211\n\n\n(2,0,1)\n0.0016\n78081.13\n78117.56\n78093.41\n0.02874\n2.30E-209\n\n\n(1,0,2)\n0.001205\n78081.19\n78117.62\n78093.48\n0.02738\n2.39E-209\n\n\n(1,0,3)\n0.001158\n78082.70\n78097.45\n78126.42\n0.09325\n1.36E-207\n\n\n(2,0,2)\n0.004335\n78048.39\n78092.11\n78063.13\n0.1218\n1.96E-201\n\n\n\nOverall, the analysis confirms the stationarity of the differenced gold price data and identifies the (2,0,2) ARIMA model as the most suitable for forecasting, despite some issues with variance constancy.\n\n\nDurbin Watson Test on Time Series Data\nIntroduction:\nThis report presents the application of the Durbin-Watson test to detect autocorrelation in time series data, specifically using multiple regression on retail datasets. The objective is to determine whether the regression residuals display autocorrelation, which would violate a key assumption of linear regression: that the error terms are independent.\nAutocorrelation in residuals can lead to biased and inefficient estimates of regression coefficients, as well as incorrect inferences, such as inflated t-statistics and misleading confidence intervals. The Durbin-Watson test statistic is employed to assess the presence of autocorrelation.\nTime Series Examination of Retail 1\n\n\nInitial visual inspection of the Retail 1 data, using the “Mark I Eyeball” method, indicates a constant mean with no significant trend. This observation is promising for the assumption of uncorrelated error terms.\nFurther analysis with the autocorrelation function (ACF) plot shows some minor autocorrelation but not severe enough to cause major concerns.\nDurbin-Watson Test Results for Retail 1:\nFor a sample size of T = 49, the Durbin-Watson test boundaries at α = 0.05 are:\n\nLower Bound (DL): 1.141\nUpper Bound (DU): 1.499\n\nTo further investigate, I ran an Ordinary Least Squares (OLS) regression on the Retail 1 data using GRETL. Below are the key metrics from the regression:\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nModel\nHigh-Precision OLS, using observations (T = 49)\n\n\nDependent Variable\nSales\n\n\nCoefficient (const)\n1.2226 x 10^4\n\n\nStd. Error (const)\n829.31\n\n\nMean Dependent Var\n1.2226 x 10^4\n\n\nS.D. Dependent Var\n5.8052 x 10^3\n\n\nSum Squared Resid\n1.6176 x 10^9\n\n\nS.E. of Regression\n5.8052 x 10^3\n\n\nR-squared\n-5.1817 x 10^-77\n\n\nAdjusted R-squared\n-5.1817 x 10^-77\n\n\nF(1, 48)\n217.34\n\n\nP-value(F)\n1.9018 x 10^-19\n\n\nLog-Likelihood\n-493.68\n\n\nAkaike Criterion\n989.36\n\n\nSchwarz Criterion\n991.26\n\n\nHannan-Quinn\n990.08\n\n\nRho\n-0.0302\n\n\nDurbin-Watson\n2.049\n\n\n\nThe Durbin-Watson statistic of 2.049 exceeds the upper bound (DU), indicating no significant autocorrelation. Thus, the residuals of Retail 1 do not exhibit significant autocorrelation, validating the regression results under the assumption of independent errors.\nTime Series Examination of Retail 2 Dataset\nFor Retail 2, visual inspection also shows a constant mean and no significant trend. The ACF plot indicates minor autocorrelation, but it is not severe enough to challenge the assumption of uncorrelated error terms.\n\n\nDurbin-Watson Test Results for Retail 2:\nFor a sample size of T = 39, the Durbin-Watson test boundaries at α = 0.05 are:\n\nLower Bound (DL): 1.376\nUpper Bound (DU): 1.604\n\nRunning an OLS regression on the Retail 2 data in GRETL provides the following results:\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nModel\nHigh-Precision OLS, using observations 2011-03-15:2011-04-22 (T = 39)\n\n\nDependent Variable\nSales\n\n\nCoefficient (const)\n1.2849 x 10^4\n\n\nStd. Error (const)\n901.07\n\n\nMean Dependent Var\n1.2849 x 10^4\n\n\nS.D. Dependent Var\n5.6272 x 10^3\n\n\nSum Squared Resid\n1.2033 x 10^9\n\n\nS.E. of Regression\n5.6272 x 10^3\n\n\nR-squared\n-6.9089 x 10^-77\n\n\nAdjusted R-squared\n-6.9089 x 10^-77\n\n\nF(1, 38)\n203.35\n\n\nP-value(F)\n7.7600 x 10^-17\n\n\nLog-Likelihood\n-391.61\n\n\nAkaike Criterion\n785.22\n\n\nSchwarz Criterion\n786.89\n\n\nHannan-Quinn\n785.82\n\n\nRho\n0.1236\n\n\nDurbin-Watson\n1.737\n\n\n\nThe Durbin-Watson statistic of 1.737 is between the lower bound (DL) and upper bound (DU), indicating no significant autocorrelation. Thus, the residuals of Retail 2 also do not exhibit significant autocorrelation, validating the regression results under the assumption of independent errors.\nSummary:\nThe Durbin-Watson test results for both Retail 1 and Retail 2 datasets suggest that autocorrelation is not a significant issue. The residuals in the regression models are uncorrelated, which supports the validity of the regression results. The assumption of independent errors is upheld, ensuring the reliability of the models used.\n\n\nARIMA Model Comparison\nThis report analyzes the unemployment rate in the US from January 1948 to March 2020. The analysis involves plotting the time series data, differencing to achieve stationarity, and fitting various ARIMA models to the differenced data. The models are evaluated using several metrics, including AIC, BIC, and the Ljung-Box test.\nTime Series Plot: The plot below shows the unemployment rate from January 1948 to March 2020.\n\nDifferenced Data Plot: The first differenced data is used to achieve stationarity, as illustrated in the plot below.\n\n________________________________________________________________________________________\nARIMA Model Analysis\nModel 1: ARMA(1,1)\n\nObservations: 1948:02-2020:03 (T = 866)\nDependent Variable: d_UNRATE\nStandard Errors: Based on Hessian\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz-value\np-value\nSignificance\n\n\n\n\nconst\n0.002873\n0.014791\n0.1942\n0.8460\n\n\n\nphi_1\n0.870665\n0.029667\n29.3500\n&lt;0.0001\n***\n\n\ntheta_1\n-0.718031\n0.037947\n-18.9200\n&lt;0.0001\n***\n\n\n\nModel Fit Statistics:\n\n\n\nStatistic\nValue\n\n\n\n\nMean dependent var\n0.001155\n\n\nS.D. dependent var\n0.209924\n\n\nMean of innovations\n-0.000378\n\n\nS.D. of innovations\n0.200521\n\n\nR-squared\n0.086522\n\n\nAdjusted R-squared\n0.085465\n\n\nLog-likelihood\n162.6270\n\n\nAkaike criterion\n-317.2540\n\n\nSchwarz criterion\n-298.1985\n\n\nHannan-Quinn\n-309.9612\n\n\n\nRoots of AR and MA Polynomials:\n\n\n\nType\nRoot\nReal\nImaginary\nModulus\nFrequency\n\n\n\n\nAR\nRoot 1\n1.1485\n0.0000\n1.1485\n0.0000\n\n\nMA\nRoot 1\n1.3927\n0.0000\n1.3927\n0.0000\n\n\n\n________________________________________________________________________________________\nAutocorrelation Test:\n\nLjung-Box Q’ = 75.3636\np-value = 4.042e-012\n\nModel 2: ARMA(2,1)\n\nObservations: 1948:02-2020:03 (T = 866)\nDependent Variable: d_UNRATE\nStandard Errors: Based on Hessian\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz-value\np-value\nSignificance\n\n\n\n\nconst\n0.002986\n0.014898\n0.2004\n0.8412\n\n\n\nphi_1\n0.555245\n0.062518\n8.881\n&lt;0.0001\n***\n\n\nphi_2\n0.238727\n0.037380\n6.386\n&lt;0.0001\n***\n\n\ntheta_1\n-0.538385\n0.058356\n-9.226\n&lt;0.0001\n***\n\n\n\nModel Fit Statistics:\n\n\n\nStatistic\nValue\n\n\n\n\nMean dependent var\n0.001155\n\n\nS.D. dependent var\n0.209924\n\n\nMean of innovations\n-0.000420\n\n\nS.D. of innovations\n0.196462\n\n\nR-squared\n0.123133\n\n\nAdjusted R-squared\n0.121101\n\n\nLog-likelihood\n180.2785\n\n\nAkaike criterion\n-350.5570\n\n\nSchwarz criterion\n-326.7375\n\n\nHannan-Quinn\n-341.4410\n\n\n\nRoots of AR and MA Polynomials:\n\n\n\nType\nRoot\nReal\nImaginary\nModulus\nFrequency\n\n\n\n\nAR\nRoot 1\n1.1911\n0.0000\n1.1911\n0.0000\n\n\nAR\nRoot 2\n-3.5169\n0.0000\n3.5169\n0.5000\n\n\nMA\nRoot 1\n1.8574\n0.0000\n1.8574\n0.0000\n\n\n\n________________________________________________________________________________________\nAutocorrelation Test:\n\nLjung-Box Q’ = 36.8101\np-value = 2.845e-005\n\nModel 3: ARMA(2,2)\n\nObservations: 1948:02-2020:03 (T = 866)\nDependent Variable: d_UNRATE\nStandard Errors: Based on Hessian\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz-value\np-value\nSignificance\n\n\n\n\nconst\n0.002579\n0.011520\n0.2239\n0.8228\n\n\n\nphi_1\n1.655610\n0.037484\n44.1700\n&lt;0.0001\n***\n\n\nphi_2\n-0.782771\n0.043359\n-18.0500\n&lt;0.0001\n***\n\n\ntheta_1\n-1.641770\n0.038375\n-42.7800\n&lt;0.0001\n***\n\n\ntheta_2\n0.863215\n0.047917\n18.0100\n&lt;0.0001\n***\n\n\n\nModel Fit Statistics:\n\n\n\nStatistic\nValue\n\n\n\n\nMean dependent var\n0.001155\n\n\nS.D. dependent var\n0.209924\n\n\nMean of innovations\n-0.000443\n\n\nS.D. of innovations\n0.194870\n\n\nR-squared\n0.137289\n\n\nAdjusted R-squared\n0.134286\n\n\nLog-likelihood\n187.0535\n\n\nAkaike criterion\n-362.1069\n\n\nSchwarz criterion\n-333.5236\n\n\nHannan-Quinn\n-351.1678\n\n\n\nRoots of AR and MA Polynomials:\n\n\n\nType\nRoot\nReal\nImaginary\nModulus\nFrequency\n\n\n\n\nAR\nRoot 1\n1.0575\n-0.3989\n1.1303\n-0.0574\n\n\nAR\nRoot 2\n1.0575\n0.3989\n1.1303\n0.0574\n\n\nMA\nRoot 1\n0.9510\n-0.5041\n1.0763\n-0.0776\n\n\nMA\nRoot 2\n0.9510\n0.5041\n1.0763\n0.0776\n\n\n\n________________________________________________________________________________________\nAutocorrelation Test:\n\nLjung-Box Q’ = 39.2977\np-value = 4.328e-006\n\nModel 4: ARMA(4,5)\n\nObservations: 1948:02-2020:03 (T = 866)\nDependent Variable: d_UNRATE\nStandard Errors: Based on Hessian\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient\nEstimate\nStd. Error\nz-value\np-value\nSignificance\n\n\n\n\nconst\n0.002507\n0.011390\n0.2201\n0.8258\n\n\n\nphi_1\n0.578072\n0.062491\n9.250\n&lt;0.0001\n***\n\n\nphi_2\n0.117027\n0.073948\n1.583\n0.1135\n\n\n\nphi_3\n0.611279\n0.108845\n5.616\n&lt;0.0001\n***\n\n\nphi_4\n-0.695650\n0.055781\n-12.4700\n&lt;0.0001\n***\n\n\ntheta_1\n-0.585967\n0.067105\n-8.732\n&lt;0.0001\n***\n\n\ntheta_2\n0.063179\n0.074000\n0.8538\n0.3932\n\n\n\ntheta_3\n-0.595233\n0.107839\n-5.520\n&lt;0.0001\n***\n\n\ntheta_4\n0.766918\n0.069361\n11.060\n&lt;0.0001\n***\n\n\ntheta_5\n0.030504\n0.070963\n0.4299\n0.6673\n\n\n\n\nModel Fit Statistics:\n\n\n\nStatistic\nValue\n\n\n\n\nMean dependent var\n0.001155\n\n\nS.D. dependent var\n0.209924\n\n\nMean of innovations\n-0.000422\n\n\nS.D. of innovations\n0.192210\n\n\nR-squared\n0.160680\n\n\nAdjusted R-squared\n0.152845\n\n\nLog-likelihood\n198.7941\n\n\nAkaike criterion\n-375.5881\n\n\nSchwarz criterion\n-323.1854\n\n\nHannan-Quinn\n-355.5330\n\n\n\nRoots of AR and MA Polynomials:\n\n\n\nType\nRoot\nReal\nImaginary\nModulus\nFrequency\n\n\n\n\nAR\nRoot 1\n1.0508\n0.4052\n1.1262\n0.0586\n\n\nAR\nRoot 2\n1.0508\n-0.4052\n1.1262\n-0.0586\n\n\nAR\nRoot 3\n-0.6114\n-0.8715\n1.0646\n-0.3474\n\n\nAR\nRoot 4\n-0.6114\n0.8715\n1.0646\n0.3474\n\n\nMA\nRoot 1\n0.9450\n0.5028\n1.0704\n0.0778\n\n\nMA\nRoot 2\n0.9450\n-0.5028\n1.0704\n-0.0778\n\n\n\nAutocorrelation Test:\nTest for autocorrelation up to order 12:\n\nLjung-Box Q’ = 41.5484\np-value = P(Chi-square(10) &gt; 41.5484) = 1.741e-007\n\nARIMA Model Analysis Report\nMetrics Overview:\nThe following criteria are used to evaluate the goodness of fit for ARIMA models:\n\nAkaike Information Criterion (AIC): AIC measures the relative fit of a model while penalizing for the number of parameters. Lower values indicate a better model fit.\nBayesian Information Criterion (BIC): Also known as the Schwarz criterion, BIC is similar to AIC but imposes a stricter penalty for additional parameters. It favors simpler models, with lower values indicating a better fit.\nHannan-Quinn Information Criterion (HQIC): HQIC provides a balance between AIC and BIC in terms of penalizing model complexity. It offers a measure of fit with a focus on both model accuracy and simplicity.\n\nBIC and HQIC are more conservative compared to AIC, as they impose higher penalties for additional parameters. Thus, smaller values of these metrics suggest a better model fit, with a preference for models that are more parsimonious.\nLjung-Box Q Statistic:\nThe Ljung-Box Q test assesses autocorrelation in the residuals of a model.\n\nNull Hypothesis (H0): There is no serial autocorrelation in the residuals.\nAlternative Hypothesis (Ha): There is serial autocorrelation in the residuals.\n\nA high p-value (greater than 0.05) suggests that the residuals are not significantly autocorrelated, indicating a good model fit. A low p-value indicates potential autocorrelation in the residuals, which suggests that the model may not fully capture the underlying data structure.\nARIMA Model Comparison\nBelow is a summary of the evaluated ARIMA models, including their adjusted R-squared, AIC, BIC, and Ljung-Box Q values:\n\n\n\nModel\nAdjusted R-squared\nAIC\nBIC\nLjung-Box Q\n\n\n\n\n1\n0.085465\n-317.254\n-298.199\n75.3636\n\n\n2\n0.121101\n-350.557\n-326.738\n36.8101\n\n\n3\n0.134286\n-362.107\n-333.524\n39.2977\n\n\n4\n0.152845\n-375.588\n-323.185\n17.9674\n\n\n\nBest Model:\nModel 4 is identified as the best among the evaluated models. It has the highest adjusted R-squared (0.1528), the lowest AIC (-375.588), and the third lowest BIC (-323.185). Despite the Ljung-Box Q statistic indicating some residual autocorrelation, Model 4 performs best overall in terms of fit statistics.\nResidual Variance and Model Exploration\nThe Ljung-Box test results suggest that some autocorrelation remains in the residuals of the models. The p-values for the Ljung-Box test are below 0.05, indicating that the residuals are not completely free of autocorrelation. This implies that additional variance might be captured by exploring further ARIMA models or adjusting model parameters to improve fit and reduce residual autocorrelation.\n\n\nSAS Code: ARIMA MODELS\n\n# LIBNAME mylib \"P:\\\";\n# FILENAME bigrec \"P:\\fa15_data.txt\" LRECL = 65576;\n# DATA mytemp;\n# INFILE bigrec;\n# INPUT\n# myid 1-7\n# purchase_online_safe_aglo 5526\n# purchase_online_safe_agli 5564\n# purchase_online_safe_neit 5640\n# purchase_online_safe_dgli 5678\n# purchase_online_safe_dglo 5716\n\n# buy_online_aglo 5518\n# buy_online_agli 5556\n# buy_online_neit 5632\n# buy_online_dgli 5670\n# buy_online_dglo 5708\n\n# use_devices_for_deal_aglo 5508\n# use_devices_for_deal_agli 5546\n# use_devices_for_deal_neit 5622\n# use_devices_for_deal_dgli 5660\n# use_devices_for_deal_dglo 5698\n\n# hear_products_email_aglo 5525\n# hear_products_email_agli 5563\n# hear_products_email_neit 5639\n# hear_products_email_dgli 5677\n# hear_products_email_dglo 5715\n\n# internet_chnge_shop_aglo 5495\n# internet_chnge_shop_agli 5533\n# internet_chnge_shop_neit 5609\n# internet_chnge_shop_dgli 5647\n# internet_chnge_shop_dglo 5685\n\n# environ_friendly_aglo 4181\n# environ_friendly_agli 4195\n# environ_friendly_neit 4223\n# environ_friendly_dgli 4237\n# environ_friendly_dglo 4251\n\n# recycle_prods_aglo 4190\n# recycle_prods_agli 4204\n# recycle_prods_neit 4232\n# recycle_prods_dgli 4246\n\n# recycle_prods_dglo 4260\n# environ_good_business_aglo 4182\n# environ_good_business_agli 4196\n# environ_good_business_neit 4224\n# environ_good_business_dgli 4238\n# environ_good_business_dglo 4252\n\n# environ_personal_ob_aglo 4184\n# environ_personal_ob_agli 4198\n# environ_personal_ob_neit 4226\n# environ_personal_ob_dgli 4240\n# environ_personal_ob_dglo 4254\n\n# comp_help_cons_env_aglo 4183\n# comp_help_cons_env_agli 4197\n# comp_help_cons_env_neit 4225\n# comp_help_cons_env_dgli 4239\n# comp_help_cons_env_dglo 4253\n\n# adidas_brand 42607;\n# RUN;\n\n# \\* the above reads in the raw data from the data file - now create five point scale variables \\*/\n# \\* now before we create variables lets create formats so we know what each value will mean \\*/\n\n# PROC FORMAT;\n# VALUE myscale\n\n#   1 = \"disagree a lot\"\n#   2 = \"disagree a little\"\n#   3 = \"neither agree nor disagree\"\n#   4 = \"agree a little\"\n#   5 = \"agree a lot\";\n\n# VALUE yesno\n\n#   0 = \"no\"\n#   1 = \"yes\";\n\n# RUN;\n\n# \\* do that by creating a new temp sas data set myvars by starting with the temp sas data set mytemp \\*/\n\n# DATA myvars;\n# SET mytemp;\n\n# IF purchase_online_safe_dglo = 1 THEN purchase_online_safe = 1;\n# IF purchase_online_safe_dgli = 1 THEN purchase_online_safe = 2;\n# IF purchase_online_safe_neit = 1 THEN purchase_online_safe = 3;\n# IF purchase_online_safe_agli = 1 THEN purchase_online_safe = 4;\n# IF purchase_online_safe_aglo = 1 THEN purchase_online_safe = 5;\n\n# IF buy_online_dglo = 1 THEN buy_online = 1;\n# IF buy_online_dgli = 1 THEN buy_online = 2;\n# IF buy_online_neit = 1 THEN buy_online = 3;\n# IF buy_online_agli = 1 THEN buy_online = 4;\n# IF buy_online_aglo = 1 THEN buy_online = 5;\n\n# IF use_devices_for_deal_dglo = 1 THEN use_devices_for_deal = 1;\n# IF use_devices_for_deal_dgli = 1 THEN use_devices_for_deal = 2;\n# IF use_devices_for_deal_neit = 1 THEN use_devices_for_deal = 3;\n# IF use_devices_for_deal_agli = 1 THEN use_devices_for_deal = 4;\n# IF use_devices_for_deal_aglo = 1 THEN use_devices_for_deal = 5;\n\n# IF hear_products_email_dglo = 1 THEN hear_products_email = 1;\n# IF hear_products_email_dgli = 1 THEN hear_products_email = 2;\n# IF hear_products_email_neit = 1 THEN hear_products_email = 3;\n# IF hear_products_email_agli = 1 THEN hear_products_email = 4;\n# IF hear_products_email_aglo = 1 THEN hear_products_email = 5;\n\n# IF internet_chnge_shop_dglo = 1 THEN internet_chnge_shop = 1;\n# IF internet_chnge_shop_dgli = 1 THEN internet_chnge_shop = 2;\n# IF internet_chnge_shop_neit = 1 THEN internet_chnge_shop = 3;\n# IF internet_chnge_shop_agli = 1 THEN internet_chnge_shop = 4;\n# IF internet_chnge_shop_aglo = 1 THEN internet_chnge_shop = 5;\n\n# IF environ_friendly_dglo = 1 THEN environ_friendly = 1;\n# IF environ_friendly_dgli = 1 THEN environ_friendly = 2;\n# IF environ_friendly_neit = 1 THEN environ_friendly = 3;\n# IF environ_friendly_agli = 1 THEN environ_friendly = 4;\n# IF environ_friendly_aglo = 1 THEN environ_friendly = 5;\n\n# IF recycle_prods_dglo = 1 THEN recycle_prods = 1;\n# IF recycle_prods_dgli = 1 THEN recycle_prods = 2;\n# IF recycle_prods_neit = 1 THEN recycle_prods = 3;\n# IF recycle_prods_agli = 1 THEN recycle_prods = 4;\n# IF recycle_prods_aglo = 1 THEN recycle_prods = 5;\n\n# IF environ_good_business_dglo = 1 THEN environ_good_business = 1;\n# IF environ_good_business_dgli = 1 THEN environ_good_business = 2;\n# IF environ_good_business_neit = 1 THEN environ_good_business = 3;\n# IF environ_good_business_agli = 1 THEN environ_good_business = 4;\n# IF environ_good_business_aglo = 1 THEN environ_good_business = 5;\n\n# IF environ_personal_ob_dglo = 1 THEN environ_personal_ob = 1;\n# IF environ_personal_ob_dgli = 1 THEN environ_personal_ob = 2;\n# IF environ_personal_ob_neit = 1 THEN environ_personal_ob = 3;\n# IF environ_personal_ob_agli = 1 THEN environ_personal_ob = 4;\n# IF environ_personal_ob_aglo = 1 THEN environ_personal_ob = 5;\n\n# IF comp_help_cons_env_dglo = 1 THEN comp_help_cons_env = 1;\n# IF comp_help_cons_env_dgli = 1 THEN comp_help_cons_env = 2;\n# IF comp_help_cons_env_neit = 1 THEN comp_help_cons_env = 3;\n# IF comp_help_cons_env_agli = 1 THEN comp_help_cons_env = 4;\n# IF comp_help_cons_env_aglo = 1 THEN comp_help_cons_env = 5;\n\n# IF adidas_brand = . THEN adidas = 0;\n# IF adidas_brand = 1 THEN adidas = 1;\n\n# FORMAT purchase_online_safe buy_online use_devices_for_deal hear_products_email internet_chnge_shop environ_friendly recycle_prods environ_good_business environ_personal_ob comp_help_cons_env myscale.\n\n#   adidas yesno. ;\n# RUN;\n\n# PROC FACTOR DATA = myvars\n# MAXITER=100\n# METHOD=principal\n# MINEIGEN=1\n# ROTATE=varimax\n# MSA\n# SCREE\n# SCORE\n# PRINT\n# NFACTORS=2\n# OUT=myscores;\n# VAR purchase_online_safe\n# buy_online\n# use_devices_for_deal\n# hear_products_email\n# internet_chnge_shop\n# environ_friendly\n# recycle_prods\n# environ_good_business\n# environ_personal_ob\n# comp_help_cons_env;\n# RUN;\n\n# DATA myscores1;\n# SET myscores;\n# RENAME factor1 = onlineshopper;\n# RENAME factor2 = environconscious;\n# RENAME my_id = resp_id;\n\n# RUN;\n\n\n\nBasic Statistics: Levels of Measurement, and Statistical Techniques\nDependent Variable: A variable that depends on the changes made to the independent variable.\nIndependent Variable: The variable that you manipulate or control to observe its effect on the dependent variable.\nControl Variable: A variable that may affect the dependent variable but is not of primary interest in the study.\nLevels of Measurement\n\nNominal: Categorical data with distinct groups, without any order or ranking.\n\n\nExample: Eye color (brown, black, blue).\n\n\nOrdinal: Categorical data that can be ordered or ranked.\n\n\nExample: Education levels (high school diploma, bachelor’s, master’s, Ph.D.).\n\n\nInterval: Data with equal intervals between values, but no true zero. Example: Temperature in Celsius. Ratio: Data with equal intervals and an absolute zero point.\n\n\nExample: Height.\n\nCriteria for Choosing Statistical Techniques\nAccording to the IDRE Chart, two important criteria for selecting a statistical technique are:\n\nThe levels of measurement of both the dependent and independent variables.\nThe number of dependent and independent variables.\n\nAdditionally, the nature of the research question plays a crucial role in determining the appropriate technique.\nDescriptive vs. Inferential Statistics\n\nDescriptive Statistics: Summarize and present the main features of the data.\nInferential Statistics: Make predictions and draw conclusions based on the data.\n\nAssumptions in Statistical Tests\n\nBenefit: Proper assumptions ensure reliable and trustworthy results.\nCost: Violating assumptions can compromise the validity and credibility of the test.\n\nImpact of Violating Assumptions\nViolating the assumptions of a statistical test can undermine the integrity of the analysis, leading to questionable results and a loss of credibility.\nSelecting Appropriate Statistical Tests\n\nColored Contact Lenses and Gender:\n\n\nVariables: Color of lenses (independent), gender (dependent).\nTest: Chi-square test (both variables are nominal).\n\n\nArt Auction Prices by Gender:\n\n\nVariables: Gender (independent), price paid for art (dependent).\nTest: t-test (price is continuous).\n\n\nVegematic Sales by Product and Demographics:\n\n\nVariables: Product color, price, region, gender, household income (independent), number of products sold (dependent).\nTest: ANOVA.\n\n\nMagazine Pages and Sales:\n\n\nVariables: Number of pages (independent), copies sold (dependent).\nTest: Pearson correlation.\n\n\nHair Growth in Cats by Drug, Gender, and Coat Color:\n\n\nVariables: Drug, cat gender, coat color (independent), hair growth (dependent).\nTest: Three-way ANOVA.\n\nOther Key Statical Concepts:\nPopulation and Sample\n\nPopulation: The entire set or group being studied.\nSample: A subset of the population used for the study.\nInferential Statistics: The goal is to make predictions about the population based on the sample.\n\nMeasures of Central Tendency\n\nMean: The average of all data points.\nMedian: The middle value when data is arranged in order.\nExample: In an exam, the mean tells you the overall class performance, while the median can indicate the typical student’s performance.\n\nVariance and Standard Deviation\n\nVariance: Measures how much data points deviate from the mean.\nStandard Deviation: The square root of the variance, indicating the spread of data around the mean.\n\nWhen data points are tightly packed around the mean, variance and standard deviation are lower.\nConfidence Intervals\n\nConfidence Interval: Represents the range in which a population parameter is expected to fall a certain percentage of the time.\nFor example, in a survey of 150 voters where 45% support a candidate, a 95% confidence interval around this proportion ranges from ~0.3704 to 0.5296.\n\nZ-Scores\n\nZ-Score: Measures how many standard deviations a data point is from the mean.\nExample: For UTSA student ages with a mean of 26 and a standard deviation of 4, the Z-score for a student aged 24 is calculated as:\n\n\\[ Z = \\frac{24 - 26}{4} = -0.5 \\]"
  },
  {
    "objectID": "tidytuesday-exercise/readme.html",
    "href": "tidytuesday-exercise/readme.html",
    "title": "2024 Data",
    "section": "",
    "text": "2024 Data\nArchive of datasets and articles from the 2024 series of #TidyTuesday events.\n\n\n\nWeek\nDate\nData\nSource\nArticle\n\n\n\n\n1\n2024-01-02\nBring your own data from 2023!\n\n\n\n\n2\n2024-01-09\nCanadian NHL Player Birth Dates\nStatistics Canada, NHL team list endpoint, NHL API\nAre Birth Dates Still Destiny for Canadian NHL Players?\n\n\n3\n2024-01-16\nUS Polling Places 2012-2020\nCenter for Public Integrity\nNational data release sheds light on past polling place changes\n\n\n4\n2024-01-23\nEducational attainment of young people in English towns\nThe UK Office for National Statistics\nWhy do children and young people in smaller towns do better academically than those in larger towns?\n\n\n5\n2024-01-30\nGroundhog predictions\nGroundhog-day.com API\nGroundhog-day.com Predictions by Year\n\n\n6\n2024-02-06\nWorld heritage sites\nUNESCO World Heritage Sites\n1 dataset 100 visualizations\n\n\n7\n2024-02-13\nValentine’s Day consumer data\nValentine’s Days consumer survey data\nNational Retail Federation Valentine’s Day Data Center\n\n\n8\n2024-02-20\nR Consortium ISC Grants\nR Consortium ISC Funded Projects\nR Consortium ISC Call for Proposals 2024\n\n\n9\n2024-02-27\nLeap Day\nWikipedia: February 29\nWikipedia: February 29\n\n\n10\n2024-03-05\nTrash Wheel Collection Data\nHealthy Harbor Trash Wheel Collection Data\nMr. Trash Wheel\n\n\n11\n2024-03-12\nFiscal Sponsors\nFiscal Sponsor Directory\nFiscal Sponsor Directory facts\n\n\n12\n2024-03-19\nX-Men Mutant Moneyball\nMutant Moneyball Data\nMutant moneyball: a data-driven ultimate X-Men\n\n\n13\n2024-03-26\nNCAA Men’s March Madness\nMen’s March Madness Data\nBracketology: predicting March Madness\n\n\n14\n2024-04-02\nDu Bois Visualization Challenge 2024\nWeek 10 Data\nDu Bois Visualization Challenge: 2024\n\n\n15\n2024-04-09\n2023 & 2024 US Solar Eclipses\nNASA’s Scientific Visualization Studio cities-eclipse-2024.json, NASA’s Scientific Visualization Studio cities-eclipse-2023.json\nThe 2023 and 2024 Solar Eclipses: Map and Data\n\n\n16\n2024-04-16\nShiny Packages\nshiny on CRAN\nShinyConf2024: The Future is Shiny\n\n\n17\n2024-04-23\nObjects Launched into Space\nOur World in Data: Annual number of objects launched into space\nUN Office for Outer Space Affairs: Online index of objects launched into outer space\n\n\n18\n2024-04-30\nWorldwide Bureaucracy Indicators\nWorld Bank Data Catalog: Worldwide Bureaucracy Indicators\nIntroducing the Worldwide Bureaucracy Indicators\n\n\n19\n2024-05-07\nRolling Stone Album Rankings\nRolling Stone 500\nWhat Makes an Album the Greatest of All Time\n\n\n20\n2024-05-14\nThe Great American Coffee Taste Test\nJames Hoffmann and Cometeer Great American Coffee Taste Test Survey\nGreat American Coffee Taste Test Breakdown\n\n\n21\n2024-05-21\nCarbon Majors emissions data\nCarbonMajors dataset\nCarbonMajors\n\n\n22\n2024-05-28\nLisa’s Vegetable Garden Data\n{gardenR} package\nMacalester College COMP/STAT 112 Tutorials\n\n\n23\n2024-06-04\nCheese\ncheese.com\ncheese.com site and blog\n\n\n24\n2024-06-11\nCampus Pride Index\nCampus Pride Index search results\nCampus Pride Index\n\n\n25\n2024-06-18\nUS Federal Holidays\nWikipedia Federal holidays in the United States\nWikipedia Federal holidays in the United States\n\n\n26\n2024-06-25\ntidyRainbow Datasets\ntidyRainbow LGBTQ Movie Database\ntidyRainbow Datasets\n\n\n27\n2024-07-02\nTidyTuesday Datasets\nttmeta package\nttmeta package\n\n\n28\n2024-07-09\nDavid Robinson’s TidyTuesday Functions\nfunspotr examples\nNetwork Visualizations of Code Collections\n\n\n29\n2024-07-16\nEnglish Women’s Football\nThe English Women’s Football (EWF) Database, May 2024\nThe English Women’s Football (EWF) Database, May 2024\n\n\n30\n2024-07-23\nAmerican Idol data\nAmerican Idol data\nAmerican Idol data"
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\n\n\nWarning: package 'here' was built under R version 4.3.3"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#seth-harris-contributed-to-this-exercise",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#seth-harris-contributed-to-this-exercise",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\n\n\nWarning: package 'here' was built under R version 4.3.3"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.2 Description of data and data source",
    "text": "2.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "2.3 Questions/Hypotheses to be addressed",
    "text": "2.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 Data aquisition",
    "text": "3.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Data import and cleaning",
    "text": "3.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Statistical analysis",
    "text": "3.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Exploratory/Descriptive analysis",
    "text": "4.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Basic statistical analysis",
    "text": "4.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\nFigure 1: Height and weight stratified by gender.\n\n\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n#Create a boxplot with the new categorical variable on the x-axis and height on the y-axis\np_boxplot&lt;-ggplot(mydata, aes(x = Occupation, y = Height)) +   geom_boxplot() +   labs(title = \"Boxplot of Height by Occupation\",        \nx = \"Occupation\",        y = \"Height\") \n#Display boxplot\nprint(p_boxplot)\n\n\n\n# Save the boxplot to a file \nggsave(\"boxplot.png\", plot = p_boxplot) \n\nSaving 7 x 5 in image\n\n# Create a scatterplot with weight on the x-axis and the new numerical variable on the y-axis \np_scatterplot&lt;-ggplot(mydata, aes(x = Weight, y = Age)) +   geom_point() +   labs(title = \"Scatterplot of Weight and Age\",        \nx = \"Weight\",        y = \"Age\") \n\n#Display scatterplot\nprint(p_scatterplot)\n\n\n\n# Save the scatterplot to a file ggsave(\"scatterplot.png\", plot = p_scatterplot)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871\n\n\n\n\n\n\n\n\n\n\nTable 3: Another linear model fit table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n163.4615385\n64.914805\n2.5180933\n0.1280971\n\n\nAge\n0.5192308\n2.137083\n0.2429624\n0.8306802\n\n\nOccupationDoctor\n-14.5384615\n36.933011\n-0.3936441\n0.7318458\n\n\nOccupationEngineer\n-22.0769231\n31.984926\n-0.6902290\n0.5613871\n\n\nOccupationOccupation\n3.5576923\n36.162404\n0.0983810\n0.9306019\n\n\nOccupationStudent\n-21.4807692\n35.653647\n-0.6024845\n0.6080640\n\n\nOccupationUnemployed\n-28.1538462\n39.483050\n-0.7130616\n0.5497811"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nSummarize what you did, what you found and what it means.\nThe boxplot of height by occupation illustrates the distribution of heights among different occupations. Each box represents the interquartile range (IQR) of heights for a given occupation, with the line inside the box indicating the median height. The whiskers extend to the minimum and maximum heights within 1.5 times the IQR from the quartiles, and any data points outside this range are considered outliers.\nFrom the boxplot: Doctors have a relatively narrow range of heights, clustered around the median, with no outliers. Engineers show a wider range of heights and a few outliers. Students, unemployed individuals, and artists have relatively consistent heights with smaller IQRs and no outliers. The variation in the range of heights among different occupations can reflect diverse physical characteristics or sampling variations within these groups.\nThe scatterplot displays the relationship between weight and age. Key observations include: There seems to be a slight downward trend, indicating that higher weights are associated with younger ages in this dataset. There are a few outliers, such as an individual weighing over 100 kg and others weighing less than 60 kg. The data points are somewhat dispersed, indicating variability in the relationship between weight and age among the individuals. This scatterplot helps in visualizing how weight and age are distributed and correlated within the dataset.\nThis table summarizes the results of a linear regression model. Here’s a brief explanation of the columns and some of the key findings:\nterm: This indicates the predictor variables in the model, including the intercept and various categories of occupation. estimate: These are the estimated coefficients for each term in the model. For example, the intercept is 163.46, which represents the expected value of the dependent variable when all predictors are zero. std.error: This column provides the standard errors for the estimates, reflecting the variability of the coefficient estimates. statistic: This is the t-statistic for each coefficient, which is the estimate divided by its standard error. p.value: The p-value tests the null hypothesis that the coefficient is equal to zero. Smaller p-values (typically less than 0.05) indicate that the coefficient is statistically significant. Key points:\nNone of the predictors, including Age and Occupation categories, have p-values less than 0.05, indicating that they are not statistically significant in predicting the dependent variable in this model. The coefficients for occupation categories show how different occupations compare to the baseline category in their relationship to the dependent variable, but these differences are not statistically significant."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(here) #to set paths\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/Joaquin/School/DA - 6833 (Summer 2024)/Joaquin_Ramriez_Portfolio_II\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`        \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                   \n1 Height          height in centimeters                 numeric value &gt;0 or NA  \n2 Weight          weight in kilograms                   numeric value &gt;0 or NA  \n3 Gender          identified gender (male/female/other) M/F/O/NA                \n4 Age             age in years                          numeric value &gt;0 or NA  \n5 Occupation      individual's occupation               Student/Engineer/Artist…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height     &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"15…\n$ Weight     &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender     &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\"…\n$ Age        &lt;dbl&gt; 25, 30, 22, 28, 40, 35, 27, 45, 19, 50, 33, 29, 41, 36\n$ Occupation &lt;chr&gt; \"Occupation\", \"Doctor\", \"Student\", \"Artist\", \"Unemployed\", …\n\nsummary(rawdata)\n\n    Height              Weight          Gender               Age       \n Length:14          Min.   :  45.0   Length:14          Min.   :19.00  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:27.25  \n Mode  :character   Median :  70.0   Mode  :character   Median :31.50  \n                    Mean   : 602.7                      Mean   :32.86  \n                    3rd Qu.:  90.0                      3rd Qu.:39.00  \n                    Max.   :7000.0                      Max.   :50.00  \n                    NA's   :1                                          \n  Occupation       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender   Age Occupation\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;     \n1 180        80 M         25 Occupation\n2 175        70 O         30 Doctor    \n3 sixty      60 F         22 Student   \n4 178        76 F         28 Artist    \n5 192        90 NA        40 Unemployed\n6 6          55 F         35 Engineer  \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nOccupation\n0\n1\n6\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70.0\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n32.86\n8.88\n19\n27.25\n31.5\n39\n50\n▆▇▆▃▃\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nOccupation\n0\n1\n6\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n33.69\n8.65\n19\n28.00\n33\n40\n50\n▃▇▆▃▃\n\n\n\n\nhist(d1$Height)\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nOccupation\n0\n1\n6\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n33.69\n8.65\n19\n28.00\n33\n40\n50\n▃▇▆▃▃\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nOccupation\n0\n1\n6\n10\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n32.27\n7.81\n19\n27.5\n30\n38\n45\n▂▇▂▃▆\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nOccupation\n0\n1\n6\n10\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n32.27\n7.81\n19\n27.5\n30\n38\n45\n▂▇▂▃▆\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nOccupation\n0\n1\n6\n10\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n32.89\n6.81\n25\n28\n30\n36\n45\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata2 &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata2, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda - Copy.html",
    "href": "starter-analysis-exercise/code/eda-code/eda - Copy.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/Joaquin/School/DA - 6833 (Summer 2024)/Joaquin_Ramriez_Portfolio_II\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Occupation            0             1   6  10     0        6          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0  133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2   45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  32.9  6.81  25  28  30  36   45 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\n#Create a boxplot with the new categorical variable on the x-axis and height on the y-axis\np_boxplot&lt;-ggplot(mydata, aes(x = Occupation, y = Height)) +   geom_boxplot() +   labs(title = \"Boxplot of Height by Occupation\",        \nx = \"Occupation\",        y = \"Height\") \n#Display boxplot\nprint(p_boxplot)\n\n\n\n# Save the boxplot to a file \nggsave(\"boxplot.png\", plot = p_boxplot) \n\nSaving 7 x 5 in image\n\n# Create a scatterplot with weight on the x-axis and the new numerical variable on the y-axis \np_scatterplot&lt;-ggplot(mydata, aes(x = Weight, y = Age)) +   geom_point() +   labs(title = \"Scatterplot of Weight and Age\",        \nx = \"Weight\",        y = \"Age\") \n\n#Display scatterplot\nprint(p_scatterplot)\n\n\n\n# Save the scatterplot to a file ggsave(\"scatterplot.png\", plot = p_scatterplot)\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "In this analysis, I will reproduce a graph from the FiveThirtyEight article “Congress Today Is Older Than It’s Ever Been” and develop tables to support the findings. The dataset titled data_aging_congress.csv includes demographic information about the United States Senate and House of Representatives over time, focusing on age demographics from the 66th Congress (1919-192) to the 118th Congress (2023-2025). The analysis aims to visualize trends in median age over time for both the House and Senate. The graph will be created using plotly to allow interactive features, and the tables will be generated using the gt package."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#introduction",
    "href": "presentation-exercise/presentation-exercise.html#introduction",
    "title": "Presentation Exercise",
    "section": "",
    "text": "In this analysis, I will reproduce a graph from the FiveThirtyEight article “Congress Today Is Older Than It’s Ever Been” and develop tables to support the findings. The dataset titled data_aging_congress.csv includes demographic information about the United States Senate and House of Representatives over time, focusing on age demographics from the 66th Congress (1919-192) to the 118th Congress (2023-2025). The analysis aims to visualize trends in median age over time for both the House and Senate. The graph will be created using plotly to allow interactive features, and the tables will be generated using the gt package."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#data",
    "href": "presentation-exercise/presentation-exercise.html#data",
    "title": "Presentation Exercise",
    "section": "Data",
    "text": "Data\nThe data used for this analysis is sourced from the FiveThirtyEight Congress demographics dataset, located in the “fivethirtyeight- data”\nTo effectively recreate and refine the visualizations, I’ve documented some of the AI prompts, below:\nReproduce Graphs/Tables\n\nPrompt: Can you recreate this graph to show the median age trends over time for the House and Senate?\nResponse: The initial graph was created to visualize the median age trends for the House and Senate over time, using plotly for interactivity and styling elements from ggplot2 such as labs and theme settings.\n\nError Fixing and Debugging\n\nPrompt: How do I fix the error in geom_line(aes(color = chamber))?\nResponse: Error fixed by ensuring the correct mapping of the chamber column and adjusting the data preprocessing steps.\n\nTable Creation\n\nPrompt: Create a publication-quality table showing the median age for each Congress session.\nResponse: A table was generated using the gt package, formatted to match publication standards.\n\nVisual Enhancements\n\nPrompt: How can I make the graph more visually appealing with color schemes and annotations?\nResponse: The graph was enhanced with a color scheme and annotations to highlight key trends and make it more visually appealing.\nPrompt: Can I include interactive features in the graph for better user engagement?\nResponse: Interactive features were added to the graph using Plotly, allowing users to hover over data points for additional information.\n\nThese questions guided the step-by-step process for creating and modifying the graphs and tables in R."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#load-necessary-libraries",
    "href": "presentation-exercise/presentation-exercise.html#load-necessary-libraries",
    "title": "Presentation Exercise",
    "section": "Load Necessary Libraries",
    "text": "Load Necessary Libraries\nAs we move forward, the next crucial step involves loading the appropriate libraries and verifying the data to ensure it is ready for the upcoming analyses and visualizations.\n\nlibrary(readr)\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nlibrary(gt)\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nlibrary(tibble)\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/Joaquin/School/DA - 6833 (Summer 2024)/Joaquin_Ramriez_Portfolio_II"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#data-loading-and-verification",
    "href": "presentation-exercise/presentation-exercise.html#data-loading-and-verification",
    "title": "Presentation Exercise",
    "section": "Data Loading and Verification",
    "text": "Data Loading and Verification\n\n# Check the root directory determined by here\nprint(here::here())\n\n[1] \"C:/Users/Joaquin/School/DA - 6833 (Summer 2024)/Joaquin_Ramriez_Portfolio_II\"\n\n# Define the file path for the aging congress data\ndata_aging_congress_path &lt;- here::here(\"presentation-exercise\", \"data_aging_congress.csv\")\n\n# Load the data from the CSV file\ndata_aging_congress &lt;- read_csv(\"data_aging_congress.csv\")\n\nRows: 29120 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): chamber, state_abbrev, bioname, bioguide_id, generation\ndbl  (6): congress, party_code, cmltv_cong, cmltv_chamber, age_days, age_years\ndate (2): start_date, birthday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Print the first few rows of the data frame to verify it is loaded correctly\nprint(head(data_aging_congress))\n\n# A tibble: 6 × 13\n  congress start_date chamber state_abbrev party_code bioname        bioguide_id\n     &lt;dbl&gt; &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;      \n1       82 1951-01-03 House   ND                  200 AANDAHL, Fred… A000001    \n2       80 1947-01-03 House   VA                  100 ABBITT, Watki… A000002    \n3       81 1949-01-03 House   VA                  100 ABBITT, Watki… A000002    \n4       82 1951-01-03 House   VA                  100 ABBITT, Watki… A000002    \n5       83 1953-01-03 House   VA                  100 ABBITT, Watki… A000002    \n6       84 1955-01-03 House   VA                  100 ABBITT, Watki… A000002    \n# ℹ 6 more variables: birthday &lt;date&gt;, cmltv_cong &lt;dbl&gt;, cmltv_chamber &lt;dbl&gt;,\n#   age_days &lt;dbl&gt;, age_years &lt;dbl&gt;, generation &lt;chr&gt;\n\n# Check the structure of the data frame\nstr(data_aging_congress)\n\nspc_tbl_ [29,120 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ congress     : num [1:29120] 82 80 81 82 83 84 85 86 87 88 ...\n $ start_date   : Date[1:29120], format: \"1951-01-03\" \"1947-01-03\" ...\n $ chamber      : chr [1:29120] \"House\" \"House\" \"House\" \"House\" ...\n $ state_abbrev : chr [1:29120] \"ND\" \"VA\" \"VA\" \"VA\" ...\n $ party_code   : num [1:29120] 200 100 100 100 100 100 100 100 100 100 ...\n $ bioname      : chr [1:29120] \"AANDAHL, Fred George\" \"ABBITT, Watkins Moorman\" \"ABBITT, Watkins Moorman\" \"ABBITT, Watkins Moorman\" ...\n $ bioguide_id  : chr [1:29120] \"A000001\" \"A000002\" \"A000002\" \"A000002\" ...\n $ birthday     : Date[1:29120], format: \"1897-04-09\" \"1908-05-21\" ...\n $ cmltv_cong   : num [1:29120] 1 1 2 3 4 5 6 7 8 9 ...\n $ cmltv_chamber: num [1:29120] 1 1 2 3 4 5 6 7 8 9 ...\n $ age_days     : num [1:29120] 19626 14106 14837 15567 16298 ...\n $ age_years    : num [1:29120] 53.7 38.6 40.6 42.6 44.6 ...\n $ generation   : chr [1:29120] \"Lost\" \"Greatest\" \"Greatest\" \"Greatest\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   congress = col_double(),\n  ..   start_date = col_date(format = \"\"),\n  ..   chamber = col_character(),\n  ..   state_abbrev = col_character(),\n  ..   party_code = col_double(),\n  ..   bioname = col_character(),\n  ..   bioguide_id = col_character(),\n  ..   birthday = col_date(format = \"\"),\n  ..   cmltv_cong = col_double(),\n  ..   cmltv_chamber = col_double(),\n  ..   age_days = col_double(),\n  ..   age_years = col_double(),\n  ..   generation = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nHaving successfully loaded and verified the dataset, we can now proceed to visualize the data. The next step involves recreating plots to analyze the trends in median age for both the U.S. Senate and House of Representatives from 1919 to 2023."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#plot-creation-median-age-of-the-u.s.-senate-and-u.s.-house-by-congress-from-1919-to-2023.",
    "href": "presentation-exercise/presentation-exercise.html#plot-creation-median-age-of-the-u.s.-senate-and-u.s.-house-by-congress-from-1919-to-2023.",
    "title": "Presentation Exercise",
    "section": "Plot Creation: Median age of the U.S. Senate and U.S. House by Congress from 1919 to 2023.",
    "text": "Plot Creation: Median age of the U.S. Senate and U.S. House by Congress from 1919 to 2023.\nThis graph is adapted from the FiveThirtyEight article“Congress Today Is Older Than It’s Ever Been”.\n\n# Aggregate the data to get the median age for each Congress for House and Senate\nmedian_age_congress &lt;- data_aging_congress %&gt;%\n  group_by(congress, chamber) %&gt;%\n  summarize(median_age = median(age_years, na.rm = TRUE), .groups = 'drop')\n\n# Adjust the year calculation based on the 66th Congress starting in 1919\nfirst_congress &lt;- 66\nfirst_congress_year &lt;- 1919\nmedian_age_congress &lt;- median_age_congress %&gt;%\n  mutate(year = first_congress_year + (congress - first_congress) * 2)\n\n# Create the plotly plot\ninteractive_plot &lt;- plot_ly(data = median_age_congress, \n                            x = ~year, \n                            y = ~median_age, \n                            color = ~chamber, \n                            colors = c(\"Senate\" = \"purple\", \"House\" = \"darkgreen\"),\n                            type = 'scatter', \n                            mode = 'lines+markers', \n                            line = list(width = 2),\n                            marker = list(size = 5),\n                            text = ~paste(\n                              chamber, \"&lt;br&gt;\",\n                              year, \"&lt;br&gt;\",\n                              round(median_age, 1)\n                            ),\n                            hoverinfo = 'text') %&gt;%\nlayout(\n    title = list(\n      text = \"&lt;b&gt;The House and Senate are older than ever before&lt;/b&gt;\", \n      font = list(size = 14, family = \"Georgia\", weight = \"bolder\"),\n      x = .075, # Align title to the left\n      y = 0.95, # Position title at the top\n      xanchor = 'left',\n      yanchor = 'top',\n      pad = list(b = 0) # No extra padding between title and subtitle\n    ),\n    annotations = list(\n      list(\n        x = 0, # Align subtitle to the left\n        y = 1.23, # Position just below the title\n        text = \"Median age of the U.S. Senate and U.S. House by Congress, 1919 to 2023\",\n        showarrow = FALSE,\n        xref = \"paper\",\n        yref = \"paper\",\n        font = list(size = 13, family = \"Georgia\"),\n        align = \"left\"\n      )\n    ),\n    xaxis = list(\n      title = TRUE, \n      range = c(1918, 2024),\n      tickvals = seq(1920, 2020, 10),\n      tickfont = list(size = 12),\n      showline = TRUE,\n      linewidth = 2,\n      linecolor = 'white',\n      showgrid = FALSE\n    ),\n    yaxis = list(\n      title = FALSE, \n      range = c(43, 68),\n      tickvals = seq(45, 70, 5),\n      tickfont = list(size = 12),\n      showline = FALSE,\n      zeroline = FALSE, # Ensure the zero line is not shown\n      showgrid = TRUE # Optionally hide the grid lines\n    ),\n    legend = list(\n      orientation = \"h\", \n      x = 0, \n      y = 1, \n      xanchor = \"left\", \n      yanchor = \"bottom\",\n      font = list(size = 14)\n    ),\n    plot_bgcolor = 'white',\n    paper_bgcolor = 'white',\n    margin = list(t = 120) # Increase top margin to shift graph down and accommodate title and subtitle\n  )\n\n# Print the interactive plot\ninteractive_plot\n\n\n\n\n\nFrom the graph: the median age of Senate and House members, several trends are evident. Over the years, the median age of both the Senate and the House has fluctuated. From 1919 to the early 2000s, the median age of Senate members never exceeded 60 years, and the median age of the House of Representatives never surpassed 55 years.\nAccording to the FiveThirtyEight article “Congress Today Is Older Than It’s Ever Been”, the median age of the 118th Congress is 59 years. Specifically, the median age of a senator is 65 years, while the median age of a representative is about 58 years. In comparison, the median age of the entire U.S. population is around 39 years, with about 42 percent of people being 45 years or older.\nIn contrast, the House of Representatives has also experienced fluctuations in median age, reaching its lowest point in the 1980s. Despite these fluctuations, the median age in Congress has increased over the past decade.\nIt is also important to note that the House of Representatives has consistently had a younger median age compared to the Senate. This could be a result of the fact that members of the House of Representatives must be at least 25 years old, while members of the Senate must be at least 30 years old.\nTo better understand these trends, let’s look at the detailed data presented in the table below."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#table-median-age-of-u.s.-congress-members-by-chamber-and-party.",
    "href": "presentation-exercise/presentation-exercise.html#table-median-age-of-u.s.-congress-members-by-chamber-and-party.",
    "title": "Presentation Exercise",
    "section": "Table: Median Age of U.S. Congress Members by Chamber and Party.",
    "text": "Table: Median Age of U.S. Congress Members by Chamber and Party.\n\n# Calculate median age by chamber and party, and sort by median age within each chamber\nmedian_age_table &lt;- data_aging_congress %&gt;%\n  group_by(chamber, party_code) %&gt;%\n  summarize(\n    median_age = median(age_years, na.rm = TRUE),\n    .groups = 'drop'\n  ) %&gt;%\n  # Arrange by chamber first, then by median_age in descending order within each chamber\n  arrange(chamber, desc(median_age)) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Median Age of U.S. Congress Members by Chamber and Party\",\n    subtitle = \"From 1919 to 2023\"\n  ) %&gt;%\n  cols_label(\n    chamber = \"Chamber\",\n    party_code = \"Party Code\",\n    median_age = \"Median Age\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(median_age),  # Updated from vars(median_age) to c(median_age)\n    decimals = 1\n  ) %&gt;%\n  cols_align(\n    align = \"center\",\n    columns = everything()\n  ) %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"lightgrey\"),\n      cell_borders(\n        sides = \"all\",\n        color = \"black\",\n        weight = px(1)\n      )\n    ),\n    locations = cells_body(\n      columns = c(chamber, party_code, median_age)  # Updated from vars(chamber, party_code, median_age) to c(chamber, party_code, median_age)\n    )\n  ) %&gt;%\n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"lightblue\")\n    ),\n    locations = cells_column_labels()\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    table.border.top.color = \"black\",\n    table.border.top.width = px(2),\n    table.border.bottom.color = \"black\",\n    table.border.bottom.width = px(2),\n    column_labels.border.top.color = \"black\",\n    column_labels.border.top.width = px(2),\n    column_labels.border.bottom.color = \"black\",\n    column_labels.border.bottom.width = px(2),\n    data_row.padding = px(5)\n  )\n\n# Print the table\nmedian_age_table\n\n\n\n\n\n  \n    \n      Median Age of U.S. Congress Members by Chamber and Party\n    \n    \n      From 1919 to 2023\n    \n    \n      Chamber\n      Party Code\n      Median Age\n    \n  \n  \n    House\n380\n64.0\n    House\n370\n57.6\n    House\n537\n55.4\n    House\n328\n54.2\n    House\n356\n54.0\n    House\n347\n53.6\n    House\n200\n52.7\n    House\n100\n52.2\n    House\n331\n51.7\n    House\n329\n42.9\n    House\n523\n42.2\n    House\n522\n40.1\n    House\n402\n34.4\n    Senate\n328\n69.3\n    Senate\n200\n58.0\n    Senate\n100\n57.4\n    Senate\n537\n51.5\n    Senate\n112\n49.8\n    Senate\n370\n44.9\n  \n  \n  \n\n\n\n\nThe table above provides a detailed view of the “Median Age of U.S. Congress Members by Chamber and Party from 1919 to 2023.” It highlights the median ages of Congress members across different chambers and party. Key observations include:\nHouse of Representatives:\n\nThe median age of House members ranges from 34 years to 64 years.\nLiberal (party code 402): lowest median age at 34 years.\nIndependents (party code 328): highest median age at 64 years.\nDemocrats (party code 100) median age of 52 years.\nRepublicans (party code 200) median age of at 52 years.\n\nSenate:\n\nThe median age of Senate members ranges from 44 years to 69 years.\nProgressive (party code 328), lowest median age of 44 years.\nIndependent (party code 328), highest median age of 69 years.\nDemocrats (party code 100), median age is 57 years.\nRepublicans (party code 200), median age of 58 years.\n\nThe trends in median age across different chambers and party affiliations provide valuable insights into the changing demographics of Congress. To deepen our understanding, we further explored the composition of Congress by examining the top generation. This additional analysis will highlight the generational representation."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#table-number-of-u.s.-congress-members-by-generation-and-chamber.",
    "href": "presentation-exercise/presentation-exercise.html#table-number-of-u.s.-congress-members-by-generation-and-chamber.",
    "title": "Presentation Exercise",
    "section": "Table: Number of U.S. Congress Members by Generation and Chamber.",
    "text": "Table: Number of U.S. Congress Members by Generation and Chamber.\n\n# Calculate the number of members by generation and chamber\ngeneration_chamber_table &lt;- data_aging_congress %&gt;%\n  group_by(chamber, generation) %&gt;%\n  summarize(\n    member_count = n(),\n    .groups = 'drop'\n  ) %&gt;%\n  # Arrange by chamber and then by member_count in descending order within each chamber\n  arrange(chamber, desc(member_count)) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Number of U.S. Congress Members by Generation and Chamber\",\n    subtitle = \"From 1919 to 2023\"\n  ) %&gt;%\n  cols_label(\n    chamber = \"Chamber\",\n    generation = \"Generation\",\n    member_count = \"Number of Members\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(member_count),  # Updated from vars(member_count) to c(member_count)\n    decimals = 0\n  ) %&gt;%\n  cols_align(\n    align = \"center\",\n    columns = everything()\n  ) %&gt;%\n  tab_style(\n    style = list(\n      cell_fill(color = \"lightgrey\"),\n      cell_borders(\n        sides = \"all\",\n        color = \"black\",\n        weight = px(1)\n      )\n    ),\n    locations = cells_body(\n      columns = c(chamber, generation, member_count)  # Updated from vars(chamber, generation, member_count) to c(chamber, generation, member_count)\n    )\n  ) %&gt;%\n  tab_style(\n    style = list(\n      cell_text(weight = \"bold\"),\n      cell_fill(color = \"lightblue\")\n    ),\n    locations = cells_column_labels()\n  ) %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.title.font.size = 14,\n    heading.subtitle.font.size = 12,\n    table.border.top.color = \"black\",\n    table.border.top.width = px(2),\n    table.border.bottom.color = \"black\",\n    table.border.bottom.width = px(2),\n    column_labels.border.top.color = \"black\",\n    column_labels.border.top.width = px(2),\n    column_labels.border.bottom.color = \"black\",\n    column_labels.border.bottom.width = px(2),\n    data_row.padding = px(5)\n  )\n\n# Print the table\ngeneration_chamber_table\n\n\n\n\n\n  \n    \n      Number of U.S. Congress Members by Generation and Chamber\n    \n    \n      From 1919 to 2023\n    \n    \n      Chamber\n      Generation\n      Number of Members\n    \n  \n  \n    House\nGreatest\n5,858\n    House\nSilent\n4,416\n    House\nBoomers\n4,323\n    House\nLost\n3,830\n    House\nMissionary\n3,713\n    House\nGen X\n1,014\n    House\nProgressive\n325\n    House\nMillennial\n129\n    House\nGilded\n13\n    House\nGen Z\n1\n    Senate\nGreatest\n1,289\n    Senate\nSilent\n1,185\n    Senate\nMissionary\n1,055\n    Senate\nLost\n902\n    Senate\nBoomers\n785\n    Senate\nProgressive\n160\n    Senate\nGen X\n116\n    Senate\nMillennial\n4\n    Senate\nGilded\n2\n  \n  \n  \n\n\n\n\nThe historical data reveals notable generational distributions within the U.S. Congress from 1919 to 2023:\nHouse of Representatives:\n\nGreatest Generation: 5,858 members\nSilent Generation: 4,416 members\nBoomer Generation: 4,323 members\nLost Generation: 3,830 members\nMissionary Generation: 3,713 members\n\nSenate:\n\nGreatest Generation: 1,289 members\nSilent Generation: 1,185 members\nMissionary Generation: 1,055 members\nLost Generation: 902 members\nBoommers - 785 members\n\nThe data highlights the Greatest Generation with the most represented group in both the House and Senate. Followed by the Silent Generation and the Boomers. In the House, the Boomer Generation ranks third, while in the Senate, it ranks fifth.\nMoving forward, we will dive deeper into the generational impact on Congress by focusing specifically on the Baby Boomers, Lost Generation, Greatest Generation, and Silent Generation. This next analysis will provide a closer look at how these generations have influenced the legislative landscape and how their representation has evolved over time."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#plot-creation-congress-is-never-dominated-by-generations-as-old-as-boomers.",
    "href": "presentation-exercise/presentation-exercise.html#plot-creation-congress-is-never-dominated-by-generations-as-old-as-boomers.",
    "title": "Presentation Exercise",
    "section": "Plot Creation: Congress is never dominated by generations as old as boomers.",
    "text": "Plot Creation: Congress is never dominated by generations as old as boomers.\n\n# Calculate the share of each generation within each Congress\ngeneration_share &lt;- data_aging_congress %&gt;%\n  group_by(congress, generation) %&gt;%\n  summarise(count = n(), .groups = 'drop') %&gt;%\n  group_by(congress) %&gt;%\n  mutate(total = sum(count),\n         share = count / total) %&gt;%\n  ungroup()\n\n# Identify the largest generation in each Congress\nlargest_generation &lt;- generation_share %&gt;%\n  group_by(congress) %&gt;%\n  filter(share == max(share)) %&gt;%\n  select(congress, generation)\n\n# Calculate the median age for the largest generation\nmedian_age_largest_gen &lt;- data_aging_congress %&gt;%\n  semi_join(largest_generation, by = c(\"congress\", \"generation\")) %&gt;%\n  group_by(congress, generation) %&gt;%\n  summarize(median_age = median(age_years, na.rm = TRUE), .groups = 'drop') %&gt;%\n  mutate(year = 1919 + (congress - 66) * 2)  # Convert Congress number to year\n\n# Filter years to 1940 through 2020\nmedian_age_largest_gen &lt;- median_age_largest_gen %&gt;%\n  filter(year &gt;= 1940 & year &lt;= 2023)\n\n\n# Create the plotly plot directly with adjusted year range and annotations\ninteractive_plot &lt;- plot_ly(\n  data = median_age_largest_gen,\n  x = ~year,\n  y = ~median_age,\n  color = ~generation,\n  colors = c(\"Lost\" = \"black\", \"Greatest\" = \"darkgreen\", \"Silent\" = \"orange\", \"Boomers\" = \"red\"),\n  type = 'scatter',\n  mode = 'lines+markers',  # Show lines and markers\n  line = list(width = 2),\n  marker = list(size = 5, symbol = 'circle', line = list(width = 2)),\n  text = ~paste(\n    \"\", generation, \"&lt;br&gt;\",\n    \"\", year, \"&lt;br&gt;\",\n    \"\", round(median_age, 1))  # Round the age to the nearest tenth\n  ,\n  hoverinfo = 'text'\n) %&gt;%\n  layout(\n    title = list(\n      text = \"&lt;b&gt;Congress is never dominated by generations as old as boomers&lt;/b&gt;\",\n      font = list(size = 14, family = \"Georgia\", weight = \"bolder\"),\n      x = .075,\n      y = 0.95,\n      xanchor = 'left',\n      yanchor = 'top',\n      pad = list(b = 0)\n    ),\n    annotations = list(\n      list(\n        x = 0,\n        y = 1.23,\n        text = \"Median age of the largest generation in each Congress, 1940 to 2023\",  # Updated subtitle to reflect the extended year range\n        showarrow = FALSE,\n        xref = \"paper\",\n        yref = \"paper\",\n        font = list(size = 13, family = \"Georgia\"),\n        align = \"left\"\n      )\n    ),\n    xaxis = list(\n      title = TRUE,\n      range = c(1935, 2027),  # Ensures the x-axis covers years from 1935 to 2027\n      tickvals = seq(1940, 2023, 10),\n      tickfont = list(size = 12),\n      showline = TRUE,\n      linewidth = 2,\n      linecolor = 'white',\n      showgrid = FALSE\n    ),\n    yaxis = list(\n      title = FALSE,\n      range = c(37, 73),\n      tickvals = seq(40, 70, 5),\n      tickfont = list(size = 12),\n      showline = TRUE,\n      linecolor = 'white',\n      zeroline = FALSE,\n      showgrid = TRUE\n    ),\n    legend = list(\n      orientation = \"h\",\n      x = 0,\n      y = 1,\n      xanchor = \"left\",\n      yanchor = \"bottom\",\n      font = list(size = 14)\n    ),\n    plot_bgcolor = 'white',\n    paper_bgcolor = 'white',\n    margin = list(t = 120)\n  )\n\n# Print the interactive plot\ninteractive_plot\n\n\n\n\n\nThe plot “Congress is Never Dominated by Generations as Old as Boomers” visualizes the median age of the largest generation in each Congress from 1940 to 2023. The plot illustrates how, despite fluctuations, no single generation has dominated Congress as significantly as the Baby Boomers. This visualization reveals that while older generations like the Greatest Generation and the Silent Generation had notable impacts.The plot’s line highlight the changing generational leadership in Congress and show how the median age of the largest generation has evolved over the decades."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#conclusion",
    "href": "presentation-exercise/presentation-exercise.html#conclusion",
    "title": "Presentation Exercise",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis explored the evolving age demographics of the U.S. Congress from 1919 to 2023, focusing on trends in median age and generational representation. The recreated graph from the FiveThirtyEight article “Congress Today Is Older Than It’s Ever Been” highlighted that the median age of Congress members has increased over time, with the 118th Congress showing a median age of 59 years compared to historical figures.\nThe examination of the median age of Congress members by chamber and party revealed that, while the House of Representatives generally has a younger median age compared to the Senate, both chambers have seen significant age increases in recent decades.\nAdditionally, the analysis of generational representation demonstrated that the Greatest Generation, the Lost Generation, the Silent Generation and the Baby Boomers had the most significant presence in both the House and Senate.\nFinally, the plot “Congress is Never Dominated by Generations as Old as Boomers” illustrated that despite fluctuations, no single generation has dominated Congress as consistently as the Baby Boomers did at their peak. This plot effectively visualizes how generational influence has shifted and provides insights into the legislative landscape over the past century.\nOverall, this study underscores the dynamic nature of Congressional demographics and offers a detailed view of how age and generational shifts shape U.S. legislative bodies."
  },
  {
    "objectID": "market-segmentation/market-segmentation.html",
    "href": "market-segmentation/market-segmentation.html",
    "title": "Market Segmentation: Dr Pepper",
    "section": "",
    "text": "Market Segmentation Analysis Report: Dr Pepper\n\nIntroduction\nThis report presents a comprehensive market segmentation analysis aimed at identifying consumer segments that align with Dr Pepper’s marketing strategies. By leveraging variables related to consumer behavior, preferences, and attitudes, actionable insights have been extracted to inform targeted marketing efforts.\nThe objective of this analysis is to uncover distinct consumer segments by analyzing key drivers and descriptors derived from the NCS data dictionary. These insights aim to guide targeted marketing strategies, enhance customer engagement, and increase market share for Dr Pepper.\nData Sources: Data were obtained from a personal survey booklet, and the segmentation analysis was conducted using SAS software.\n\n\nMethodology\nIn this section, I will outline the key analytical steps taken to perform the segmentation analysis.\nThe Target variable for segmentation is Dr Pepper, representing consumer preferences for the brand.\nThe Driver Variables were utilized to effectively segment the market, four key driver variables were selected, representing consumer behaviors that are likely to influence their choices regarding beverages:\n   drink_between_meals: Reflects whether the consumer often drinks between meals.\n\n   like_to_try_new_drinks: Captures curiosity and willingness to try new beverages.\n\n   when_on_tv_go_online_get_more: Measures how likely the consumer is to go online after seeing an ad on TV.\n\n   buy_online_or_in_store: Indicates whether the consumer prefers to shop online or in-store.\nFor the Abstract Constructor Factor Variables: I selected two key factors that were derived from multiple related variables using Principal Component Analysis (PCA):\nIndulgence Behavior and Attitude: How likely is the consumer indulging in various indulgent behaviors or habits?\n\n     DRINKING_GET_DRUNK: \"The point of drinking is to get drunk.\"\n\n     OFTEN_DRINK_ALCOHOL: \"Often drink alcoholic beverages at restaurants.\"\n\n     TRY_NEW_FOOD_PRDCT: \"I'm usually first to try new food products.\"\n\n     FEEL_GUILTY_SWEETS: \"I feel guilty when I eat sweets.\"\n\nEnvironmental Behavior and Attitude: How important does the consumer believe in taking responsibility for the environment?\n\n     PEOPLE_NEED_TO_RECYCLE: \"People have a duty to recycle.\"\n\n     MAKE_EFFORT_TO_RECYCLE: \"I make an effort to recycle.\"\n\n     PEOPLE_RESPONS_TO_RECYCLD_PRDCTS: \"People have a responsibility to use recycled products.\"\n\n     PERSONAL_ENVRNMNT_RESPONSIBLE: \"Personal obligation and environmental responsibility.\"\nNext, the Descriptor Variables: provide additional insights into consumer characteristics:\n   Competitive Preferences:\n\n      dr_pepper: \"I prefer Dr Pepper.\"\n\n      Coca_Cola: \"I prefer Coca-Cola.\"\n\n      Sprite: \"I prefer Sprite.\"\n\n   Demographic Factors:\n\n      Gender: Male, Female\n\n      Region: Northeast, Midwest, South, West\n\n      Age Groups**: 18-24, 25-49, 50+\n\n   Media Preferences**:\n\n      Spotify**: \"Spotify was used during the last 7 days.\"\n\n      YouTube**: \"YouTube was used during the last 7 days.\"\n \n      ESPN**: \"ESPN was viewed during the last 7 days.\"\n\n\nFactor Analysis\nTo reduce dimensionality and identify underlying factors, PCA was conducted. Two key factors were derived, reflecting consumer behaviors and attitudes.\nPrincipal Component Analysis (PCA)\n\nThe PCA was utilized to extract abstract constructs from multiple variables. The Kaiser Criterion was applied to retain factors with eigenvalues greater than or equal to 1. The analysis resulted in two factors accounting for 52.34% of the total variance (Factor 1: 35.79%, Factor 2: 16.55%).\n________________________________________________________________________________________\nTo further refine our factor selection, we examined the scree plot and applied the elbow criterion. By focusing on the point where the curve begins to flatten, we confirmed the decision to retain two factors. This approach ensures that we capture the most meaningful patterns in the data without overcomplicating the model.\n\n________________________________________________________________________________________\nAfter identifying the two key factors through PCA, the Rotated Factor Pattern was applied to enhance interpretability. Interestingly, the rotation revealed that Factor 1, initially labeled as reflecting Indulgent Behaviors, actually exhibited stronger correlations with variables related to Environmental Responsibility. Conversely, Factor 2, originally linked to Environmental Responsibility, displayed significant associations with Indulgent Behaviors. This shift in factor interpretation underscores the nuanced interplay between consumer behaviors and attitudes, paving the way for a deeper exploration of these patterns.\n\n________________________________________________________________________________________\nBuilding on the insights from factor analysis, K-Means Clustering was employed to further segment the consumer groups. The number of clusters was evaluated using the CCC Plot and Pseudo F Plot, which suggested viable cluster counts of 3, 5, and 9. Through detailed analysis of cluster means and interpretability, a three-cluster solution emerged as the most parsimonious and insightful segmentation.\n\n\n\nNumber of Clusters\nR square\nCCC\nPseudo F\n\n\n\n\n3\n0.27176\n39.855\n5622.32\n\n\n4\n0.36802\n9.71\n5076.37\n\n\n5\n0.42914\n21.246\n5104.69\n\n\n6\n0.46407\n11.616\n4523.63\n\n\n7\n0.49418\n2.269\n4118.01\n\n\n8\n0.51624\n34.21\n4258.66\n\n\n9\n0.53490\n53.675\n4263.33\n\n\n\nCCC Plot: The CCC plot reveals the first local maximum at k=3, with subsequent notable peaks at k=5 and k=9. These observations suggest that 3, 5, and 9 could be viable cluster counts.\n\nThe Pseudo F plot corroborates the findings from the CCC plot, showing the highest value at k=3, with a secondary peak at k=5. Beyond k=5, the values begin to decline, reinforcing the idea that both 3 and 5 clusters are reasonable solutions.\n\n________________________________________________________________________________________\nHowever, determining the best solution requires deeper analysis. To that end, we proceed by examining the Cluster Means to understand the variation within clusters and assess the interpretability of each model.\nAfter reviewing the Cluster Means for both 3 and 5 clusters, clear differences emerge. Cluster 3 exhibits significant variation across most variables, indicating that a simpler 3-cluster model may offer better interpretability. In contrast, while Cluster 5 also shows variation, it introduces additional complexity, which may hinder the clarity of the solution. Based on these observations, a 3-cluster solution appears to be more practical and interpretable.\n\n________________________________________________________________________________________\nTo ensure that this solution is robust, we conduct a Gap Analysis to confirm the cluster validity and further evaluate the chosen model.\nA Gap Analysis was conducted to validate the cluster solutions. While the First Peak and Global Peak initially suggested a 2-cluster solution, the K-Means clustering with 3 clusters was favored for its clarity and ease of interpretation.\n\nBy examining the Gap Analysis cluster means, we found that the clusters are well-defined, with clear distinctions between them. This supports the decision to proceed with the 3-cluster solution.\n\nExamining the GAP Analysis cluster means further supports the notion that the clusters are well-defined, demonstrating adequate discrimination between them.\nMoreover, a comparison between the K-means and HPCLUS approaches provides additional insights into the strengths of each method. With this confirmation, we can now delve into the detailed findings and descriptions of each cluster, which will provide a clearer understanding of the consumer segments identified through the analysis.\n________________________________________________________________________________________\nFindings and Cluster Descriptions\n\n\n\n\nCluster 1 (Seasonal Customers): This cluster is characterized by older individuals, predominantly women, who display a lower preference for beverages compared to other clusters. They tend to be more conservative in trying new drinks and show moderate engagement in online shopping.\nCluster 2 (Media Enthusiasts): Members of this cluster are inclined to consume beverages between meals and demonstrate a strong interest in trying new drinks. They are highly responsive to media prompts and exhibit the highest usage of media platforms such as YouTube and Spotify. The demographic distribution is more balanced in terms of age and gender, and this group shows a higher preference for beverages.\nCluster 3 (Beverage Aficionados): Individuals in this cluster have the highest propensity for drinking between meals and the greatest enthusiasm for trying new beverages. They are particularly responsive to media advertisements and exhibit a preference for online shopping over in-store purchases. This cluster also shows the highest preference for beverages and ranks second in terms of media engagement.\n\n\nRecommendations\nMy recommendation is for Dr Pepper to direct their marketing efforts towards Clusters 2 and 3.\n\nCluster 2 comprises individuals who exhibit a positive attitude towards trying new drinks and demonstrate high online activity, including making purchases online.\nCluster 3 consists of individuals who are environmentally conscious, enthusiastic about trying new beverages, and favor online shopping. They also show a strong preference for drinks.\n\nTargeting these two clusters would allow Dr Pepper to effectively leverage its advertising and marketing strategies, potentially acquiring new customers, particularly in the southern region of the country.\n\n\nConclusion\nThis market segmentation analysis offers valuable insights for Dr Pepper to refine its marketing strategies by targeting distinct consumer segments. Concentrating on Clusters 2 and 3 enables Dr Pepper to engage with segments that demonstrate a higher propensity for digital media consumption and a willingness to experiment with new beverages. This strategic focus is expected to enhance market presence and foster stronger customer retention.\n\n\n\nSAS CODE:\n\nlibname mylib \"P:\\\";\nfilename bigrec \"P:\\fa15_data.txt\"  lrecl = 65576;\ndata mytemp;\ninfile bigrec;\ninput \nmyid 1-7\n\n/*Driver Variables */\nsnack_between_meals_aglo  4280\nsnack_between_meals_agli  4327                                                  \nsnack_between_meals_anya  4374\nsnack_between_meals_neit  4421                                                  \nsnack_between_meals_dgli  4468                                                  \nsnack_between_meals_dglo  4515                                                                                                \nsnack_between_meals_anyd  4562                                                  \ntry_new_drinks_aglo  4305\ntry_new_drinks_agli  4352                                                  \ntry_new_drinks_anya  4399\ntry_new_drinks_neit  4446                                                  \ntry_new_drinks_dgli  4493                                                  \ntry_new_drinks_dglo  4540                                                                                                \ntry_new_drinks_anyd  4587                                                  \nsee_on_tv_go_online_more_aglo  5515\nsee_on_tv_go_online_more_agli  5553                                                  \nsee_on_tv_go_online_more_anya  5591\nsee_on_tv_go_online_more_neit  5629                                                  \nsee_on_tv_go_online_more_dgli  5667                                                  \nsee_on_tv_go_online_more_dglo  5705                                                                                                \nsee_on_tv_go_online_more_anyd  5743   \nbuy_online_or_store_aglo  5518\nbuy_online_or_store_agli  5556                                                  \nbuy_online_or_store_anya  5594\nbuy_online_or_store_neit  5632                                                  \nbuy_online_or_store_dgli  5670                                                  \nbuy_online_or_store_dglo  5708                                                                                                \nbuy_online_or_store_anyd  5746  \n\n/*First Abstract - indulgence */\nDRINKING_TO_GET_DRUNK_aglo  4298\nDRINKING_TO_GET_DRUNK_agli  4345\nDRINKING_TO_GET_DRUNK_anya  4392\nDRINKING_TO_GET_DRUNK_neit  4439\nDRINKING_TO_GET_DRUNK_dgli  4486\nDRINKING_TO_GET_DRUNK_dglo  4533\nDRINKING_TO_GET_DRUNK_anyd  4580\nOFTEN_DRINK_ALCOHOLIC_aglo  4291\nOFTEN_DRINK_ALCOHOLIC_agli  4338\nOFTEN_DRINK_ALCOHOLIC_anya  4385\nOFTEN_DRINK_ALCOHOLIC_neit  4432\nOFTEN_DRINK_ALCOHOLIC_dgli  4479\nOFTEN_DRINK_ALCOHOLIC_dglo  4526\nOFTEN_DRINK_ALCOHOLIC_anyd  4573\nFIRST_TRY_NEW_FOOD_aglo 4310\nFIRST_TRY_NEW_FOOD_agli 4357\nFIRST_TRY_NEW_FOOD_anya 4404\nFIRST_TRY_NEW_FOOD_neit 4451\nFIRST_TRY_NEW_FOOD_dgli 4498\nFIRST_TRY_NEW_FOOD_dglo 4545\nFIRST_TRY_NEW_FOOD_anyd 4592\nGUILTY_TO_EAT_SWEETS_aglo 4282\nGUILTY_TO_EAT_SWEETS_agli   4329\nGUILTY_TO_EAT_SWEETS_anya   4376\nGUILTY_TO_EAT_SWEETS_neit   4423\nGUILTY_TO_EAT_SWEETS_dgli   4470\nGUILTY_TO_EAT_SWEETS_dglo   4517\nGUILTY_TO_EAT_SWEETS_anyd   4564\n\n/*SECOND Abstract - Enviromental */\nPEOPLE_DUTY_RECYCLE_aglo    4192\nPEOPLE_DUTY_RECYCLE_agli    4206\nPEOPLE_DUTY_RECYCLE_anya    4220\nPEOPLE_DUTY_RECYCLE_neit    4234\nPEOPLE_DUTY_RECYCLE_dgli    4248\nPEOPLE_DUTY_RECYCLE_dglo    4262\nPEOPLE_DUTY_RECYCLE_anyd    4276\nMAKE_EFFORT_RECYCLE_aglo    4189\nMAKE_EFFORT_RECYCLE_agli    4203\nMAKE_EFFORT_RECYCLE_anya    4217\nMAKE_EFFORT_RECYCLE_neit    4231\nMAKE_EFFORT_RECYCLE_dgli    4245\nMAKE_EFFORT_RECYCLE_dglo    4259\nMAKE_EFFORT_RECYCLE_anyd    4273\nRESPONS_RECYCLD_PRD_aglo    4191\nRESPONS_RECYCLD_PRD_agli    4205\nRESPONS_RECYCLD_PRD_anya    4219\nRESPONS_RECYCLD_PRD_neit    4233\nRESPONS_RECYCLD_PRD_dgli    4247\nRESPONS_RECYCLD_PRD_dglo    4261\nRESPONS_RECYCLD_PRD_anyd    4275\nENVRNMNT_RESPONSIBLE_aglo   4184\nENVRNMNT_RESPONSIBLE_agli   4198\nENVRNMNT_RESPONSIBLE_anya   4212\nENVRNMNT_RESPONSIBLE_neit   4226\nENVRNMNT_RESPONSIBLE_dgli   4240\nENVRNMNT_RESPONSIBLE_dglo   4254\nENVRNMNT_RESPONSIBLE_anyd   4268\n\n/*Descriptor Variables */ \n\n/*Target Variable */\nDr_Pepper 39807\n\n/* Major Competitor */\nCoca_Cola   40127\nSprite      39830\n\n\n/*Demographics*/\n\nMALE    2383\nFEMALE  2384\n\nNORTHEAST   3075\nMIDWEST 3076\nSOUTH   3077\nWEST    3078\n\nAge_18_24   2401\nAge_25_49   2407\nAge_50  2415\n\n/*Media Variables*/\n\nSPOTIFY 8184\nYOUTUBE 8978\nESPN    9625\n\n\n\n\n\n\n\n;\n\n/* the above reads in the raw data from the data file -  now create five point scale variables */\n/* now before we create variables lets create formats so we know what each value will mean */\n\nproc format;\nvalue myscale\n     1 = \"disagree a lot\"\n     2 = \"disagree a little\"\n     3 = \"neither agree nor disagree\"\n     4 = \"agree a little\"\n     5 = \"agree a lot\";\nvalue yesno\n     0 = \"no\"\n     1 = \"yes\";\n\n\n\n\n/* do that by creating a new temp sas data set myvars by starting with the temp sas data set mytemp */\ndata myvars;\nset mytemp;\n\n/*Driver Variables */\nif snack_between_meals_dglo = 1 then drink_between_meals = 1;                                                  \nif snack_between_meals_dgli = 1 then drink_between_meals = 2;                                             \nif snack_between_meals_neit = 1 then drink_between_meals = 3;                                                  \nif snack_between_meals_agli = 1 then drink_between_meals = 4;                                                  \nif snack_between_meals_aglo = 1 then drink_between_meals = 5;     \nif try_new_drinks_dglo = 1 then like_to_try_new_drinks = 1;                                                 \nif try_new_drinks_dgli = 1 then like_to_try_new_drinks = 2;                                                \nif try_new_drinks_neit = 1 then like_to_try_new_drinks = 3;    \nif try_new_drinks_agli = 1 then like_to_try_new_drinks = 4;                                                 \nif try_new_drinks_aglo = 1 then like_to_try_new_drinks = 5;   \nif see_on_tv_go_online_more_dglo = 1 then when_on_tv_go_online_get_more = 1;                                                 \nif see_on_tv_go_online_more_dgli = 1 then when_on_tv_go_online_get_more = 2;                                                \nif see_on_tv_go_online_more_neit = 1 then when_on_tv_go_online_get_more = 3;    \nif see_on_tv_go_online_more_agli = 1 then when_on_tv_go_online_get_more = 4;                                                 \nif see_on_tv_go_online_more_aglo = 1 then when_on_tv_go_online_get_more = 5;  \nif buy_online_or_store_dglo = 1 then buy_online_or_in_store = 1;                                                 \nif buy_online_or_store_dgli = 1 then buy_online_or_in_store = 2;                                                \nif buy_online_or_store_neit = 1 then buy_online_or_in_store = 3;    \nif buy_online_or_store_agli = 1 then buy_online_or_in_store = 4;                                                 \nif buy_online_or_store_aglo = 1 then buy_online_or_in_store = 5;  \n/*First Abstract - indulgence */\nif DRINKING_TO_GET_DRUNK_dglo = 1   then  DRINKING_GET_DRUNK = 1;\nif DRINKING_TO_GET_DRUNK_dgli = 1   then  DRINKING_GET_DRUNK = 2;\nif DRINKING_TO_GET_DRUNK_neit = 1   then  DRINKING_GET_DRUNK = 3;\nif DRINKING_TO_GET_DRUNK_agli = 1   then  DRINKING_GET_DRUNK = 4;\nif DRINKING_TO_GET_DRUNK_aglo = 1   then  DRINKING_GET_DRUNK = 5;\nif OFTEN_DRINK_ALCOHOLIC_dglo = 1   then  OFTEN_DRINK_ALCOHOL = 1;\nif OFTEN_DRINK_ALCOHOLIC_dgli = 1   then  OFTEN_DRINK_ALCOHOL = 2;\nif OFTEN_DRINK_ALCOHOLIC_neit = 1   then  OFTEN_DRINK_ALCOHOL = 3;\nif OFTEN_DRINK_ALCOHOLIC_agli = 1   then  OFTEN_DRINK_ALCOHOL = 4;\nif OFTEN_DRINK_ALCOHOLIC_aglo = 1   then  OFTEN_DRINK_ALCOHOL = 5;\nif FIRST_TRY_NEW_FOOD_dglo = 1  then  TRY_NEW_FOOD_PRDCT = 1;\nif FIRST_TRY_NEW_FOOD_dgli = 1  then  TRY_NEW_FOOD_PRDCT = 2;\nif FIRST_TRY_NEW_FOOD_neit = 1  then  TRY_NEW_FOOD_PRDCT = 3;\nif FIRST_TRY_NEW_FOOD_agli = 1  then  TRY_NEW_FOOD_PRDCT = 4;\nif FIRST_TRY_NEW_FOOD_aglo = 1  then  TRY_NEW_FOOD_PRDCT = 5;\nif GUILTY_TO_EAT_SWEETS_dglo = 1    then  FEEL_GUILTY_SWEETS = 1;\nif GUILTY_TO_EAT_SWEETS_dgli = 1    then  FEEL_GUILTY_SWEETS = 2;\nif GUILTY_TO_EAT_SWEETS_neit = 1    then  FEEL_GUILTY_SWEETS = 3;\nif GUILTY_TO_EAT_SWEETS_agli = 1    then  FEEL_GUILTY_SWEETS = 4;\nif GUILTY_TO_EAT_SWEETS_aglo = 1    then  FEEL_GUILTY_SWEETS = 5;\n/*SECOND Abstract - Enviromental */\nif PEOPLE_DUTY_RECYCLE_dglo = 1 then PEOPLE_NEED_TO_RECYCLE = 1;                                                 \nif PEOPLE_DUTY_RECYCLE_dgli = 1 then PEOPLE_NEED_TO_RECYCLE = 2;                                                \nif PEOPLE_DUTY_RECYCLE_neit = 1 then PEOPLE_NEED_TO_RECYCLE = 3;    \nif PEOPLE_DUTY_RECYCLE_agli = 1 then PEOPLE_NEED_TO_RECYCLE = 4;                                                 \nif PEOPLE_DUTY_RECYCLE_aglo = 1 then PEOPLE_NEED_TO_RECYCLE = 5; \nif MAKE_EFFORT_RECYCLE_dglo = 1 then  MAKE_EFFORT_TO_RECYCLE = 1;\nif MAKE_EFFORT_RECYCLE_dgli = 1 then  MAKE_EFFORT_TO_RECYCLE = 2;\nif MAKE_EFFORT_RECYCLE_neit = 1 then  MAKE_EFFORT_TO_RECYCLE = 3;\nif MAKE_EFFORT_RECYCLE_agli = 1 then  MAKE_EFFORT_TO_RECYCLE = 4;\nif MAKE_EFFORT_RECYCLE_aglo = 1 then  MAKE_EFFORT_TO_RECYCLE = 5;\nif RESPONS_RECYCLD_PRD_dglo = 1 then  PEOPLE_RESPONS_TO_RECYCLD_PRDCTS = 1;\nif RESPONS_RECYCLD_PRD_dgli = 1 then  PEOPLE_RESPONS_TO_RECYCLD_PRDCTS = 2;\nif RESPONS_RECYCLD_PRD_neit = 1 then  PEOPLE_RESPONS_TO_RECYCLD_PRDCTS = 3;\nif RESPONS_RECYCLD_PRD_agli = 1 then  PEOPLE_RESPONS_TO_RECYCLD_PRDCTS = 4;\nif RESPONS_RECYCLD_PRD_aglo = 1 then  PEOPLE_RESPONS_TO_RECYCLD_PRDCTS = 5;\nif ENVRNMNT_RESPONSIBLE_dglo = 1    then  PERSONAL_ENVRNMNT_RESPONSIBLE = 1;\nif ENVRNMNT_RESPONSIBLE_dgli = 1    then  PERSONAL_ENVRNMNT_RESPONSIBLE = 2;\nif ENVRNMNT_RESPONSIBLE_neit = 1    then  PERSONAL_ENVRNMNT_RESPONSIBLE = 3;\nif ENVRNMNT_RESPONSIBLE_agli = 1    then  PERSONAL_ENVRNMNT_RESPONSIBLE = 4;\nif ENVRNMNT_RESPONSIBLE_aglo = 1    then  PERSONAL_ENVRNMNT_RESPONSIBLE = 5;\n\n\n/* now set up binary yes   no variables knowing that missing values get a zero and a 1 gets a 1 */\n\n/*Descriptor Variables*/\n\n/*Target Variable*/\nif Dr_Pepper = .  then Dr_Pepper = 0;\nif Dr_Pepper = 1 then Dr_Pepper = 1;\n/*Competitor*/\nif Coca_Cola = .  then Coca_Cola = 0;\nif Coca_Cola = 1 then Coca_Cola = 1;\nif Sprite = .  then Sprite = 0;\nif Sprite = 1 then Sprite = 1;\n/*Demographic*/\nif MALE  = .  then Male = 0;\nif MALE  = 1 then Male = 1;\nif FEMALE = .  then Female = 0;\nif FEMALE = 1 then Female = 1;\nif NORTHEAST = .  then NorthEast = 0;\nif NORTHEAST = 1 then NorthEast = 1;\nif MIDWEST = .  then MidWest = 0;\nif MIDWEST = 1 then MidWest = 1;\nif SOUTH = .  then South = 0;\nif SOUTH = 1 then South = 1;\nif WEST = .  then West = 0;\nif WEST = 1 then West = 1;\nif Age_18_24 = .  then Age_18_24 = 0;\nif Age_18_24 = 1 then Age_18_24 = 1;\nif Age_25_49 = .  then Age_25_49 = 0;\nif Age_25_49 = 1 then Age_25_49 = 1;\nif Age_50 = .  then Age_50 = 0;\nif Age_50 = 1 then Age_50 = 1;\n/*Media*/\nif SPOTIFY = .  then Spotify = 0;\nif SPOTIFY = 1 then Spotify = 1;\nif YOUTUBE   = .  then YouTube = 0;\nif YOUTUBE   = 1 then YouTube = 1;\nif ESPN = .  then ESPN = 0;\nif ESPN = 1 then ESPN = 1;\n\n\n\n/* Assign labels to variables */\n\nlabel drink_between_meals = 'I often drink between meals';\nlabel like_to_try_new_drinks ='I like to try new drinks';\nlabel when_on_tv_go_online_get_more = 'When I see on tv I go online and find more';\nlabel buy_online_or_in_store ='More likely to buy online than in store';\n\n\nlabel DRINKING_GET_DRUNK = 'The point of drinking is to get drunk';\nlabel OFTEN_DRINK_ALCOHOL ='Often drink alcoholic beverages at resturants';\nlabel TRY_NEW_FOOD_PRDCT = 'Im usually first to try new food products';\nlabel FEEL_GUILTY_SWEETS ='I feel guilty when I eat sweets';\n\n\nlabel PEOPLE_NEED_TO_RECYCLE = 'People have a duty to recycle';\nlabel MAKE_EFFORT_TO_RECYCLE ='I make an effort to recycle';\nlabel PEOPLE_RESPONS_TO_RECYCLD_PRDCTS = 'People have a response to use recycle products';\nlabel PERSONAL_ENVRNMNT_RESPONSIBLE ='Personal obligation and enviroment responsibility';\n\n\nlabel Dr_Pepper = 'I prefer DrPepper';\n\nlabel Coca_Cola ='I prefer CocaCola';\nlabel Sprite ='I prefer Sprite';\n\nlabel Male = 'Response by Male';\nlabel Female ='Response by Female';\n\nlabel NorthEast = 'Region: NorthEast';\nlabel MidWest ='Region: Mid-West';\nlabel South = 'Region: South';\nlabel West ='Region: West';\nlabel Age_18_24 = 'Age: 18-24';\nlabel Age_25_49 ='Age: 25-49';\nlabel Age_50 = 'Age: 50+';\n\nlabel Spotify ='Spotify was used during last 7 days';\nlabel YouTube = 'YouTube was used during last 7 days';\nlabel ESPN ='ESPN was viewed during last 7 days';\n\n\n\n\n\n\n/* now attach the values for each of the variables using the proc format labels */\n\nformat\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store\n\nDRINKING_GET_DRUNK\nOFTEN_DRINK_ALCOHOL\nTRY_NEW_FOOD_PRDCT\nFEEL_GUILTY_SWEETS\n\nPEOPLE_NEED_TO_RECYCLE\nMAKE_EFFORT_TO_RECYCLE\nPEOPLE_RESPONS_TO_RECYCLD_PRDCTS\nPERSONAL_ENVRNMNT_RESPONSIBLE\nmyscale.\n\nDr_Pepper\nCoca_Cola\nSprite \nMALE\nFEMALE \nNORTHEAST\nMIDWEST \nSOUTH \nWEST\nAge_18_24\nAge_25_49 \nAge_50\nSPOTIFY\nYOUTUBE\nESPN\nyesno. \n\n\nrun;\n\n/* now run freqs to check your work */\nproc freq data = myvars;\ntables\nsnack_between_meals_dglo                                                  \nsnack_between_meals_dgli                                             \nsnack_between_meals_neit                                                  \nsnack_between_meals_agli                                                  \nsnack_between_meals_aglo     \ntry_new_drinks_dglo                                                \ntry_new_drinks_dgli                                              \ntry_new_drinks_neit   \ntry_new_drinks_agli                                                \ntry_new_drinks_aglo   \nsee_on_tv_go_online_more_dglo                                                 \nsee_on_tv_go_online_more_dgli                                                \nsee_on_tv_go_online_more_neit    \nsee_on_tv_go_online_more_agli                                                 \nsee_on_tv_go_online_more_aglo  \nbuy_online_or_store_dglo                                            \nbuy_online_or_store_dgli                                                \nbuy_online_or_store_neit    \nbuy_online_or_store_agli                                                 \nbuy_online_or_store_aglo\n\nDRINKING_TO_GET_DRUNK_aglo  \nDRINKING_TO_GET_DRUNK_agli  \nDRINKING_TO_GET_DRUNK_anya  \nDRINKING_TO_GET_DRUNK_neit  \nDRINKING_TO_GET_DRUNK_dgli  \nDRINKING_TO_GET_DRUNK_dglo  \nDRINKING_TO_GET_DRUNK_anyd  \nOFTEN_DRINK_ALCOHOLIC_aglo  \nOFTEN_DRINK_ALCOHOLIC_agli  \nOFTEN_DRINK_ALCOHOLIC_anya  \nOFTEN_DRINK_ALCOHOLIC_neit  \nOFTEN_DRINK_ALCOHOLIC_dgli  \nOFTEN_DRINK_ALCOHOLIC_dglo  \nOFTEN_DRINK_ALCOHOLIC_anyd  \nFIRST_TRY_NEW_FOOD_aglo \nFIRST_TRY_NEW_FOOD_agli \nFIRST_TRY_NEW_FOOD_anya \nFIRST_TRY_NEW_FOOD_neit \nFIRST_TRY_NEW_FOOD_dgli \nFIRST_TRY_NEW_FOOD_dglo \nFIRST_TRY_NEW_FOOD_anyd \nGUILTY_TO_EAT_SWEETS_aglo \nGUILTY_TO_EAT_SWEETS_agli   \nGUILTY_TO_EAT_SWEETS_anya   \nGUILTY_TO_EAT_SWEETS_neit   \nGUILTY_TO_EAT_SWEETS_dgli   \nGUILTY_TO_EAT_SWEETS_dglo   \nGUILTY_TO_EAT_SWEETS_anyd\n\nPEOPLE_DUTY_RECYCLE_dglo                                                 \nPEOPLE_DUTY_RECYCLE_dgli                                              \nPEOPLE_DUTY_RECYCLE_neit  \nPEOPLE_DUTY_RECYCLE_agli                                                 \nPEOPLE_DUTY_RECYCLE_aglo\nMAKE_EFFORT_RECYCLE_dglo\nMAKE_EFFORT_RECYCLE_dgli \nMAKE_EFFORT_RECYCLE_neit\nMAKE_EFFORT_RECYCLE_agli\nMAKE_EFFORT_RECYCLE_aglo\nRESPONS_RECYCLD_PRD_dglo\nRESPONS_RECYCLD_PRD_dgli\nRESPONS_RECYCLD_PRD_neit\nRESPONS_RECYCLD_PRD_agli\nRESPONS_RECYCLD_PRD_aglo\nENVRNMNT_RESPONSIBLE_dglo\nENVRNMNT_RESPONSIBLE_dgli\nENVRNMNT_RESPONSIBLE_neit\nENVRNMNT_RESPONSIBLE_agli\nENVRNMNT_RESPONSIBLE_aglo \n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store\n\nDRINKING_GET_DRUNK\nOFTEN_DRINK_ALCOHOL\nTRY_NEW_FOOD_PRDCT\nFEEL_GUILTY_SWEETS\n\nPEOPLE_NEED_TO_RECYCLE\nMAKE_EFFORT_TO_RECYCLE\nPEOPLE_RESPONS_TO_RECYCLD_PRDCTS\nPERSONAL_ENVRNMNT_RESPONSIBLE\n\nDr_Pepper\nCoca_Cola\nSprite\nMale\nFemale \nNorthEast\nMidWest \nSouth \nWest \nAge_18_24 \nAge_25_49 \nAge_50\nSpotify \nYouTube \nESPN;\n\n/* K-MEANS STARTS */\n \nPROC FACTOR DATA = myvars \nMAXITER=100\nMETHOD=principal\nMINEIGEN=1\nROTATE=varimax\nMSA\nSCREE\nSCORE\nPRINT\nNFACTORS=2\nOUT=myscores;\nvar\nDRINKING_GET_DRUNK\nOFTEN_DRINK_ALCOHOL\nTRY_NEW_FOOD_PRDCT\nFEEL_GUILTY_SWEETS\n\nPEOPLE_NEED_TO_RECYCLE \nMAKE_EFFORT_TO_RECYCLE \nPEOPLE_RESPONS_TO_RECYCLD_PRDCTS \nPERSONAL_ENVRNMNT_RESPONSIBLE;\nrun;\n\n\nDATA myscores1;\nSET myscores;\nRENAME factor1 = IndulgenceFactors;\nRENAME factor2 = GreenAttitudeFactor;\nrun;\n\n\nPROC FASTCLUS DATA=myscores1 MAXITER=100 MAXCLUSTERS=3 OUT=finalclus;\nVAR\nIndulgenceFactors\nGreenAttitudeFactor\n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store;\nRUN;\nPROC FASTCLUS DATA=myscores1 MAXITER=100 MAXCLUSTERS=4 OUT=finalclus;\nVAR\nIndulgenceFactors\nGreenAttitudeFactor\n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store;\nRUN;\nPROC FASTCLUS DATA=myscores1 MAXITER=100 MAXCLUSTERS=5 OUT=finalclus;\nVAR\nIndulgenceFactors\nGreenAttitudeFactor\n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store;\nRUN;\nPROC FASTCLUS DATA=myscores1 MAXITER=100 MAXCLUSTERS=6 OUT=finalclus;\nVAR\nIndulgenceFactors\nGreenAttitudeFactor\n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store;\nRUN;\nPROC FASTCLUS DATA=myscores1 MAXITER=100 MAXCLUSTERS=7 OUT=finalclus;\nVAR\nIndulgenceFactors\nGreenAttitudeFactor\n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store;\nRUN;\nPROC FASTCLUS DATA=myscores1 MAXITER=100 MAXCLUSTERS=8 OUT=finalclus;\nVAR\nIndulgenceFactors\nGreenAttitudeFactor\n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store;\nRUN;\n\nPROC FASTCLUS DATA=myscores1 MAXITER=100 MAXCLUSTERS=9 OUT=finalclus;\nVAR\nIndulgenceFactors\nGreenAttitudeFactor\n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store;\nRUN;     /* K-MEANS ENDS */\n\n\n\n/* GAP ANALYSIS STARTS */\n\n\n/*proc hpclus data=myscores1 MAXITER=100 maxclusters=6\nnoc=abc(b=20 minclusters=2 align=pca criterion=globalpeak);\n        /* score puts CLUSTER variable in dataset and OUT= outputs the data set */\n/*score out=mycluster;\n        /* here are the drivers for the HPCLUS cluster solution */\n/*input \nIndulgenceFactors\nGreenAttitudeFactor\n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store / level=interval;\nrun;\n\n\n        /* only the variables listed in the ID statement will be kept in the MYCLUSTER data set*/\n        /* so be sure to put your id variable, drivers and descritpor variables in the ID statement */\n        /* the drivers are earlyadopt socialphone loser ad_receptivity and the descriptors are coca_cola_classic \npepsi_classic \nespn_sports  \nikea_furniture  \nkfc_chicken  \nnike_trainers  */\n\n\n/*id myid\n\nIndulgenceFactors\nGreenAttitudeFactor\n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store\n\n        /*IndulgenceFactors\nDRINKING_GET_DRUNK\nOFTEN_DRINK_ALCOHOL\nTRY_NEW_FOOD_PRDCT\nFEEL_GUILTY_SWEETS\n\nGreenAttitudeFactor\nPEOPLE_NEED_TO_RECYCLE\nMAKE_EFFORT_TO_RECYCLE\nPEOPLE_RESPONS_TO_RECYCLD_PRDCTS\nPERSONAL_ENVRNMNT_RESPONSIBLE*/\n\n/*dr_pepper\nSprite\nMALE\nFEMALE \nNORTHEAST\nSOUTH \nWEST\nAge_18_24\nAge_25_49 \nAge_50\nYOUTUBE\nESPN;\nrun;                                        \n\nproc contents data=mycluster;\nrun; \nproc sort data=mycluster out=mysort;\nby _CLUSTER_ID_ ;\nproc means data=mysort;\nby _CLUSTER_ID_;\nvar\n\nIndulgenceFactors\nGreenAttitudeFactor\n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store\n\n        /*IndulgenceFactors\nDRINKING_GET_DRUNK\nOFTEN_DRINK_ALCOHOL\nTRY_NEW_FOOD_PRDCT\nFEEL_GUILTY_SWEETS\n\nGreenAttitudeFactor\nPEOPLE_NEED_TO_RECYCLE\nMAKE_EFFORT_TO_RECYCLE\nPEOPLE_RESPONS_TO_RECYCLD_PRDCTS\nPERSONAL_ENVRNMNT_RESPONSIBLE*/\n\n/*Sprite\nMALE\nFEMALE \nNORTHEAST\nSOUTH \nWEST\nAge_18_24\nAge_25_49 \nAge_50\nYOUTUBE\nESPN;\nrun;   /* GAP ANALYSIS ENDS */\n\n\n\n\n\n\n\n\n\n\n\n\n\n/* I found k=3 as my first local maximum */\n/* so lets run it again and save the cluster file */\nproc fastclus data=myscores1 out=cluster_results maxiter=100 maxclusters=3;\nvar \n\nIndulgenceFactors\nGreenAttitudeFactor\n\ndrink_between_meals\nlike_to_try_new_drinks\nwhen_on_tv_go_online_get_more\nbuy_online_or_in_store;\nrun;                             \n                                   \n/* now we need to sort the data set by cluster to use the BY option in proc means */\nproc sort data=cluster_results  out=cluster_sorted;\nBy cluster;\nrun;\n\n/* now we can produce means for some descriptor variables */\nproc means data=cluster_sorted;\nBy cluster;\nvar\nDr_Pepper\nCoca_Cola\nSprite\nMALE\nFEMALE \nNORTHEAST\nSOUTH \nAge_18_24\nAge_50\nYOUTUBE\nSpotify;\nrun;\n\n\n\nSAS Code Output:\n\n\n\nThe SAS System\n\n\n\nThe FREQ Procedure\n\n\n\n\n\n\n\n\n\n\nsnack_between_meals_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n1919\n100.00\n1919\n100.00\n\n\nFrequency Missing = 23520\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsnack_between_meals_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n2526\n100.00\n2526\n100.00\n\n\nFrequency Missing = 22913\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsnack_between_meals_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n3409\n100.00\n3409\n100.00\n\n\nFrequency Missing = 22030\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsnack_between_meals_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n9817\n100.00\n9817\n100.00\n\n\nFrequency Missing = 15622\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsnack_between_meals_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6582\n100.00\n6582\n100.00\n\n\nFrequency Missing = 18857\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry_new_drinks_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n5504\n100.00\n5504\n100.00\n\n\nFrequency Missing = 19935\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry_new_drinks_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n3428\n100.00\n3428\n100.00\n\n\nFrequency Missing = 22011\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry_new_drinks_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6830\n100.00\n6830\n100.00\n\n\nFrequency Missing = 18609\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry_new_drinks_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n5040\n100.00\n5040\n100.00\n\n\nFrequency Missing = 20399\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntry_new_drinks_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n3259\n100.00\n3259\n100.00\n\n\nFrequency Missing = 22180\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsee_on_tv_go_online_more_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n4997\n100.00\n4997\n100.00\n\n\nFrequency Missing = 20442\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsee_on_tv_go_online_more_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n2420\n100.00\n2420\n100.00\n\n\nFrequency Missing = 23019\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsee_on_tv_go_online_more_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6244\n100.00\n6244\n100.00\n\n\nFrequency Missing = 19195\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsee_on_tv_go_online_more_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6783\n100.00\n6783\n100.00\n\n\nFrequency Missing = 18656\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsee_on_tv_go_online_more_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n3502\n100.00\n3502\n100.00\n\n\nFrequency Missing = 21937\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbuy_online_or_store_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6591\n100.00\n6591\n100.00\n\n\nFrequency Missing = 18848\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbuy_online_or_store_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n4045\n100.00\n4045\n100.00\n\n\nFrequency Missing = 21394\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbuy_online_or_store_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6541\n100.00\n6541\n100.00\n\n\nFrequency Missing = 18898\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbuy_online_or_store_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n4251\n100.00\n4251\n100.00\n\n\nFrequency Missing = 21188\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbuy_online_or_store_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n2705\n100.00\n2705\n100.00\n\n\nFrequency Missing = 22734\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDRINKING_TO_GET_DRUNK_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n1060\n100.00\n1060\n100.00\n\n\nFrequency Missing = 24379\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDRINKING_TO_GET_DRUNK_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n1443\n100.00\n1443\n100.00\n\n\nFrequency Missing = 23996\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDRINKING_TO_GET_DRUNK_anya\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n2503\n100.00\n2503\n100.00\n\n\nFrequency Missing = 22936\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDRINKING_TO_GET_DRUNK_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n4201\n100.00\n4201\n100.00\n\n\nFrequency Missing = 21238\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDRINKING_TO_GET_DRUNK_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n2622\n100.00\n2622\n100.00\n\n\nFrequency Missing = 22817\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDRINKING_TO_GET_DRUNK_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n14730\n100.00\n14730\n100.00\n\n\nFrequency Missing = 10709\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDRINKING_TO_GET_DRUNK_anyd\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n17352\n100.00\n17352\n100.00\n\n\nFrequency Missing = 8087\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOFTEN_DRINK_ALCOHOLIC_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n2635\n100.00\n2635\n100.00\n\n\nFrequency Missing = 22804\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOFTEN_DRINK_ALCOHOLIC_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n3944\n100.00\n3944\n100.00\n\n\nFrequency Missing = 21495\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOFTEN_DRINK_ALCOHOLIC_anya\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6579\n100.00\n6579\n100.00\n\n\nFrequency Missing = 18860\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOFTEN_DRINK_ALCOHOLIC_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n3844\n100.00\n3844\n100.00\n\n\nFrequency Missing = 21595\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOFTEN_DRINK_ALCOHOLIC_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n2923\n100.00\n2923\n100.00\n\n\nFrequency Missing = 22516\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOFTEN_DRINK_ALCOHOLIC_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n10655\n100.00\n10655\n100.00\n\n\nFrequency Missing = 14784\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOFTEN_DRINK_ALCOHOLIC_anyd\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n13578\n100.00\n13578\n100.00\n\n\nFrequency Missing = 11861\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST_TRY_NEW_FOOD_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n1981\n100.00\n1981\n100.00\n\n\nFrequency Missing = 23458\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST_TRY_NEW_FOOD_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n3335\n100.00\n3335\n100.00\n\n\nFrequency Missing = 22104\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST_TRY_NEW_FOOD_anya\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n5316\n100.00\n5316\n100.00\n\n\nFrequency Missing = 20123\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST_TRY_NEW_FOOD_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n8873\n100.00\n8873\n100.00\n\n\nFrequency Missing = 16566\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST_TRY_NEW_FOOD_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n4584\n100.00\n4584\n100.00\n\n\nFrequency Missing = 20855\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST_TRY_NEW_FOOD_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n5257\n100.00\n5257\n100.00\n\n\nFrequency Missing = 20182\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFIRST_TRY_NEW_FOOD_anyd\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n9841\n100.00\n9841\n100.00\n\n\nFrequency Missing = 15598\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUILTY_TO_EAT_SWEETS_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n3990\n100.00\n3990\n100.00\n\n\nFrequency Missing = 21449\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUILTY_TO_EAT_SWEETS_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6231\n100.00\n6231\n100.00\n\n\nFrequency Missing = 19208\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUILTY_TO_EAT_SWEETS_anya\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n10221\n100.00\n10221\n100.00\n\n\nFrequency Missing = 15218\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUILTY_TO_EAT_SWEETS_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6081\n100.00\n6081\n100.00\n\n\nFrequency Missing = 19358\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUILTY_TO_EAT_SWEETS_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n3700\n100.00\n3700\n100.00\n\n\nFrequency Missing = 21739\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUILTY_TO_EAT_SWEETS_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n3897\n100.00\n3897\n100.00\n\n\nFrequency Missing = 21542\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGUILTY_TO_EAT_SWEETS_anyd\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n7597\n100.00\n7597\n100.00\n\n\nFrequency Missing = 17842\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPEOPLE_DUTY_RECYCLE_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n658\n100.00\n658\n100.00\n\n\nFrequency Missing = 24781\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPEOPLE_DUTY_RECYCLE_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n910\n100.00\n910\n100.00\n\n\nFrequency Missing = 24529\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPEOPLE_DUTY_RECYCLE_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n5955\n100.00\n5955\n100.00\n\n\nFrequency Missing = 19484\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPEOPLE_DUTY_RECYCLE_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6201\n100.00\n6201\n100.00\n\n\nFrequency Missing = 19238\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPEOPLE_DUTY_RECYCLE_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n10597\n100.00\n10597\n100.00\n\n\nFrequency Missing = 14842\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAKE_EFFORT_RECYCLE_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n1082\n100.00\n1082\n100.00\n\n\nFrequency Missing = 24357\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAKE_EFFORT_RECYCLE_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n1195\n100.00\n1195\n100.00\n\n\nFrequency Missing = 24244\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAKE_EFFORT_RECYCLE_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n3756\n100.00\n3756\n100.00\n\n\nFrequency Missing = 21683\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAKE_EFFORT_RECYCLE_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n5406\n100.00\n5406\n100.00\n\n\nFrequency Missing = 20033\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMAKE_EFFORT_RECYCLE_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n13114\n100.00\n13114\n100.00\n\n\nFrequency Missing = 12325\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRESPONS_RECYCLD_PRD_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n507\n100.00\n507\n100.00\n\n\nFrequency Missing = 24932\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRESPONS_RECYCLD_PRD_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n791\n100.00\n791\n100.00\n\n\nFrequency Missing = 24648\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRESPONS_RECYCLD_PRD_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6176\n100.00\n6176\n100.00\n\n\nFrequency Missing = 19263\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRESPONS_RECYCLD_PRD_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n6971\n100.00\n6971\n100.00\n\n\nFrequency Missing = 18468\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRESPONS_RECYCLD_PRD_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n9884\n100.00\n9884\n100.00\n\n\nFrequency Missing = 15555\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nENVRNMNT_RESPONSIBLE_dglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n442\n100.00\n442\n100.00\n\n\nFrequency Missing = 24997\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nENVRNMNT_RESPONSIBLE_dgli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n550\n100.00\n550\n100.00\n\n\nFrequency Missing = 24889\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nENVRNMNT_RESPONSIBLE_neit\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n4847\n100.00\n4847\n100.00\n\n\nFrequency Missing = 20592\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nENVRNMNT_RESPONSIBLE_agli\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n8332\n100.00\n8332\n100.00\n\n\nFrequency Missing = 17107\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nENVRNMNT_RESPONSIBLE_aglo\nFrequency\nPercent\nCumulative\nFrequency\nCumulative\nPercent\n\n\n\n\n1\n10152\n100.00\n10152\n100.00\n\n\nFrequency Missing = 15287\n\n\n\n\n\n\n\n\n\n\nI often drink between meals\n\n\n\n\ndrink_between_meals\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 1186\n\n\n\n\n\n\nI like to try new drinks\n\n\n\n\nlike_to_try_new_drinks\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 1378\n\n\n\n\n\n\nWhen I see on tv I go online and find more\n\n\n\n\nwhen_on_tv_go_online_get_more\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 1493\n\n\n\n\n\n\nMore likely to buy online than in store\n\n\n\n\nbuy_online_or_in_store\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 1306\n\n\n\n\n\n\nThe point of drinking is to get drunk\n\n\n\n\nDRINKING_GET_DRUNK\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 1383\n\n\n\n\n\n\nOften drink alcoholic beverages at resturants\n\n\n\n\nOFTEN_DRINK_ALCOHOL\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 1438\n\n\n\n\n\n\nIm usually first to try new food products\n\n\n\n\nTRY_NEW_FOOD_PRDCT\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 1409\n\n\n\n\n\n\nI feel guilty when I eat sweets\n\n\n\n\nFEEL_GUILTY_SWEETS\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 1540\n\n\n\n\n\n\nPeople have a duty to recycle\n\n\n\n\nPEOPLE_NEED_TO_RECYCLE\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 1118\n\n\n\n\n\n\nI make an effort to recycle\n\n\n\n\nMAKE_EFFORT_TO_RECYCLE\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 886\n\n\n\n\n\n\nPeople have a response to use recycle products\n\n\n\n\nPEOPLE_RESPONS_TO_RECYCLD_PRDCTS\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 1110\n\n\n\n\n\n\nPersonal obligation and enviroment responsibility\n\n\n\n\nPERSONAL_ENVRNMNT_RESPONSIBLE\n\n\ndisagree a lot\n\n\ndisagree a little\n\n\nneither agree nor disagree\n\n\nagree a little\n\n\nagree a lot\n\n\nFrequency Missing = 1116\n\n\n\n\n\n\nI prefer DrPepper\n\n\n\n\nDr_Pepper\n\n\nno\n\n\nyes\n\n\n\n\n\n\nI prefer CocaCola\n\n\n\n\nCoca_Cola\n\n\nno\n\n\nyes\n\n\n\n\n\n\nI prefer Sprite\n\n\n\n\nSprite\n\n\nno\n\n\nyes\n\n\n\n\n\n\nResponse by Male\n\n\n\n\nMALE\n\n\nno\n\n\nyes\n\n\n\n\n\n\nResponse by Female\n\n\n\n\nFEMALE\n\n\nno\n\n\nyes\n\n\n\n\n\n\nRegion: NorthEast\n\n\n\n\nNORTHEAST\n\n\nno\n\n\nyes\n\n\n\n\n\n\nRegion: Mid-West\n\n\n\n\nMIDWEST\n\n\nno\n\n\nyes\n\n\n\n\n\n\nRegion: South\n\n\n\n\nSOUTH\n\n\nno\n\n\nyes\n\n\n\n\n\n\nRegion: West\n\n\n\n\nWEST\n\n\nno\n\n\nyes\n\n\n\n\n\n\nAge: 18-24\n\n\n\n\nAge_18_24\n\n\nno\n\n\nyes\n\n\n\n\n\n\nAge: 25-49\n\n\n\n\nAge_25_49\n\n\nno\n\n\nyes\n\n\n\n\n\n\nAge: 50+\n\n\n\n\nAge_50\n\n\nno\n\n\nyes\n\n\n\n\n\n\nSpotify was used during last 7 days\n\n\n\n\nSPOTIFY\n\n\nno\n\n\nyes\n\n\n\n\n\n\nYouTube was used during last 7 days\n\n\n\n\nYOUTUBE\n\n\nno\n\n\nyes\n\n\n\n\n\n\nESPN was viewed during last 7 days\n\n\n\n\nESPN\n\n\nno\n\n\nyes\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FACTOR Procedure\n\n\n\nInput Data Type\nRaw Data\n\n\n\n\nNumber of Records Read\n25439\n\n\nNumber of Records Used\n21942\n\n\nN for Significance Tests\n21942\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FACTOR Procedure\nInitial Factor Method: Principal Components\n\n\n\nPartial Correlations Controlling all other Variables\n\n\n\n\n\n\n\nDRINKING_GET_DRUNK\n\n\nOFTEN_DRINK_ALCOHOL\n\n\nTRY_NEW_FOOD_PRDCT\n\n\nFEEL_GUILTY_SWEETS\n\n\nPEOPLE_NEED_TO_RECYCLE\n\n\nMAKE_EFFORT_TO_RECYCLE\n\n\nPEOPLE_RESPONS_TO_RECYCLD_PRDCTS\n\n\nPERSONAL_ENVRNMNT_RESPONSIBLE\n\n\n\n\n\n\nKaiser’s Measure of Sampling Adequacy: Overall MSA = 0.78651538\n\n\n\n\nDRINKING_GET_DRUNK\n\n\n0.68962461\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FACTOR Procedure\nInitial Factor Method: Principal Components\n\n\n\nPrior Communality Estimates: ONE\n\n\n\n\n\n\n\n\n\nEigenvalues of the Correlation Matrix: Total\n= 8 Average = 1\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n\n\n\n\n\nFactor Pattern\n\n\n\n\n\n\n\nDRINKING_GET_DRUNK\n\n\nOFTEN_DRINK_ALCOHOL\n\n\nTRY_NEW_FOOD_PRDCT\n\n\nFEEL_GUILTY_SWEETS\n\n\nPEOPLE_NEED_TO_RECYCLE\n\n\nMAKE_EFFORT_TO_RECYCLE\n\n\nPEOPLE_RESPONS_TO_RECYCLD_PRDCTS\n\n\nPERSONAL_ENVRNMNT_RESPONSIBLE\n\n\n\n\n\n\n\n\n\nVariance Explained by Each\nFactor\n\n\n\n\nFactor1\n\n\n2.8634159\n\n\n\n\n\n\nFinal Communality Estimates: Total = 4.187467\n\n\n\n\nDRINKING_GET_DRUNK\n\n\n0.37758061\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FACTOR Procedure\nRotation Method: Varimax\n\n\n\nOrthogonal Transformation Matrix\n\n\n\n\n\n\n\n1\n\n\n2\n\n\n\n\n\n\nRotated Factor Pattern\n\n\n\n\n\n\n\nDRINKING_GET_DRUNK\n\n\nOFTEN_DRINK_ALCOHOL\n\n\nTRY_NEW_FOOD_PRDCT\n\n\nFEEL_GUILTY_SWEETS\n\n\nPEOPLE_NEED_TO_RECYCLE\n\n\nMAKE_EFFORT_TO_RECYCLE\n\n\nPEOPLE_RESPONS_TO_RECYCLD_PRDCTS\n\n\nPERSONAL_ENVRNMNT_RESPONSIBLE\n\n\n\n\n\n\n\n\n\nVariance Explained by Each\nFactor\n\n\n\n\nFactor1\n\n\n2.8568718\n\n\n\n\n\n\nFinal Communality Estimates: Total = 4.187467\n\n\n\n\nDRINKING_GET_DRUNK\n\n\n0.37758061\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FACTOR Procedure\nRotation Method: Varimax\n\n\n\nScoring Coefficients Estimated by Regression\n\n\n\n\n\n\n\n\n\nSquared Multiple Correlations\nof the Variables with Each\nFactor\n\n\n\n\nFactor1\n\n\n1.0000000\n\n\n\n\n\n\nStandardized Scoring Coefficients\n\n\n\n\n\n\n\nDRINKING_GET_DRUNK\n\n\nOFTEN_DRINK_ALCOHOL\n\n\nTRY_NEW_FOOD_PRDCT\n\n\nFEEL_GUILTY_SWEETS\n\n\nPEOPLE_NEED_TO_RECYCLE\n\n\nMAKE_EFFORT_TO_RECYCLE\n\n\nPEOPLE_RESPONS_TO_RECYCLD_PRDCTS\n\n\nPERSONAL_ENVRNMNT_RESPONSIBLE\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FASTCLUS Procedure\nReplace=FULL Radius=0 Maxclusters=3 Maxiter=100 Converge=0.02\n\n\n\nInitial Seeds\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\nMinimum Distance Between Initial Seeds =\n8.408203\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n\n\nConvergence criterion is satisfied.\n\n\n\n\n\n\nCriterion Based on Final Seeds =\n1.0133\n\n\n\n\n\n\n\n\nCluster Summary\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\n323 Observation(s) were omitted due to missing values.\n\n\n\n\n\n\nStatistics for Variables\n\n\n\n\nVariable\n\n\nIndulgenceFactors\n\n\nGreenAttitudeFactor\n\n\ndrink_between_meals\n\n\nlike_to_try_new_drinks\n\n\nwhen_on_tv_go_online_get_more\n\n\nbuy_online_or_in_store\n\n\nOVER-ALL\n\n\n\n\n\n\nPseudo F Statistic =\n5622.32\n\n\n\n\n\n\n\n\nApproximate Expected Over-All R-Squared =\n0.27176\n\n\n\n\n\n\n\n\nCubic Clustering Criterion =\n39.855\n\n\n\n\n\n\n\n\n\n\n\nWARNING: The two values above are invalid for correlated variables.\n\n\n\n\n\n\nCluster Means\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\nCluster Standard Deviations\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FASTCLUS Procedure\nReplace=FULL Radius=0 Maxclusters=4 Maxiter=100 Converge=0.02\n\n\n\nInitial Seeds\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n\n\nMinimum Distance Between Initial Seeds =\n7.360341\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n\n\nConvergence criterion is satisfied.\n\n\n\n\n\n\nCriterion Based on Final Seeds =\n0.9619\n\n\n\n\n\n\n\n\nCluster Summary\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n\n\n323 Observation(s) were omitted due to missing values.\n\n\n\n\n\n\nStatistics for Variables\n\n\n\n\nVariable\n\n\nIndulgenceFactors\n\n\nGreenAttitudeFactor\n\n\ndrink_between_meals\n\n\nlike_to_try_new_drinks\n\n\nwhen_on_tv_go_online_get_more\n\n\nbuy_online_or_in_store\n\n\nOVER-ALL\n\n\n\n\n\n\nPseudo F Statistic =\n5076.37\n\n\n\n\n\n\n\n\nApproximate Expected Over-All R-Squared =\n0.36802\n\n\n\n\n\n\n\n\nCubic Clustering Criterion =\n9.710\n\n\n\n\n\n\n\n\n\n\n\nWARNING: The two values above are invalid for correlated variables.\n\n\n\n\n\n\nCluster Means\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n\n\nCluster Standard Deviations\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FASTCLUS Procedure\nReplace=FULL Radius=0 Maxclusters=5 Maxiter=100 Converge=0.02\n\n\n\nInitial Seeds\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\nMinimum Distance Between Initial Seeds =\n7.104607\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n\n\n\n\nConvergence criterion is satisfied.\n\n\n\n\n\n\nCriterion Based on Final Seeds =\n0.9051\n\n\n\n\n\n\n\n\nCluster Summary\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\n323 Observation(s) were omitted due to missing values.\n\n\n\n\n\n\nStatistics for Variables\n\n\n\n\nVariable\n\n\nIndulgenceFactors\n\n\nGreenAttitudeFactor\n\n\ndrink_between_meals\n\n\nlike_to_try_new_drinks\n\n\nwhen_on_tv_go_online_get_more\n\n\nbuy_online_or_in_store\n\n\nOVER-ALL\n\n\n\n\n\n\nPseudo F Statistic =\n5104.69\n\n\n\n\n\n\n\n\nApproximate Expected Over-All R-Squared =\n0.42914\n\n\n\n\n\n\n\n\nCubic Clustering Criterion =\n21.246\n\n\n\n\n\n\n\n\n\n\n\nWARNING: The two values above are invalid for correlated variables.\n\n\n\n\n\n\nCluster Means\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\n\nCluster Standard Deviations\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FASTCLUS Procedure\nReplace=FULL Radius=0 Maxclusters=6 Maxiter=100 Converge=0.02\n\n\n\nInitial Seeds\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n\n\n\n\nMinimum Distance Between Initial Seeds =\n6.589243\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\n\n\nConvergence criterion is satisfied.\n\n\n\n\n\n\nCriterion Based on Final Seeds =\n0.8842\n\n\n\n\n\n\n\n\nCluster Summary\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n\n\n\n\n323 Observation(s) were omitted due to missing values.\n\n\n\n\n\n\nStatistics for Variables\n\n\n\n\nVariable\n\n\nIndulgenceFactors\n\n\nGreenAttitudeFactor\n\n\ndrink_between_meals\n\n\nlike_to_try_new_drinks\n\n\nwhen_on_tv_go_online_get_more\n\n\nbuy_online_or_in_store\n\n\nOVER-ALL\n\n\n\n\n\n\nPseudo F Statistic =\n4523.63\n\n\n\n\n\n\n\n\nApproximate Expected Over-All R-Squared =\n0.46407\n\n\n\n\n\n\n\n\nCubic Clustering Criterion =\n11.616\n\n\n\n\n\n\n\n\n\n\n\nWARNING: The two values above are invalid for correlated variables.\n\n\n\n\n\n\nCluster Means\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n\n\n\n\nCluster Standard Deviations\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FASTCLUS Procedure\nReplace=FULL Radius=0 Maxclusters=7 Maxiter=100 Converge=0.02\n\n\n\nInitial Seeds\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\n\n\nMinimum Distance Between Initial Seeds =\n6.513331\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\n\n\nConvergence criterion is satisfied.\n\n\n\n\n\n\nCriterion Based on Final Seeds =\n0.8658\n\n\n\n\n\n\n\n\nCluster Summary\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\n\n\n323 Observation(s) were omitted due to missing values.\n\n\n\n\n\n\nStatistics for Variables\n\n\n\n\nVariable\n\n\nIndulgenceFactors\n\n\nGreenAttitudeFactor\n\n\ndrink_between_meals\n\n\nlike_to_try_new_drinks\n\n\nwhen_on_tv_go_online_get_more\n\n\nbuy_online_or_in_store\n\n\nOVER-ALL\n\n\n\n\n\n\nPseudo F Statistic =\n4118.01\n\n\n\n\n\n\n\n\nApproximate Expected Over-All R-Squared =\n0.49418\n\n\n\n\n\n\n\n\nCubic Clustering Criterion =\n2.269\n\n\n\n\n\n\n\n\n\n\n\nWARNING: The two values above are invalid for correlated variables.\n\n\n\n\n\n\nCluster Means\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n\n\n\nCluster Standard Deviations\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FASTCLUS Procedure\nReplace=FULL Radius=0 Maxclusters=8 Maxiter=100 Converge=0.02\n\n\n\nInitial Seeds\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n\n\n\n\nMinimum Distance Between Initial Seeds =\n6.213953\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n\n\n\n\nConvergence criterion is satisfied.\n\n\n\n\n\n\nCriterion Based on Final Seeds =\n0.8240\n\n\n\n\n\n\n\n\nCluster Summary\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n\n\n\n\n323 Observation(s) were omitted due to missing values.\n\n\n\n\n\n\nStatistics for Variables\n\n\n\n\nVariable\n\n\nIndulgenceFactors\n\n\nGreenAttitudeFactor\n\n\ndrink_between_meals\n\n\nlike_to_try_new_drinks\n\n\nwhen_on_tv_go_online_get_more\n\n\nbuy_online_or_in_store\n\n\nOVER-ALL\n\n\n\n\n\n\nPseudo F Statistic =\n4258.66\n\n\n\n\n\n\n\n\nApproximate Expected Over-All R-Squared =\n0.51624\n\n\n\n\n\n\n\n\nCubic Clustering Criterion =\n34.210\n\n\n\n\n\n\n\n\n\n\n\nWARNING: The two values above are invalid for correlated variables.\n\n\n\n\n\n\nCluster Means\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n\n\n\n\nCluster Standard Deviations\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FASTCLUS Procedure\nReplace=FULL Radius=0 Maxclusters=9 Maxiter=100 Converge=0.02\n\n\n\nInitial Seeds\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n\n\n\n\nMinimum Distance Between Initial Seeds =\n5.902531\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10\n\n\n11\n\n\n12\n\n\n13\n\n\n14\n\n\n15\n\n\n\n\n\n\nConvergence criterion is satisfied.\n\n\n\n\n\n\nCriterion Based on Final Seeds =\n0.7934\n\n\n\n\n\n\n\n\nCluster Summary\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n\n\n\n\n323 Observation(s) were omitted due to missing values.\n\n\n\n\n\n\nStatistics for Variables\n\n\n\n\nVariable\n\n\nIndulgenceFactors\n\n\nGreenAttitudeFactor\n\n\ndrink_between_meals\n\n\nlike_to_try_new_drinks\n\n\nwhen_on_tv_go_online_get_more\n\n\nbuy_online_or_in_store\n\n\nOVER-ALL\n\n\n\n\n\n\nPseudo F Statistic =\n4263.33\n\n\n\n\n\n\n\n\nApproximate Expected Over-All R-Squared =\n0.53490\n\n\n\n\n\n\n\n\nCubic Clustering Criterion =\n53.675\n\n\n\n\n\n\n\n\n\n\n\nWARNING: The two values above are invalid for correlated variables.\n\n\n\n\n\n\nCluster Means\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n\n\n\n\nCluster Standard Deviations\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe FASTCLUS Procedure\nReplace=FULL Radius=0 Maxclusters=3 Maxiter=100 Converge=0.02\n\n\n\nInitial Seeds\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\nMinimum Distance Between Initial Seeds =\n8.408203\n\n\n\n\n\n\n\n\nIteration History\n\n\n\n\nIteration\n\n\n1\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n\n\n\n\nConvergence criterion is satisfied.\n\n\n\n\n\n\nCriterion Based on Final Seeds =\n1.0133\n\n\n\n\n\n\n\n\nCluster Summary\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\n323 Observation(s) were omitted due to missing values.\n\n\n\n\n\n\nStatistics for Variables\n\n\n\n\nVariable\n\n\nIndulgenceFactors\n\n\nGreenAttitudeFactor\n\n\ndrink_between_meals\n\n\nlike_to_try_new_drinks\n\n\nwhen_on_tv_go_online_get_more\n\n\nbuy_online_or_in_store\n\n\nOVER-ALL\n\n\n\n\n\n\nPseudo F Statistic =\n5622.32\n\n\n\n\n\n\n\n\nApproximate Expected Over-All R-Squared =\n0.27176\n\n\n\n\n\n\n\n\nCubic Clustering Criterion =\n39.855\n\n\n\n\n\n\n\n\n\n\n\nWARNING: The two values above are invalid for correlated variables.\n\n\n\n\n\n\nCluster Means\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n\n\n\n\nCluster Standard Deviations\n\n\n\n\nCluster\n\n\n1\n\n\n2\n\n\n3\n\n\n\n________________________________________________________________________________________\n\n\n\nThe SAS System\n\n\n\nThe MEANS Procedure\nCluster=.\n\n\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nStd Dev\nMinimum\nMaximum\n\n\n\n\nDr_Pepper\n323\n0.0773994\n0.2676387\n0.0000000\n1.0000000\n\n\nCoca_Cola\n323\n0.3188854\n0.4667677\n0.0000000\n1.0000000\n\n\nSprite\n323\n0.0650155\n0.2469357\n0.0000000\n1.0000000\n\n\nMALE\n323\n0.4551084\n0.4987533\n0.0000000\n1.0000000\n\n\nFEMALE\n323\n0.5448916\n0.4987533\n0.0000000\n1.0000000\n\n\nNORTHEAST\n323\n0.1919505\n0.3944454\n0.0000000\n1.0000000\n\n\nSOUTH\n323\n0.4520124\n0.4984641\n0.0000000\n1.0000000\n\n\nAge_18_24\n323\n0.0990712\n0.2992211\n0.0000000\n1.0000000\n\n\nAge_50\n323\n0.6377709\n0.4813903\n0.0000000\n1.0000000\n\n\nYOUTUBE\n323\n0.1052632\n0.3073684\n0.0000000\n1.0000000\n\n\nSPOTIFY\n323\n0.0216718\n0.1458355\n0.0000000\n1.0000000\n\n\nI prefer DrPepper\n323\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nI prefer CocaCola\n323\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nI prefer Sprite\n323\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nResponse by Male\n323\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nResponse by Female\n323\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nRegion: NorthEast\n323\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nRegion: South\n323\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nAge: 18-24\n323\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nAge: 50+\n323\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nYouTube was used during last 7 days\n323\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nSpotify was used during last 7 days\n323\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\n\nCluster = 1\n\n\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nStd Dev\nMinimum\nMaximum\n\n\n\n\nDr_Pepper\n7538\n0.0862298\n0.2807217\n0.0000000\n1.0000000\n\n\nCoca_Cola\n7538\n0.3357655\n0.4722887\n0.0000000\n1.0000000\n\n\nSprite\n7538\n0.0874237\n0.2824737\n0.0000000\n1.0000000\n\n\nMALE\n7538\n0.4292916\n0.4950079\n0.0000000\n1.0000000\n\n\nFEMALE\n7538\n0.5707084\n0.4950079\n0.0000000\n1.0000000\n\n\nNORTHEAST\n7538\n0.1768374\n0.3815563\n0.0000000\n1.0000000\n\n\nSOUTH\n7538\n0.3857787\n0.4868110\n0.0000000\n1.0000000\n\n\nAge_18_24\n7538\n0.0416556\n0.1998142\n0.0000000\n1.0000000\n\n\nAge_50\n7538\n0.7512603\n0.4323113\n0.0000000\n1.0000000\n\n\nYOUTUBE\n7538\n0.1771027\n0.3817809\n0.0000000\n1.0000000\n\n\nSPOTIFY\n7538\n0.0222871\n0.1476254\n0.0000000\n1.0000000\n\n\nI prefer DrPepper\n7538\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nI prefer CocaCola\n7538\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nI prefer Sprite\n7538\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nResponse by Male\n7538\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nResponse by Female\n7538\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nRegion: NorthEast\n7538\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nRegion: South\n7538\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nAge: 18-24\n7538\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nAge: 50+\n7538\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nYouTube was used during last 7 days\n7538\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nSpotify was used during last 7 days\n7538\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\n\nCluster = 2\n\n\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nStd Dev\nMinimum\nMaximum\n\n\n\n\nDr_Pepper\n10503\n0.1126345\n0.3161605\n0.0000000\n1.0000000\n\n\nCoca_Cola\n10503\n0.3767495\n0.4845943\n0.0000000\n1.0000000\n\n\nSprite\n10503\n0.1139674\n0.3177868\n0.0000000\n1.0000000\n\n\nMALE\n10503\n0.4606303\n0.4984713\n0.0000000\n1.0000000\n\n\nFEMALE\n10503\n0.5393697\n0.4984713\n0.0000000\n1.0000000\n\n\nNORTHEAST\n10503\n0.1939446\n0.3954048\n0.0000000\n1.0000000\n\n\nSOUTH\n10503\n0.3812244\n0.4857106\n0.0000000\n1.0000000\n\n\nAge_18_24\n10503\n0.1168238\n0.3212255\n0.0000000\n1.0000000\n\n\nAge_50\n10503\n0.4356850\n0.4958699\n0.0000000\n1.0000000\n\n\nYOUTUBE\n10503\n0.3752261\n0.4842043\n0.0000000\n1.0000000\n\n\nSPOTIFY\n10503\n0.0910216\n0.2876535\n0.0000000\n1.0000000\n\n\nI prefer DrPepper\n10503\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nI prefer CocaCola\n10503\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nI prefer Sprite\n10503\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nResponse by Male\n10503\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nResponse by Female\n10503\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nRegion: NorthEast\n10503\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nRegion: South\n10503\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nAge: 18-24\n10503\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nAge: 50+\n10503\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nYouTube was used during last 7 days\n10503\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nSpotify was used during last 7 days\n10503\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\n\nCluster 3\n\n\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nStd Dev\nMinimum\nMaximum\n\n\n\n\nDr_Pepper\n7075\n0.1390813\n0.3460558\n0.0000000\n1.0000000\n\n\nCoca_Cola\n7075\n0.4171025\n0.4931150\n0.0000000\n1.0000000\n\n\nSprite\n7075\n0.1301767\n0.3365215\n0.0000000\n1.0000000\n\n\nMALE\n7075\n0.4187986\n0.4933971\n0.0000000\n1.0000000\n\n\nFEMALE\n7075\n0.5812014\n0.4933971\n0.0000000\n1.0000000\n\n\nNORTHEAST\n7075\n0.1831802\n0.3868415\n0.0000000\n1.0000000\n\n\nSOUTH\n7075\n0.3817668\n0.4858542\n0.0000000\n1.0000000\n\n\nAge_18_24\n7075\n0.1139223\n0.3177393\n0.0000000\n1.0000000\n\n\nAge_50\n7075\n0.4705300\n0.4991660\n0.0000000\n1.0000000\n\n\nYOUTUBE\n7075\n0.3188693\n0.4660712\n0.0000000\n1.0000000\n\n\nSPOTIFY\n7075\n0.0809894\n0.2728381\n0.0000000\n1.0000000\n\n\nI prefer DrPepper\n7075\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nI prefer CocaCola\n7075\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nI prefer Sprite\n7075\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nResponse by Male\n7075\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nResponse by Female\n7075\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nRegion: NorthEast\n7075\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nRegion: South\n7075\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nAge: 18-24\n7075\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nAge: 50+\n7075\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nYouTube was used during last 7 days\n7075\n0.0000000\n0.0000000\n0.0000000\n1.0000000\n\n\nSpotify was used during last 7 days\n7075\n0.0000000\n0.0000000\n0.0000000\n1.0000000"
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "Here is how the synthetic dataset is generated. The dataset consists of 100 observations and includes the following variables:\n\nlibrary(simstudy)\n\nWarning: package 'simstudy' was built under R version 4.3.3\n\nset.seed(123)\n\n# Define data structure\ndef &lt;- defData(varname = \"X1\", formula = 0, variance = 1)\ndef &lt;- defData(def, varname = \"X2\", formula = \"2*X1 + rnorm(1, mean = 0, sd = 0.5)\")\ndef &lt;- defData(def, varname = \"X3\", formula = 0, variance = 1)\ndef &lt;- defData(def, varname = \"Y\", formula = \"3*X1 + 4*X2 + 2*X3 + rnorm(1, mean = 0, sd = 1)\")\n\n# Generate data with 100 observations\ndata &lt;- genData(100, def)\n\n# Convert 'data' to data frame\ndata &lt;- as.data.frame(data)\n\n# Show the first few rows of the dataset\nhead(data)\n\n  id          X1          X2          X3         Y\n1  1 -0.56047565 -1.47615457  0.25688371 -5.759865\n2  2 -0.23017749 -0.81555826 -0.24669188 -3.133736\n3  3  1.55870831  2.76221335 -0.34754260 16.342306\n4  4  0.07050839 -0.21418650 -0.95161857 -1.236045\n5  5  0.12928774 -0.09662781 -0.04502772  1.223709\n6  6  1.71506499  3.07492669 -0.78490447 17.187506\n\n\nExploring the generated data using scatterplot matrices and correlation matrices. This helped visualize the relationships between the variables.\n\n# Scatterplot matrix\npairs(~ X1 + X2 + X3 + Y, data = data)\n\n\n\n\nScatterplot matrix:\n\nX1 vs X2: There is a strong positive linear relationship between them.\nX1 vs Y and X2 vs Y: Both scatterplots show a positive relationship, which aligns with the data generation process where Y is influenced by both X1 and X2.\nX3: There seems to be no clear linear relationship between X3 and the other variables X1 and X2. However, there is a positive relationship between X3 and Y.\nY: The variable Y shows a linear relationship with X1, X2, and to X3.\nX1 and X2 are strongly correlated. Y is influenced by X1, X2, and X3, with X1 and X2 having a more apparent impact on Y.\n\n\n# Correlation matrix\nprint(cor(data))\n\n           id           X1           X2           X3         Y\nid 1.00000000  0.079808324  0.079808324  0.123034289 0.1022722\nX1 0.07980832  1.000000000  1.000000000 -0.006486112 0.9809823\nX2 0.07980832  1.000000000  1.000000000 -0.006486112 0.9809823\nX3 0.12303429 -0.006486112 -0.006486112  1.000000000 0.1877305\nY  0.10227216  0.980982268  0.980982268  0.187730530 1.0000000\n\n\nCorrelation Matrix:\n\nHigh correlation between X1 and X2.\nLow correlations between X3 and the other variables (except Y) are expected since X3 was generated independently.\n\nModel Fitting:\nTwo linear regression models.\n\n# Fit linear regression model\nmodel1 &lt;- lm(Y ~ X1, data = data)\nprint(summary(model1))\n\n\nCall:\nlm(formula = Y ~ X1, data = data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.938 -1.435 -0.244  1.192  6.633 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -0.2640     0.2004  -1.317    0.191    \nX1           10.9859     0.2196  50.033   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.994 on 98 degrees of freedom\nMultiple R-squared:  0.9623,    Adjusted R-squared:  0.9619 \nF-statistic:  2503 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nSimple Linear Regression:\nModel with Y as the response variable and X1 as the predictor. The simple linear regression model with Y ~ X1 showed a significant relationship between Y and X1. However, the model’s fit may not capture all the variability in Y due to the omission of X2 and X3\n\n# Fit multiple linear regression model\nmodel2 &lt;- lm(Y ~ X1 + X2 + X3, data = data)\nprint(summary(model2))\n\nWarning in summary.lm(model2): essentially perfect fit: summary may be\nunreliable\n\n\n\nCall:\nlm(formula = Y ~ X1 + X2 + X3, data = data)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-2.702e-15 -8.811e-16  1.000e-17  4.697e-16  1.338e-14 \n\nCoefficients: (1 not defined because of singularities)\n              Estimate Std. Error    t value Pr(&gt;|t|)    \n(Intercept) -1.084e-01  1.917e-16 -5.654e+14   &lt;2e-16 ***\nX1           1.100e+01  2.094e-16  5.253e+16   &lt;2e-16 ***\nX2                  NA         NA         NA       NA    \nX3           2.000e+00  1.927e-16  1.038e+16   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.902e-15 on 97 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 1.43e+33 on 2 and 97 DF,  p-value: &lt; 2.2e-16\n\n\nMultiple Linear Regression:\nModel with Y as the response variable and X1, X2, and X3 as predictors. The multiple linear regression model with Y ~ X1 + X2 + X3 showed significant relationships between Y and all predictors (X1, X2, and X3). The coefficients were close to the values used in the data generation process:\n\nCoefficient for X1 was close to 3.\nCoefficient for X2 was close to 4.\nCoefficient for X3 was close to 2.\n\nThe residuals plot indicated a good fit for the multiple linear regression model, with no major patterns observed in the residuals.\nNow I will examine the residuals of the multiple linear regression model to check the model fit.\n\n# Plotting residuals to check model fit\npar(mfrow = c(2, 2))\nplot(model2)\n\n\n\n\nSummary:\n\nLinearity: The residuals-fitted plot suggests the linearity assumption is met.\nNormality: The Q-Q plot indicates that the residuals are normally distributed.\nHomoscedasticity: Both the residuals-fitted and scale-location plots suggest that variance is constant.\nInfluential Points: The residuals-leverage plot indicates the presence of some influential points."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The dataset “Potentially Excess Deaths from the Five Leading Causes of Death” offers a detailed look at how many deaths in different areas of the U.S. Essentially, it highlights how many deaths occurred and what expected. In addition we are also provided with this following information:\nUnderlying cause of death is based on the International Classification of Diseases, and are as follows:\n\nHeart disease (I00-I09, I11, I13, and I20–I51)\nCancer (C00–C97)\nUnintentional injury (V01–X59 and Y85–Y86)\nChronic lower respiratory disease (J40–J47)\nStroke (I60–I69)\nLocality (nonmetropolitan vs. metropolitan).\nCDC’s Potentially Excess Deaths webpage\nThe dataset has 206k rows.\n13 columns:\n\nYear: The year of the data record - 2005 through 2015.\nCause of Death: Primary cause of death based on ICD-10 codes.\nState: U.S. state where deaths occurred.\nLocality: Classification as nonmetropolitan or metropolitan.\nObserved Deaths: Actual number of deaths.\nExpected Deaths: Number of deaths expected based on benchmark rates.\nPotentially Excess Deaths: Deaths exceeding expected numbers.\nPercent Potentially Excess Deaths: Percentage of deaths exceeding expectations."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#dataset-overview",
    "href": "cdcdata-exercise/cdcdata-exercise.html#dataset-overview",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The dataset “Potentially Excess Deaths from the Five Leading Causes of Death” offers a detailed look at how many deaths in different areas of the U.S. Essentially, it highlights how many deaths occurred and what expected. In addition we are also provided with this following information:\nUnderlying cause of death is based on the International Classification of Diseases, and are as follows:\n\nHeart disease (I00-I09, I11, I13, and I20–I51)\nCancer (C00–C97)\nUnintentional injury (V01–X59 and Y85–Y86)\nChronic lower respiratory disease (J40–J47)\nStroke (I60–I69)\nLocality (nonmetropolitan vs. metropolitan).\nCDC’s Potentially Excess Deaths webpage\nThe dataset has 206k rows.\n13 columns:\n\nYear: The year of the data record - 2005 through 2015.\nCause of Death: Primary cause of death based on ICD-10 codes.\nState: U.S. state where deaths occurred.\nLocality: Classification as nonmetropolitan or metropolitan.\nObserved Deaths: Actual number of deaths.\nExpected Deaths: Number of deaths expected based on benchmark rates.\nPotentially Excess Deaths: Deaths exceeding expected numbers.\nPercent Potentially Excess Deaths: Percentage of deaths exceeding expectations."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#loading-and-cleaning-the-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#loading-and-cleaning-the-data",
    "title": "CDC Data Exercise",
    "section": "Loading and Cleaning the Data",
    "text": "Loading and Cleaning the Data\nWe first begin by loading and cleaning the dataset.\n\n# Load necessary libraries\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(readr)\nlibrary(RColorBrewer)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tibble)\n\n# Set the working directory to the project directory\ndata &lt;- here::here(\"cdcdata-exercise\",\"NCHS_-_Five_Leading_Causes_of_Death.csv\")\n# Define the file path and load the data\ndata &lt;- read_csv(\"NCHS_-_Five_Leading_Causes_of_Death.csv\")\n\nRows: 205920 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Cause of Death, State, State FIPS Code, Age Range, Benchmark, Locality\ndbl (7): Year, HHS Region, Observed Deaths, Population, Expected Deaths, Pot...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Tell us the name of the columns \ncolnames(data)\n\n [1] \"Year\"                              \"Cause of Death\"                   \n [3] \"State\"                             \"State FIPS Code\"                  \n [5] \"HHS Region\"                        \"Age Range\"                        \n [7] \"Benchmark\"                         \"Locality\"                         \n [9] \"Observed Deaths\"                   \"Population\"                       \n[11] \"Expected Deaths\"                   \"Potentially Excess Deaths\"        \n[13] \"Percent Potentially Excess Deaths\"\n\n#output two newline characters\ncat(\"\\n\\n\")\n# Display the structure of the dataset\nstr(data)\n\nspc_tbl_ [205,920 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Year                             : num [1:205920] 2005 2005 2005 2005 2005 ...\n $ Cause of Death                   : chr [1:205920] \"Cancer\" \"Cancer\" \"Cancer\" \"Cancer\" ...\n $ State                            : chr [1:205920] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ State FIPS Code                  : chr [1:205920] \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ HHS Region                       : num [1:205920] 4 4 4 4 4 4 4 4 4 4 ...\n $ Age Range                        : chr [1:205920] \"0-49\" \"0-49\" \"0-49\" \"0-49\" ...\n $ Benchmark                        : chr [1:205920] \"2005 Fixed\" \"2005 Fixed\" \"2005 Fixed\" \"2010 Fixed\" ...\n $ Locality                         : chr [1:205920] \"All\" \"Metropolitan\" \"Nonmetropolitan\" \"All\" ...\n $ Observed Deaths                  : num [1:205920] 756 556 200 756 556 ...\n $ Population                       : num [1:205920] 3148377 2379871 768506 3148377 2379871 ...\n $ Expected Deaths                  : num [1:205920] 451 341 111 421 318 103 451 341 111 784 ...\n $ Potentially Excess Deaths        : num [1:205920] 305 217 89 335 238 97 305 217 89 562 ...\n $ Percent Potentially Excess Deaths: num [1:205920] 40.3 39 44.5 44.3 42.8 48.5 40.3 39 44.5 41.8 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Year = col_double(),\n  ..   `Cause of Death` = col_character(),\n  ..   State = col_character(),\n  ..   `State FIPS Code` = col_character(),\n  ..   `HHS Region` = col_double(),\n  ..   `Age Range` = col_character(),\n  ..   Benchmark = col_character(),\n  ..   Locality = col_character(),\n  ..   `Observed Deaths` = col_double(),\n  ..   Population = col_double(),\n  ..   `Expected Deaths` = col_double(),\n  ..   `Potentially Excess Deaths` = col_double(),\n  ..   `Percent Potentially Excess Deaths` = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\ncat(\"\\n\\n\")\n# Check for missing values\ncolSums(is.na(data))\n\n                             Year                    Cause of Death \n                                0                                 0 \n                            State                   State FIPS Code \n                                0                                 0 \n                       HHS Region                         Age Range \n                                0                                 0 \n                        Benchmark                          Locality \n                                0                                 0 \n                  Observed Deaths                        Population \n                            10212                              5280 \n                  Expected Deaths         Potentially Excess Deaths \n                            10212                             10212 \nPercent Potentially Excess Deaths \n                            10212 \n\ncat(\"\\n\\n\")\n# Remove rows with NA values\ndata_cleaned &lt;- data %&gt;%\n  drop_na()\n\n# Display the structure of the cleaned dataset\nstr(data_cleaned)\n\ntibble [195,708 × 13] (S3: tbl_df/tbl/data.frame)\n $ Year                             : num [1:195708] 2005 2005 2005 2005 2005 ...\n $ Cause of Death                   : chr [1:195708] \"Cancer\" \"Cancer\" \"Cancer\" \"Cancer\" ...\n $ State                            : chr [1:195708] \"Alabama\" \"Alabama\" \"Alabama\" \"Alabama\" ...\n $ State FIPS Code                  : chr [1:195708] \"AL\" \"AL\" \"AL\" \"AL\" ...\n $ HHS Region                       : num [1:195708] 4 4 4 4 4 4 4 4 4 4 ...\n $ Age Range                        : chr [1:195708] \"0-49\" \"0-49\" \"0-49\" \"0-49\" ...\n $ Benchmark                        : chr [1:195708] \"2005 Fixed\" \"2005 Fixed\" \"2005 Fixed\" \"2010 Fixed\" ...\n $ Locality                         : chr [1:195708] \"All\" \"Metropolitan\" \"Nonmetropolitan\" \"All\" ...\n $ Observed Deaths                  : num [1:195708] 756 556 200 756 556 ...\n $ Population                       : num [1:195708] 3148377 2379871 768506 3148377 2379871 ...\n $ Expected Deaths                  : num [1:195708] 451 341 111 421 318 103 451 341 111 784 ...\n $ Potentially Excess Deaths        : num [1:195708] 305 217 89 335 238 97 305 217 89 562 ...\n $ Percent Potentially Excess Deaths: num [1:195708] 40.3 39 44.5 44.3 42.8 48.5 40.3 39 44.5 41.8 ..."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#visualizations-and-exploratory-data-analysis",
    "href": "cdcdata-exercise/cdcdata-exercise.html#visualizations-and-exploratory-data-analysis",
    "title": "CDC Data Exercise",
    "section": "Visualizations and Exploratory Data Analysis",
    "text": "Visualizations and Exploratory Data Analysis\n1) Summary statistics for variables.\n\n# Summary statistics for data \nsummary(data)\n\n      Year      Cause of Death        State           State FIPS Code   \n Min.   :2005   Length:205920      Length:205920      Length:205920     \n 1st Qu.:2007   Class :character   Class :character   Class :character  \n Median :2010   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2010                                                           \n 3rd Qu.:2013                                                           \n Max.   :2015                                                           \n                                                                        \n   HHS Region      Age Range          Benchmark           Locality        \n Min.   : 0.000   Length:205920      Length:205920      Length:205920     \n 1st Qu.: 3.000   Class :character   Class :character   Class :character  \n Median : 5.000   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 5.231                                                           \n 3rd Qu.: 8.000                                                           \n Max.   :10.000                                                           \n                                                                          \n Observed Deaths    Population        Expected Deaths \n Min.   :    10   Min.   :    55536   Min.   :     2  \n 1st Qu.:   155   1st Qu.:   682826   1st Qu.:    92  \n Median :   508   Median :  1610936   Median :   298  \n Mean   :  2975   Mean   :  7008421   Mean   :  2113  \n 3rd Qu.:  1586   3rd Qu.:  4681585   3rd Qu.:  1019  \n Max.   :493526   Max.   :315131659   Max.   :465126  \n NA's   :10212    NA's   :5280        NA's   :10212   \n Potentially Excess Deaths Percent Potentially Excess Deaths\n Min.   :     0.0          Min.   : 0.00                    \n 1st Qu.:    41.0          1st Qu.:20.90                    \n Median :   159.0          Median :35.80                    \n Mean   :   875.1          Mean   :35.73                    \n 3rd Qu.:   543.0          3rd Qu.:50.10                    \n Max.   :175703.0          Max.   :85.30                    \n NA's   :10212             NA's   :10212                    \n\ncat(\"\\n\\n\")\n# Summary statistics for cleaned data\nsummary(data_cleaned)\n\n      Year      Cause of Death        State           State FIPS Code   \n Min.   :2005   Length:195708      Length:195708      Length:195708     \n 1st Qu.:2007   Class :character   Class :character   Class :character  \n Median :2010   Mode  :character   Mode  :character   Mode  :character  \n Mean   :2010                                                           \n 3rd Qu.:2013                                                           \n Max.   :2015                                                           \n   HHS Region    Age Range          Benchmark           Locality        \n Min.   : 0.0   Length:195708      Length:195708      Length:195708     \n 1st Qu.: 3.0   Class :character   Class :character   Class :character  \n Median : 5.0   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 5.3                                                           \n 3rd Qu.: 8.0                                                           \n Max.   :10.0                                                           \n Observed Deaths    Population        Expected Deaths \n Min.   :    10   Min.   :    55536   Min.   :     2  \n 1st Qu.:   155   1st Qu.:   724049   1st Qu.:    92  \n Median :   508   Median :  1704462   Median :   298  \n Mean   :  2975   Mean   :  7177289   Mean   :  2113  \n 3rd Qu.:  1586   3rd Qu.:  4783958   3rd Qu.:  1019  \n Max.   :493526   Max.   :315131659   Max.   :465126  \n Potentially Excess Deaths Percent Potentially Excess Deaths\n Min.   :     0.0          Min.   : 0.00                    \n 1st Qu.:    41.0          1st Qu.:20.90                    \n Median :   159.0          Median :35.80                    \n Mean   :   875.1          Mean   :35.73                    \n 3rd Qu.:   543.0          3rd Qu.:50.10                    \n Max.   :175703.0          Max.   :85.30                    \n\n\n2) Visualization of trends over time for causes of death.\n\n# Define a color palette\ncolors &lt;- c(\"Heart Disease\" = \"red\", \n            \"Cancer\" = \"blue\", \n            \"Chronic Lower Respiratory Disease\" = \"darkgreen\", \n            \"Stroke\" = \"orange\", \n            \"Unintentional Injury\" = \"grey\")\n\n# Aggregate data by Year and Cause of Death, excluding \"United States\"\nannual_totals &lt;- data_cleaned %&gt;%\n  filter(State != \"United States\") %&gt;%\n  group_by(Year, `Cause of Death`) %&gt;%\n  summarize(Total_Observed_Deaths = sum(`Observed Deaths`, na.rm = TRUE), .groups = 'drop')\n\n# Create the time series line plot with thicker lines and formatted year labels\nggplot(annual_totals, aes(x = Year, y = Total_Observed_Deaths, color = `Cause of Death`, group = `Cause of Death`)) +\n  geom_line(linewidth = 1.2) +  # Use 'linewidth' for line thickness\n  geom_point(size = 2) +  # Optionally, adjust point size\n  scale_x_continuous(breaks = seq(min(annual_totals$Year), max(annual_totals$Year), by = 1),  # Ensure years are whole numbers\n                     labels = scales::label_number(big.mark = \"\")) +  # Format year labels as whole numbers\n  scale_color_manual(values = colors) +  # Apply custom colors\n  labs(title = \"Deaths Over Time by Cause of Death\",\n       x = \"Year\",\n       y = \"Total Observed Deaths\") +  #  y-axis label\n  theme_minimal() +\n  theme(legend.position = \"bottom\",  # Adjust legend position if necessary\n        legend.title = element_blank(),  # Remove legend title\n        legend.key.width = unit(0.5, \"cm\"),  # Adjust legend key width\n        legend.key.height = unit(0.5, \"cm\"),  # Adjust legend key height\n        legend.text = element_text(size = 10))  # Adjust legend text size\n\n\n\n\nCauses of Death:\n\nCancer: This is the leading cause of death according to the graph, and the number of deaths has been slowly increasing over time. There appears to be a slight acceleration in the rate of increase in recent years.\nHeart Disease: Heart disease is the second leading cause of death. The number of deaths due to heart disease has fluctuated somewhat over the years, but there is a general downward trend from 2008 to 2013. Since then, there has been a slight uptick.\nUnintentional Injuries: Unintentional injuries are the third leading cause of death according to the graph. The number deaths due to unintentional injuries has been increasing steadily over time.\nChronic Lower Respiratory Disease and Stroke: Chronic lower respiratory disease and stroke have the two lowest numbers of deaths depicted in the graph. The number of deaths due to these causes appears to be relatively stable.\n\n3) Visualization: Causes of Death over time (Percentages).\n\n# Aggregate total observed deaths by Year and Cause of Death, excluding \"United States\"\nannual_totals &lt;- data_cleaned %&gt;%\n  filter(State != \"United States\") %&gt;%\n  group_by(Year, `Cause of Death`) %&gt;%\n  summarize(Total_Observed_Deaths = sum(`Observed Deaths`, na.rm = TRUE), .groups = 'drop')\n\n# Calculate total deaths by Year\ntotal_deaths_by_year &lt;- annual_totals %&gt;%\n  group_by(Year) %&gt;%\n  summarize(Total_Deaths = sum(Total_Observed_Deaths, na.rm = TRUE))\n\n# Merge the total deaths with the annual totals to calculate percentages\nannual_totals_with_percentages &lt;- annual_totals %&gt;%\n  left_join(total_deaths_by_year, by = \"Year\") %&gt;%\n  mutate(Percentage_of_Deaths = (Total_Observed_Deaths / Total_Deaths) * 100)\n\n# Reorder the Cause of Death factor levels based on total percentage\nannual_totals_with_percentages &lt;- annual_totals_with_percentages %&gt;%\n  mutate(`Cause of Death` = reorder(`Cause of Death`, Percentage_of_Deaths, FUN = sum))\n\n# Create a bar plot with percentages and labels, with reordered bars\nggplot(annual_totals_with_percentages, aes(x = Year, y = Percentage_of_Deaths, fill = `Cause of Death`)) +\n  geom_bar(stat = \"identity\", position = \"stack\") +  # Stacked bar plot\n  geom_text(aes(label = sprintf(\"%.1f%%\", Percentage_of_Deaths), y = Percentage_of_Deaths), \n            position = position_stack(vjust = 0.5), size = 3.5, color = \"white\") +  # Add percentage labels inside bars\n  labs(title = \"Percentage of Observed Deaths by Cause of Death Over Time\",  # Update title\n       x = \"Year\",\n       y = \"Percentage of Total Deaths\") +\n  scale_x_continuous(breaks = seq(min(annual_totals_with_percentages$Year), max(annual_totals_with_percentages$Year), by = 1), \n                     labels = scales::label_number(big.mark = \"\")) +  # Format year labels\n  scale_fill_manual(values = colors) +  # Apply custom colors\n  theme_minimal() +\n  theme(legend.position = \"bottom\",  # Adjust legend position if necessary\n        legend.title = element_blank(),  # Remove legend title\n        legend.key.width = unit(0.5, \"cm\"),  # Adjust legend key width\n        legend.key.height = unit(0.5, \"cm\"),  # Adjust legend key height\n        legend.text = element_text(size = 10))  # Adjust legend text size\n\n\n\n\nPercentage of Deaths Over Time:\n\nCancer is the leading cause of death in the United States, representing around 40% of total deaths from the top 5 causes.\nHeart Disease is the second leading cause of death (after cancer). The graph shows a slight decrease in the percentage of deaths due to heart disease over time.\nUnintentional Injuries the graph shows a slight increase in the percentage of deaths due to unintentional injuries, meaning more deaths have occurred from unintentional causes over time.\nChronic Lower Respiratory Disease the graph suggests a slight increase in deaths due to chronic lower respiratory disease.\nStroke appears to have the lowest percentage of deaths among the top causes depicted in the graph. The percentage has remained relatively stable over time, with a possible slight decrease.\n\nOverall, the graph shows the percentage of deaths by cause over time. The leading causes of death are cancer and heart disease, but the percentage of deaths due to these causes is slowly decreasing. Deaths due to unintentional injuries show a slight upward trend, while chronic lower respiratory disease and stroke show a stable or slightly increasing percentage of deaths.\n4) Visualization: Causes of death by state.\n\n# Exclude rows where State is \"United States\" and Total_Observed_Deaths is below 650,000\nstate_observed_deaths &lt;- data_cleaned %&gt;%\n  filter(State != \"United States\") %&gt;%\n  group_by(State) %&gt;%\n  summarize(Total_Observed_Deaths = sum(`Observed Deaths`, na.rm = TRUE)) %&gt;%\n  filter(Total_Observed_Deaths &gt;= 650000) %&gt;%  # Filter out states with less than 650,000 observed deaths\n  arrange(desc(Total_Observed_Deaths))\n\n# Plotting the state-wise distribution of observed deaths excluding United States\nggplot(state_observed_deaths, aes(x = reorder(State, -Total_Observed_Deaths), y = Total_Observed_Deaths)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", width = 0.7) +  # Adjust bar width as needed\n  labs(title = \"Total Observed Deaths by State (Excluding United States)\",\n       x = \"State\",\n       y = \"Total Observed Deaths\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 7))  # Adjust angle and size\n\n\n\n\nHere are the top ten states with the most deaths according to the graph:\n\nCalifornia\nTexas\nFlorida 4.New York\nPennsylvania\nOhio\nIllinois\nMichigan\nNorth Carolina\nGeorgia\n\nIntrigued by this, we will now dive deeper and explore the leading cause of death in these top 10 states. This will help provide and understanding mortality patterns across the country.\n\n# Specify the states of interest\nstates_of_interest &lt;- c(\"California\", \"Texas\", \"Florida\", \"New York\", \"Pennsylvania\", \n                        \"Ohio\", \"Illinois\", \"Michigan\", \"North Carolina\", \"Georgia\")\n\n# Filter data for specified states and calculate percentages by cause of death\nstate_cause_percentages &lt;- data_cleaned %&gt;%\n  filter(State %in% states_of_interest) %&gt;%\n  group_by(State, `Cause of Death`) %&gt;%\n  summarize(Percentage = sum(`Observed Deaths`) / sum(data_cleaned$`Observed Deaths`) * 100, .groups = 'drop') %&gt;%\n  arrange(State, desc(Percentage))  # Arrange by State and descending Percentage\n\n# Plotting separate bar plots for each state showing percentage distribution of causes of death\nplots_list &lt;- lapply(states_of_interest, function(state) {\n  state_data &lt;- state_cause_percentages %&gt;%\n    filter(State == state) %&gt;%\n    arrange(desc(Percentage))  # Sort within each state from highest to lowest percentage\n  \n  ggplot(state_data, aes(x = reorder(`Cause of Death`, -Percentage), y = Percentage, fill = `Cause of Death`)) +\n    geom_bar(stat = \"identity\", width = 0.7) +  # Bar plot\n    geom_text(aes(label = sprintf(\"%.1f%%\", Percentage), y = Percentage), \n              position = position_stack(vjust = 0.5), size = 3.5, color = \"white\") +  # Add percentage labels inside bars\n    labs(title = paste(\"Distribution of Causes of Death in\", state),\n         x = \"Cause of Death\",\n         y = \"Percentage\") +\n    scale_fill_manual(values = colors) +  # Apply custom colors\n    theme_minimal() +\n    theme(axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate x-axis labels if needed\n          plot.title = element_text(hjust = 0.5))  # Center title horizontally\n})\n\n# Print each plot separately\nfor (i in seq_along(plots_list)) {\n  print(plots_list[[i]])\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalifornia\n\n\nCancer: 2%\nHeart Disease: 1.4%\nUnintentional Injury: 0.7%\nChronic Lower Respiratory Disease: 0.3%\nStroke: 0.3%\n\n\nTexas\n\n\nCancer: 1.4%\nHeart Disease: 1.1%\nUnintentional Injury: 0.6%\nChronic Lower Respiratory Disease: 0.2%\nStroke:0.2%\n\n\nFlorida\n\n\nCancer: 1.4%\nHeart Disease: 1.0%\nUnintentional Injury: 0.5%\nChronic Lower Respiratory Disease:0.2%\nStroke: 0.2%\n\n4.New York:\n\nCancer: 1.3%\nHeart Disease: 0.9%\nUnintentional Injury: 0.3%\nChronic Lower Respiratory Disease: 0.2%\nStroke: 0.1%\n\n\nPennsylvania\n\n\nCancer: 1.0%\nHeart Disease: 0.7%\nUnintentional Injury: 0.4%\nChronic Lower Respiratory Disease: 0.1%\nStroke: 0.1%\n\n\nOhio\n\n\nCancer: 0.9%\nHeart Disease: 0.7%\nUnintentional Injury: 0.3%\nChronic Lower Respiratory Disease: 0.2%\nStroke: 0.1%\n\n\nIllinois\n\n\nCancer: 0.9%\nHeart Disease: 0.6%\nUnintentional Injury: 0.3%\nChronic Lower Respiratory Disease: 0.1%\nStroke: 0.1%\n\n\nMichigan\n\n\nCancer: 0.7%\nHeart Disease: 0.6%\nUnintentional Injury: 0.2%\nChronic Lower Respiratory Disease: 0.1%\nStroke: 0.1%\n\n\nNorth Carolina\n\n\nCancer: 0.7%\nHeart Disease: 0.5%\nUnintentional Injury: 0.3%\nChronic Lower Respiratory Disease: 0.1%\nStroke: 0.1%\n\n\nGeorgia\n\n\nCancer: 0.6%\nHeart Disease: 0.5%\nUnintentional Injury: 0.3%\nChronic Lower Respiratory Disease: 0.1%\nStroke: 0.1%"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#conclusion",
    "href": "cdcdata-exercise/cdcdata-exercise.html#conclusion",
    "title": "CDC Data Exercise",
    "section": "Conclusion:",
    "text": "Conclusion:\nThe analysis of data from 2005 to 2015 reveals that Cancer is the leading cause of death, steadily rising over the years. Heart Disease follows, showing a slight decrease until 2013, and then a slight increase. Unintentional Injuries are continuously rising, presenting a growing public health concern. State-wise, California, Texas, and Florida report the highest number of deaths, with Cancer and Heart Disease consistently ranking at the top across all states analyzed.\nAlthough deaths from Chronic Lower Respiratory Disease and Stroke remained stable from 2005 to 2015, any small increases in these areas should be watched closely to prevent them from becoming bigger health problems. The steady rise in deaths from Unintentional Injuries shows a need for targeted public health efforts and preventive measures to reduce these incidents. The state-by-state analysis reveals that Cancer and Heart Disease are the leading causes of death in all states, but the differences between states might be influenced by other factors. The visualizations and summary statistics highlight important trends and patterns and are valuable tools for policymakers.\nFuture research could look into underlying factors like demographics, policies, and access to affordable healthcare behind the trends seen in Cancer, Heart Disease, and Unintentional Injuries. A deeper understanding of these factors can lead to better health policies and interventions."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#this-section-was-contributed-by-collin-real",
    "href": "cdcdata-exercise/cdcdata-exercise.html#this-section-was-contributed-by-collin-real",
    "title": "CDC Data Exercise",
    "section": "This section was contributed by Collin Real",
    "text": "This section was contributed by Collin Real"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#create-synthetic-dataset",
    "href": "cdcdata-exercise/cdcdata-exercise.html#create-synthetic-dataset",
    "title": "CDC Data Exercise",
    "section": "Create synthetic dataset",
    "text": "Create synthetic dataset\n\nlibrary(tibble)\nlibrary(dplyr)\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Define the number of observations\nn &lt;- 195708\n\n# Unique values lists (adjust as needed)\nstate_list &lt;- c('Alabama', 'Alaska', 'Arizona', 'Arkansas', 'Delaware', 'California', 'Colorado', 'Connecticut', \n                 'New Jersey', 'District of Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', \n                 'Indiana', 'Iowa', 'Rhode Island', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Maryland', \n                 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri', 'Montana', 'Nebraska', \n                 'Nevada', 'New Hampshire', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', \n                 'Oklahoma', 'Oregon', 'Pennsylvania', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', \n                 'United States', 'Utah', 'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming')\n\nstate_fips_list &lt;- 1:length(state_list)  # Example FIPS codes\n\nhhs_list &lt;- c('Region 1', 'Region 2', 'Region 3', 'Region 4', 'Region 5', 'Region 6', 'Region 7', 'Region 8', \n              'Region 9', 'Region 10')  # Adjust based on actual values\n\nage_range_list &lt;- c('0-4', '5-14', '15-24', '25-34', '35-44', '45-54', '55-64', '65-74', '75-84', '85+')  # Example ranges\n\nbenchmark_list &lt;- c('Benchmark 1', 'Benchmark 2', 'Benchmark 3')  # Adjust as necessary\nlocality_list &lt;- c('Urban', 'Rural')  # Adjust based on actual values\n\n# Generate synthetic data\nsynth_df &lt;- tibble(\n  Year = sample(2005:2015, n, replace = TRUE),\n  Cause.of.Death = sample(c('Cancer', 'Chronic Lower', 'Heart Disease', 'Stroke', 'Unintentional Injury'), n, replace = TRUE),\n  State = sample(state_list, n, replace = TRUE),\n  State.FIPS.Code = sample(state_fips_list, n, replace = TRUE),\n  HHS.Region = sample(hhs_list, n, replace = TRUE),\n  Age.Range = sample(age_range_list, n, replace = TRUE),\n  Benchmark = sample(benchmark_list, n, replace = TRUE),\n  Locality = sample(locality_list, n, replace = TRUE),\n  Observed.Deaths = round(rnorm(n, mean = 2975, sd = 5000)),\n  Population = round(rnorm(n, mean = 7177289, sd = 5000)),\n  Expected.Deaths = round(rnorm(n, mean = 2113, sd = 5000)),\n  Potentially.Excess.Deaths = round(rnorm(n, mean = 875.1, sd = 5000)),\n  Percent.Potentially.Excess.Deaths = round(rnorm(n, mean = 35.73, sd = 5000))\n)\n\n# Display the synthetic data\nprint(synth_df)\n\n# A tibble: 195,708 × 13\n    Year Cause.of.Death     State State.FIPS.Code HHS.Region Age.Range Benchmark\n   &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;    \n 1  2007 Heart Disease      Iowa               39 Region 9   25-34     Benchmar…\n 2  2007 Unintentional Inj… Miss…               2 Region 7   15-24     Benchmar…\n 3  2014 Heart Disease      Nort…              20 Region 4   65-74     Benchmar…\n 4  2006 Stroke             Utah               26 Region 1   55-64     Benchmar…\n 5  2010 Cancer             Okla…              42 Region 10  75-84     Benchmar…\n 6  2015 Cancer             Virg…              23 Region 9   65-74     Benchmar…\n 7  2009 Stroke             Colo…               7 Region 10  85+       Benchmar…\n 8  2008 Chronic Lower      Arka…              33 Region 9   5-14      Benchmar…\n 9  2010 Stroke             Wisc…              46 Region 7   55-64     Benchmar…\n10  2013 Cancer             Virg…              27 Region 3   15-24     Benchmar…\n# ℹ 195,698 more rows\n# ℹ 6 more variables: Locality &lt;chr&gt;, Observed.Deaths &lt;dbl&gt;, Population &lt;dbl&gt;,\n#   Expected.Deaths &lt;dbl&gt;, Potentially.Excess.Deaths &lt;dbl&gt;,\n#   Percent.Potentially.Excess.Deaths &lt;dbl&gt;"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Over the years, through my work and academic experiences, I have learned to overcome challenges by focusing on aspects within my control. I recently realized that there is a difference between simple and easy: simple is not complicated, while easy means it requires little effort. My approach is simple: step out in faith and take that first step. When I get there, I will look back and wonder why it took me so long to start.\nI am blessed with a supportive family, friends, and a community of like-minded individuals. We are not meant to walk this life alone; the beauty of life lies in sharing the little moments with those dear to us. I have the opportunity to develop my career as a Data Analyst, learning consumer behaviorism, conducting price shops to gather competitive market pricing and provide suggestions, and developing charts, graphs, and reports to convey relevant metrics.\nIn the Spring of 2021, I earned my Bachelor of Science in Mathematics from the University of Texas at San Antonio, and I am currently enrolled in the Data Analytics Master’s program, scheduled to graduate in Spring 2025.\nThrough my work and academic experience, I have become proficient in data analysis and interpretation using Tableau and RStudio, and I am currently expanding my proficiency in Python, GRTL, and SAS(SQL). Additionally, I am skilled with Microsoft Office tools, including Excel, Word, PowerPoint, and Access. I possess strong communication skills in both English and Spanish. I am detail-oriented, able to reason logically, and prioritize efficiently. I am a quick learner with a passion for continuous self-improvement, demonstrating initiative and eagerness to learn. I also have proficient problem-solving skills.\nI am originally from a small town called Marble Falls in the Hill Country and moved to San Antonio in January 2017. I try to take advantage of everything the city offers. When I am not working on my academics or career, you are likely to find me spending time with my family, exploring the rich history of the city, and eating its authentic food.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nLearn more about Data Analytics - Data Camp"
  },
  {
    "objectID": "aboutme.html#background",
    "href": "aboutme.html#background",
    "title": "About me",
    "section": "",
    "text": "Over the years, through my work and academic experiences, I have learned to overcome challenges by focusing on aspects within my control. I recently realized that there is a difference between simple and easy: simple is not complicated, while easy means it requires little effort. My approach is simple: step out in faith and take that first step. When I get there, I will look back and wonder why it took me so long to start.\nI am blessed with a supportive family, friends, and a community of like-minded individuals. We are not meant to walk this life alone; the beauty of life lies in sharing the little moments with those dear to us. I have the opportunity to develop my career as a Data Analyst, learning consumer behaviorism, conducting price shops to gather competitive market pricing and provide suggestions, and developing charts, graphs, and reports to convey relevant metrics.\nIn the Spring of 2021, I earned my Bachelor of Science in Mathematics from the University of Texas at San Antonio, and I am currently enrolled in the Data Analytics Master’s program, scheduled to graduate in Spring 2025.\nThrough my work and academic experience, I have become proficient in data analysis and interpretation using Tableau and RStudio, and I am currently expanding my proficiency in Python, GRTL, and SAS(SQL). Additionally, I am skilled with Microsoft Office tools, including Excel, Word, PowerPoint, and Access. I possess strong communication skills in both English and Spanish. I am detail-oriented, able to reason logically, and prioritize efficiently. I am a quick learner with a passion for continuous self-improvement, demonstrating initiative and eagerness to learn. I also have proficient problem-solving skills.\nI am originally from a small town called Marble Falls in the Hill Country and moved to San Antonio in January 2017. I try to take advantage of everything the city offers. When I am not working on my academics or career, you are likely to find me spending time with my family, exploring the rich history of the city, and eating its authentic food.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nLearn more about Data Analytics - Data Camp"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "## Installing the dslabs Package from a Local File\n\n# Download the appropriate Zip file for the 'dslabs' package.\n# The exact file you need depends on your version of R.\n# In this example, the file 'dslabs_0.8.0.zip' is used, suitable for the 'r-release' version of R.\n# 'repos = NULL' tells R that the package is being installed from a local file rather than a repository.\n# 'type = \"win.binary\"' specifies that the package is a Windows binary file.\n\n# install.packages(\"C:/Users/Joaquin/School/DA - 6833 (Summer 2024)/Joaquin_Ramriez_Portfolio_II/coding-exercise/dslabs_0.8.0.zip\", \n#                 repos = NULL, \n#                 type = \"win.binary\")\nlibrary(tidyverse)  # Loads tidyverse packages, including dplyr.\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dslabs)     # Loads the dslabs.\n\nWarning: package 'dslabs' was built under R version 4.4.0\n# This function helps access the documentation for the gapminder dataset and provides detailed information about the dataset structure, variables, and source.\nhelp(gapminder)\n\nstarting httpd help server ... done\n\n# The summary() function provides statistical summary of the dataset.Which includes minimum, first quartile, median, mean, third quartile, and maximum values for each variable.\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n# The class() function is used to determine the type of object that gapminder is.\nclass(gapminder)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-data",
    "href": "coding-exercise/coding-exercise.html#processing-data",
    "title": "R Coding Exercise",
    "section": "Processing Data",
    "text": "Processing Data\n\ndata(\"gapminder\")  # Load the gapminder dataset from the dslabs package.\n\nafricadata &lt;- gapminder %&gt;%  # Filter the dataset to include only African countries.\n  filter(continent == \"Africa\")  # The filter() function is used to specifically focus on the African Country.\n\n\nstr(africadata)  # Check the structure of the 'africadata'.\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata)  # Get summary statistics for the 'africadata'.\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\n\nafrica_infant_life &lt;- africadata %&gt;%  # Created a new object with only infant_mortality and life_expectancy variables.\n  select(infant_mortality, life_expectancy)  # Select only the 'infant_mortality' and 'life_expectancy' columns.\n\nstr(africa_infant_life)  # Check the structure of the new 'africa_infant_life'.\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(africa_infant_life)  # Get summary statistics for the 'africa_infant_life'.\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n\n\nafrica_population_life &lt;- africadata %&gt;%  # Created another new object with only population and life_expectancy variables.\n  select(population, life_expectancy)  # Select only the 'population' and 'life_expectancy' columns.\n\nstr(africa_population_life)  # Check the structure of the 'africa_population_life'.\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(africa_population_life)  # Get summary statistics for the 'africa_population_life'.\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#analyzing-relationships-in-african-countries-data-plotting",
    "href": "coding-exercise/coding-exercise.html#analyzing-relationships-in-african-countries-data-plotting",
    "title": "R Coding Exercise",
    "section": "Analyzing Relationships in African Countries’ Data (Plotting)",
    "text": "Analyzing Relationships in African Countries’ Data (Plotting)\n\n# Plotting life expectancy against infant mortality.\nplot(africa_infant_life$infant_mortality, africa_infant_life$life_expectancy,\n     xlab = \"Infant Mortality\", ylab = \"Life Expectancy\",\n     main = \"Life Expectancy vs Infant Mortality in African Countries\",\n     pch = 16, col = \"blue\")\n\n\n\n# Plotting life expectancy against population size (log scale x-axis).\nplot(africa_population_life$population, africa_population_life$life_expectancy,\n     xlab = \"Population Size\", ylab = \"Life Expectancy\",\n     main = \"Life Expectancy vs Population Size in African Countries\",\n     pch = 16, col =\"red\",\n     log = \"x\")  # Setting x-axis to log scale.\n\n\n\n\nThe streaks observed in the population vs life expectancy. - The dataset ‘africadata’ includes observations from different regions represented by multiple data points. - Countries with larger populations tend to have better resources.\nThe negative correlation between infant mortality and life expectancy is higher. - Infant mortality rates tend to be associated with lower life expectancies. - Countries with higher infant mortality rates often face challenges in healthcare, sanitation, and nutrition.\nBoth visualizations highlight important demographic and health trends in Africans. Presenting both challenges and potential areas for improvement.\n\nYear 2000 from African Countries Data (More Data Processing)\n\n# Check for missing values (NA) in the 'infant_mortality' variable in africadata.\nmissing_infant_mortality &lt;- table(is.na(africadata$infant_mortality))\nmissing_infant_mortality\n\n\nFALSE  TRUE \n 2681   226 \n\n\nFiltering Data for the Year 2000\n\n# Identify years with missing data for infant mortality.\nyears_with_missing &lt;- unique(africadata$year[is.na(africadata$infant_mortality)])\nyears_with_missing\n\n [1] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974\n[16] 1975 1976 1977 1978 1979 1980 1981 2016\n\n# Selecting year 2000\nyear_selected &lt;- 2000\n\n# Filter dataset to include only data for the year 2000.\nafricadata_2000 &lt;- africadata %&gt;% \n  filter(year == year_selected)\n\n# Display the structure of the new object to verify it contains 51 observations and 9 variables.\nstr(africadata_2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n# Get a statistical summary of the new object.\nsummary(africadata_2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#plotting-life-expectancy-against-infant-mortality---year-2000-more-plotting",
    "href": "coding-exercise/coding-exercise.html#plotting-life-expectancy-against-infant-mortality---year-2000-more-plotting",
    "title": "R Coding Exercise",
    "section": "Plotting Life Expectancy against Infant Mortality - Year 2000 (More Plotting)",
    "text": "Plotting Life Expectancy against Infant Mortality - Year 2000 (More Plotting)\n\n# Plotting life expectancy against infant mortality for year 2000.\nplot(africadata_2000$infant_mortality, africadata_2000$life_expectancy,\n     xlab = \"Infant Mortality\", ylab = \"Life Expectancy\",\n     main = \"Life Expectancy vs Infant Mortality in African Countries (Year 2000)\",\n     pch = 16, col = \"blue\")\n\n\n\n# Plotting life expectancy against population size (log scale x-axis) for year 2000.\nplot(africadata_2000$population, africadata_2000$life_expectancy,\n     xlab = \"Population Size\", ylab = \"Life Expectancy\",\n     main = \"Life Expectancy vs Population Size in African Countries (Year 2000)\",\n     pch = 16, col = \"red\",\n     log = \"x\")  # Setting x-axis to log scale.\n\n\n\n# Applying linear regression model to explore relationships.\nmodel_infant_life &lt;- lm(life_expectancy ~ infant_mortality, data = africadata_2000)\nmodel_pop_life &lt;- lm(life_expectancy ~ population, data = africadata_2000)\n\n# Summarize the linear regression models.\nsummary(model_infant_life)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\nsummary(model_pop_life)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africadata_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\n\nWe observe a negative correlation between life expectancy and infant mortality, higher infant mortality rates are associated with lower life expectancies.\nThe life expectancy and population size, show no clear correlation.\nThere is a statistically significant negative relation between infant_mortality and life_expectancy in African countries for the year 2000 pvalue (2.826-08). The r-squared 0.4701, tells us that infant mortality is a strong predictor of life expectancies.\nThere is no statistically significant relation between population and life_expectancy in African countries for the year 2000 pvalue (0.6159). The r-squared (0.005176), is significantly low and does not provide us meaningful information.\n\nIn conclusion, infant mortality showed a significant negative correlation with life expectancy, population size does not appear to have a significant relationship with life expectancy in African countries based on the data from the year 2000."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#this-section-contributed-by-sri-lakshmi-sudha-ganni",
    "href": "coding-exercise/coding-exercise.html#this-section-contributed-by-sri-lakshmi-sudha-ganni",
    "title": "R Coding Exercise",
    "section": "This Section contributed by SRI LAKSHMI SUDHA GANNI",
    "text": "This Section contributed by SRI LAKSHMI SUDHA GANNI\nFor this section, I will be using he mice_weight database from dslabs.\nWe are going to examine data from an experiment to see if a high fat diet increases weight.\n1.Loading Required Libraries:\n\nlibrary('dslabs')\n#library('dplyr')\nlibrary('ggplot2')\nhelp(mice_weights)\n\n2.Get an overview of the dataset:\n\nstr(mice_weights) # List of varaibles in the dataset\n\n'data.frame':   780 obs. of  7 variables:\n $ body_weight : num  27.6 23 28.7 32.6 28.6 ...\n $ bone_density: num  0.616 0.769 0.684 0.644 0.53 ...\n $ percent_fat : num  7.26 4.95 6.02 9.54 6.99 ...\n $ sex         : Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ diet        : Factor w/ 2 levels \"chow\",\"hf\": 1 1 1 1 1 1 1 1 1 1 ...\n $ gen         : Factor w/ 5 levels \"4\",\"7\",\"8\",\"9\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ litter      : Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n3.get summary of the dataset:\n\nsummary(mice_weights) # Summary of the mice weights dataset\n\n  body_weight     bone_density     percent_fat     sex       diet     gen     \n Min.   :18.13   Min.   :0.2708   Min.   : 2.552   F:398   chow:394   4 : 97  \n 1st Qu.:28.09   1st Qu.:0.4888   1st Qu.: 5.566   M:382   hf  :386   7 :195  \n Median :32.98   Median :0.5643   Median : 8.276                      8 :193  \n Mean   :34.08   Mean   :0.5697   Mean   : 8.594                      9 : 97  \n 3rd Qu.:39.37   3rd Qu.:0.6373   3rd Qu.:10.926                      11:198  \n Max.   :65.15   Max.   :0.9980   Max.   :22.154                              \n                 NA's   :4        NA's   :4                                   \n litter \n 1:442  \n 2:338  \n        \n        \n        \n        \n        \n\n\n4.determine the type of object that mice_weights is:\n\nclass(mice_weights) # Datatype of mice weights dataframe\n\n[1] \"data.frame\"\n\n\nData Processing: 1.Check for missing values:\n\nsum(is.na(mice_weights)) # Number of NA values in the dataset\n\n[1] 8\n\ndim(mice_weights) # Number of rows and columns in the dataset\n\n[1] 780   7\n\n\n2.removing the null values:\n\ncleaned_data &lt;- na.omit(mice_weights) # Remove rows with NA values\ndim(cleaned_data) # Number of rows and columns in the cleaned dataset\n\n[1] 776   7\n\n\nModeling of body weight by type of diet\nBoxplot of weight by diet:\n\nboxplot(cleaned_data$body_weight ~ cleaned_data$diet, xlab = \"Diet\", ylab = \"Weight\", main = \"Weight by Diet\") # Boxplot of weight by diet\n\n\n\n\nThe boxplot shows that the high fat diet mice are, on average, heavier than the mice on the control diet.\nComparing group means:\n\nt.test(cleaned_data$body_weight ~ cleaned_data$diet) # t-test comparing weight by diet\n\n\n    Welch Two Sample t-test\n\ndata:  cleaned_data$body_weight by cleaned_data$diet\nt = -9.2305, df = 709.9, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group chow and group hf is not equal to 0\n95 percent confidence interval:\n -6.193914 -4.021185\nsample estimates:\nmean in group chow   mean in group hf \n          31.53701           36.64455 \n\n\nThe t-test results show that the difference in weight between the high fat diet and control diet groups is statistically significant (p-value &lt; 0.05).\nHypothesis Testing:\n\nmodel &lt;- lm(body_weight ~ diet, data = cleaned_data)\nsummary(model)\n\n\nCall:\nlm(formula = body_weight ~ diet, data = cleaned_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-18.5146  -5.5614  -0.6608   4.6654  28.5054 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  31.5370     0.3866  81.574   &lt;2e-16 ***\ndiethf        5.1075     0.5510   9.269   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.674 on 774 degrees of freedom\nMultiple R-squared:  0.09992,   Adjusted R-squared:  0.09875 \nF-statistic: 85.92 on 1 and 774 DF,  p-value: &lt; 2.2e-16\n\n\nThe linear regression model shows that the high fat diet group has a statistically significant higher weight compared to the control diet group (p-value &lt; 0.05). An intecept of 31.57 and slope of 5.1075 are observed indicating a positive correlation between high fat diet and weight.\nModeling of body weight by type of diet and sex\n\ncleaned_data |&gt; ggplot(aes(diet, (body_weight), fill = sex)) + geom_boxplot() # Boxplot of weight by diet and sex\n\n\n\n\nThe boxplot shows the distribution of weight by diet. The high fat diet group has a higher average weight compared to the control diet group, and the difference is more pronounced in males.\n\nfit &lt;- lm(body_weight ~ diet*sex, data = cleaned_data)\nsummary(fit)\n\n\nCall:\nlm(formula = body_weight ~ diet * sex, data = cleaned_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-17.745  -4.260  -0.643   3.932  33.473 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  27.8286     0.4405  63.180  &lt; 2e-16 ***\ndiethf        3.8482     0.6253   6.154 1.21e-09 ***\nsexM          7.5315     0.6277  11.998  &lt; 2e-16 ***\ndiethf:sexM   2.7263     0.8948   3.047  0.00239 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.229 on 772 degrees of freedom\nMultiple R-squared:  0.4085,    Adjusted R-squared:  0.4062 \nF-statistic: 177.7 on 3 and 772 DF,  p-value: &lt; 2.2e-16\n\n\nwe can observe that\n1.Intercept (27.8286) is the estimated average body weight for the baseline group (presumably the control diet and female sex, depending on how the factors are coded) when all other predictors are held at zero.\n2.The coefficient for dietHF (3.0000) represents the estimated difference in average body weight between the high-fat diet group and the control diet\n3.sexM (7.5315): This is the additional weight estimated for males over females, assuming all else is constant.\n4.males on dietF have an additional 2.7263 units of body weight change compared to what would be predicted by dietF and sexM alone.\n5.All predictors have very small p-values (Pr(&gt;|t|)), well below 0.05, suggesting that their effects are statistically significant. Approximately 40.85% of the variance in body weight is explained by the model."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "Welcome to my digital portfolio, where I bring data to life through the projects I’ve completed as part of my Data Analytics Master’s program through UTSA.This collection highlights not just my technical skills, but my passion for uncovering hidden patterns, extracting actionable insights, and applying cutting-edge analytical techniques to tackle real-world challenges. Dive in to explore my work, see how I’ve grown as a data analyst, and feel free to check out my resume along with additional projects that showcase my expertise.\nI’m driven by curiosity and the desire to uncover stories hidden within data. Whether it’s exploring trends, building predictive models, or visualizing complex information, I’m always eager to learn and push my skills further. Every new challenge—whether in my professional work or personal projects—drives me to deepen my expertise and unlock new insights.\n\nExplore Projects: Use the navigation menu above to browse through my projects. Each project offers a glimpse into my analytical skills and problem-solving approach.\nConnect with Me: If you have any questions, feedback, or collaboration opportunities, I’d love to hear from you! Feel free to reach out via the contact information below. I’m always eager to connect with fellow data enthusiasts and professionals.\n\nLet’s embark on this data journey together!\n\n\n\n\n\n\nContact Information:\nSocial Media Links:\n\n\n\n\nEmail: joaquin.dj.ramirez@gmail.com\nLinkedIn\n\n\nPhone: +1 (830) 220-0703\nGitHub"
  },
  {
    "objectID": "index.html#contact-me",
    "href": "index.html#contact-me",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "Contact Information:\nSocial Media Links:\n\n\n\n\nEmail: joaquin.dj.ramirez@gmail.com\nLinkedIn\n\n\nPhone: +1 (830) 220-0703\nGitHub"
  },
  {
    "objectID": "predictive-modeling/predictive-modeling.html",
    "href": "predictive-modeling/predictive-modeling.html",
    "title": "Predictive Modeling",
    "section": "",
    "text": "Below is an overview of the key concepts I learned, focusing on predictive modeling techniques and analytics tools with a practical approach using R programming. The topics I explored include:\n\nData Preprocessing: Techniques for cleaning and transforming raw data to make it suitable for analysis, including handling missing values and normalizing data.\nOverfitting and Model Tuning: Understanding the risk of overfitting and employing strategies to optimize model performance through hyperparameter tuning, cross-validation, and regularization methods.\nSupervised Methods:\n\nLinear Regression: Analyzing relationships between dependent and independent variables using linear models.\nNonlinear Regression: Addressing more complex relationships between variables with nonlinear models.\nClassification: Applying algorithms to classify data into categories, using techniques such as logistic regression, decision trees, and ensemble methods.\n\nUnsupervised Methods:\n\nClustering: Grouping data into clusters to identify patterns and similarities, utilizing methods such as k-means and hierarchical clustering.\nPrincipal Component Analysis (PCA): Reducing the dimensionality of data to simplify models while preserving essential variability.\nOutlier Detection: Identifying and managing outliers to improve model accuracy and robustness.\n\nAdvanced Techniques:\n\nSupport Vector Machines (SVM): Leveraging SVM for both classification and regression tasks to find optimal decision boundaries and manage high-dimensional data.\nTree-Based Models: Implementing models such as decision trees, Random Forests, and Gradient Boosting to handle complex data structures and improve predictive accuracy.\n\n\nThrough this comprehensive approach, I gained the ability to choose, implement, and interpret predictive models for a variety of applications. I also developed the skills to create detailed and insightful data analysis reports, effectively communicating findings and supporting decision-making processes.\n\nStudent Dataset Case Study\nR offers a wide range of functions for data preprocessing, calculation, manipulation, and graphical display, and can be easily extended with new functions through downloadable packages from the Comprehensive R Archive Network (CRAN).\nAs an example, the studentdata dataset from the LearnBayes package is used, containing 657 observations across 11 variables:\nStudent student number Height height in inches Gender gender Shoes number of pairs of shoes owned Number number chosen between 1 and 10 Dvds name of movie dvds owned ToSleep time the person went to sleep the previous night (hours past midnight) WakeUp time the person woke up the next morning Haircut cost of last haircut including tip Job number of hours working on a job per week Drink usual drink at suppertime among milk, water, and pop\nInstall LearnBayes package in R/Rstudio and then access studentdata\n\n#Install the LearnBayes package\n#Keep in mind that R is case-sensitive\n\n#install.packages('LearnBayes')\n\n#You just need to install once and then you can directly use\n#so long as you access the LearnBayes package\nlibrary(LearnBayes)\n\n#Access studentdata from the LearnBayes package\ndata(studentdata)\nattach(studentdata)\n\n#show part of data\nhead(studentdata)\n\n  Student Height Gender Shoes Number Dvds ToSleep WakeUp Haircut  Job Drink\n1       1     67 female    10      5   10    -2.5    5.5      60 30.0 water\n2       2     64 female    20      7    5     1.5    8.0       0 20.0   pop\n3       3     61 female    12      2    6    -1.5    7.5      48  0.0  milk\n4       4     61 female     3      6   40     2.0    8.5      10  0.0 water\n5       5     70   male     4      5    6     0.0    9.0      15 17.5   pop\n6       6     63 female    NA      3    5     1.0    8.5      25  0.0 water\n\n\nAfter accessing the studentdata, we can now use R to answer the following questions:\n\nThe variable Dvds in the student dataset contains the number of movie DVDs owned by students in the class.\n\n\n\nConstruct a histogram of this variable using the hist command in R.\n\n\n#?hist\n# Construct a histogram of the Dvds variable\nhist(Dvds,main = \"DVDs Owned\", xlab = \"Number of DVDs\", col = \"red\")\n\n\n\n\n\nSummarize this variable using the summary command in R.\n\n\nsummary(Dvds)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   0.00   10.00   20.00   30.93   30.00 1000.00      16 \n\n\n\nUse the table command in R to construct a frequency table of the individual values of Dvds that were observed. If one constructs a barplot of these tabled values using the command barplot(table(Dvds),col=‘red’) one will see that particular response values are very popular. Is there any explanation for these popular values for the number of DVDs owned?\n\n\ntable = table(Dvds)\n\nprint(table)\n\nDvds\n   0    1    2  2.5    3    4    5    6    7    8    9   10   11   12   13   14 \n  26   10   13    1   18    9   27   14   12   12    7   78    3   20    7    4 \n  15   16   17 17.5   18   20   21   22 22.5   23   24   25 27.5   28   29   30 \n  46    1    3    1    4   83    3    3    1    3    2   31    3    1    1   45 \n  31   33   35   36   37   40   41   42   45   46   48   50   52   53   55   60 \n   1    1   12    4    1   26    1    1    5    1    2   26    1    2    1    7 \n  62   65   67   70   73   75   80   83   85   90   97  100  120  122  130  137 \n   1    2    1    4    1    3    4    1    1    1    1   10    2    1    2    1 \n 150  152  157  175  200  250  500  900 1000 \n   6    1    1    1    8    1    1    1    1 \n\nbarplot(table,col='red', main = \"DVDs Owned\", xlab = \"Number of DVDs\")\n\n\n\n\nBased on the limited information provided, we can assume there are many reasons for the number of DVDs owned. Some of these reasons include, but are not limited to: sales of DVDs, the release of new or classic DVDs, are students collecting DVDs and DVDs received as gifts. In order to dive deeper into the analysis, it would be crucial to know the name of the movies. This information could provide important details about the reasons why certain DVDs are appearing more often.\n\nThe variable Height contains the height (in inches) of each student in the class.\n\n\n\nConstruct parallel boxplots of the heights using the Gender variable. Hint: boxplot(Height~Gender)\n\n\nboxplot(Height~Gender, main = \"Height by Gender\", ylab = \"Height (inches)\")\n\n\n\n\n\nIf one assigns the boxplot output to a variable output=boxplot(Height~Gender) then output is a list that contains statistics used in constructing the boxplots. Print output to see the statistics that are stored.\n\n\noutput=boxplot(Height~Gender, main = \"Height by Gender\", ylab = \"Height (inches)\")\n\n\n\nprint(output)\n\n$stats\n      [,1] [,2]\n[1,] 57.75   65\n[2,] 63.00   69\n[3,] 64.50   71\n[4,] 67.00   72\n[5,] 73.00   76\n\n$n\n[1] 428 219\n\n$conf\n         [,1]    [,2]\n[1,] 64.19451 70.6797\n[2,] 64.80549 71.3203\n\n$out\n [1] 56 76 55 56 76 54 54 84 78 77 56 63 77 79 62 62 61 79 59 61 78 62\n\n$group\n [1] 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n\n$names\n[1] \"female\" \"male\"  \n\n\n\nOn average, how much taller are male students than female students? 3\n\n\naverage = tapply(Height, Gender, mean, na.rm = TRUE)\nprint(average)\n\n  female     male \n64.75701 70.50767 \n\n\n\navg_female_height = 64.75701 \navg_male_height = 70.50767 \n\nc =  avg_male_height - avg_female_height\nprint(c)\n\n[1] 5.75066\n\n\nMale: 70.50767 Female: 64.75701\nOn average males students are 5.75066 inches taller than female students.\n\nThe variables ToSleep and WakeUp contain, respectively, the time to bed and wake-up time for each student the previous evening. (The data are recorded as hours past midnight, so a value of −2 indicates 10 p.m.)\n\n\n\nConstruct a scatterplot of ToSleep and WakeUp.\n\n\nplot(ToSleep, WakeUp, main = \"Scatterplot: ToSleep and WakeUp\", xlab = \"Sleep-Time\", ylab = \"Wake-up Time\")\n\n\n\n\n\nFind a least-squares fit to these data using the lm command and then place the least-squares fit on the scatterplot using the abline command.\n\n\nplot(ToSleep, WakeUp, main = \"Scatterplot: ToSleep and WakeUp\", xlab = \"Sleep-Time\", ylab = \"Wake-up Time\")\nfit = lm(WakeUp~ToSleep)\nsummary(fit)\n\n\nCall:\nlm(formula = WakeUp ~ ToSleep)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.4010 -0.9628 -0.0998  0.8249  4.6125 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.96276    0.06180  128.85   &lt;2e-16 ***\nToSleep      0.42472    0.03595   11.81   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.282 on 651 degrees of freedom\n  (4 observations deleted due to missingness)\nMultiple R-squared:  0.1765,    Adjusted R-squared:  0.1753 \nF-statistic: 139.5 on 1 and 651 DF,  p-value: &lt; 2.2e-16\n\nabline(fit, col='blue', lwd=2)\n\n\n\n\n\n\nAnalysis of Glass Identification Data: Exploratory Data Analysis and Model Development\n\n#install.packages('mlbench')\n#install.packages('ggplot2')\n#install.packages('GGally')\n#install.packages('corrplot')\n#install.packages('gridExtra')\n#install.packages('kernlab')\nlibrary(kernlab) \nlibrary(mlbench)\n\nWarning: package 'mlbench' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:kernlab':\n\n    alpha\n\nlibrary(GGally)\n\nWarning: package 'GGally' was built under R version 4.3.3\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(corrplot)\n\nWarning: package 'corrplot' was built under R version 4.3.3\n\n\ncorrplot 0.92 loaded\n\nlibrary(gridExtra)\n\nWarning: package 'gridExtra' was built under R version 4.3.3\n\nlibrary(AppliedPredictiveModeling)\n\nWarning: package 'AppliedPredictiveModeling' was built under R version 4.3.3\n\nlibrary(caret)\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\n\nThe UC Irvine Machine Learning Repository contains a data set related to glass identification. The data consists of 214 glass samples labeled as one of seven class categories. There are nine predictors, including the refractive index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.\nThe data can be accessed via\n\ndata(Glass)\nstr(Glass)\n\n'data.frame':   214 obs. of  10 variables:\n $ RI  : num  1.52 1.52 1.52 1.52 1.52 ...\n $ Na  : num  13.6 13.9 13.5 13.2 13.3 ...\n $ Mg  : num  4.49 3.6 3.55 3.69 3.62 3.61 3.6 3.61 3.58 3.6 ...\n $ Al  : num  1.1 1.36 1.54 1.29 1.24 1.62 1.14 1.05 1.37 1.36 ...\n $ Si  : num  71.8 72.7 73 72.6 73.1 ...\n $ K   : num  0.06 0.48 0.39 0.57 0.55 0.64 0.58 0.57 0.56 0.57 ...\n $ Ca  : num  8.75 7.83 7.78 8.22 8.07 8.07 8.17 8.24 8.3 8.4 ...\n $ Ba  : num  0 0 0 0 0 0 0 0 0 0 ...\n $ Fe  : num  0 0 0 0 0 0.26 0 0 0 0.11 ...\n $ Type: Factor w/ 6 levels \"1\",\"2\",\"3\",\"5\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\na) Utilize suitable visualizations (employ any types of data visualization you deem appropriate) to explore the predictor variables, aiming to understand their distributions and relationships among them.\nBoxplots:\n\ndata(Glass)\nGlass$Type &lt;- as.factor(Glass$Type)\n\nboxplots &lt;- lapply(names(Glass)[1:9], function(var) {ggplot(Glass, aes_string(x = \"Type\", y = var)) + \n    geom_boxplot() + \n    ggtitle(paste(\"Boxplot of\", var)) + \n    theme_minimal() +\n    theme_dark()})\n\nWarning: `aes_string()` was deprecated in ggplot2 3.0.0.\nℹ Please use tidy evaluation idioms with `aes()`.\nℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\nboxplots_combined &lt;- do.call(grid.arrange, c(boxplots, ncol = 3))\n\n\n\n\nHistograms:\n\nhistograms &lt;- lapply(names(Glass)[1:9], function(var) {ggplot(Glass, aes_string(x = var)) + \n    geom_histogram(bins = 30) + \n    ggtitle(paste(\"Histogram of\", var)) + \n    theme_minimal() +\n    theme_classic()})\n\n\nhistogram_combined &lt;- do.call(grid.arrange, c(histograms, ncol = 3))\n\n\n\n\nI utilized both Histograms and Boxplots, you can definitely see a lot of outliers in the plots displayed. Na and Al, seem to be normally distributed but all other seem to be either skewed to the left or to the right.\nb) Do there appear to be any outliers in the data? Are any predictors skewed? Show all the work!\n\nFunction to create boxplots for each predictor:\n\n\ncreate_boxplot &lt;- function(data, var) {\n  ggplot(data, aes_string(y = var)) + \n    geom_boxplot() + \n    ggtitle(paste(\"Boxplot of\", var)) + \n    theme_minimal() +\n    theme_classic()}\n\n\nboxplots &lt;- lapply(names(Glass)[1:9], function(var) create_boxplot(Glass, var))\ndo.call(grid.arrange, c(boxplots, ncol = 3))\n\n\n\n\nPredictors Outliers from Boxplots:\n\nRI (Refractive Index): Significant outliers exist\nNa (Sodium): Fewer outliers exist, with some extreme values\nMg (Magnesium): No outliers\nAl (Aluminum): Significant Outliers exist, with extreme values\nSi (Silicon): Significant outliers exist, with extreme values\nK (Potassium): Outliers exist, fewer extreme values\nCa (Calcium): Significant outliers are existing.\nBa (Barium): Significant outliers observed, with many extreme value\nFe (Iron): Outliers observed with few extreme values\nFunction to create histograms for each predictor\n\n\ncreate_histogram &lt;- function(data, var) {\n  ggplot(data, aes_string(x = var)) + \n    geom_histogram(bins = 30) + \n    ggtitle(paste(\"Histogram of\", var)) + \n    theme_minimal() +\n    theme_classic()}\n\n\nhistograms &lt;- lapply(names(Glass)[1:9], function(var) create_histogram(Glass, var))\nhisto &lt;- do.call(grid.arrange, c(histograms, ncol = 3))\n\n\n\n\nPredictors Skewness from Histogram:\n\nRI (Refractive Index): Right - Postive skewed\nNa (Sodium): Right - Postive skewed\nMg (Magnesium):Left - Negative skewed\nAl (Aluminum): Symmetrical, balanced\nSi (Silicon): Left - Negatively skewed\nK (Potassium): Left - Negative skewed\nCa (Calcium): Symmetrical, balanced\nBa (Barium): Non symmetrical\nFe (Iron): Non symmetrical\n\nConclusion (Predictors):\nWe can determine from the observations (Boxplot and Histograms), all but one showed outliers with extreme values. Most of these predictors where either skewed to the left (negative) or to the right (positive). All this to say that we need to consider the outliers and the distribution charactertics going forward.\n\nAre there any relevant transformations of one or more predictors that might improve the classification model? Show all the work!\n\n\nGlass_transformed &lt;- Glass\nGlass_transformed[] &lt;- lapply(Glass_transformed, function(x) if (is.numeric(x)) log(x + 1) else x)\nnames(Glass_transformed)[names(Glass_transformed) != \"Type\"] &lt;- paste0(\"log_\", names(Glass_transformed)[names(Glass_transformed) != \"Type\"])\n\nGlass_sqrt_transformed &lt;- Glass\nGlass_sqrt_transformed[] &lt;- lapply(Glass_sqrt_transformed, function(x) if (is.numeric(x)) sqrt(x) else x)\nnames(Glass_sqrt_transformed)[names(Glass_sqrt_transformed) != \"Type\"] &lt;- paste0(\"sqrt_\", names(Glass_sqrt_transformed)[names(Glass_sqrt_transformed) != \"Type\"])\n\nGlass_all_transformed &lt;- cbind(Glass_transformed, Glass_sqrt_transformed[,-which(names(Glass_sqrt_transformed) == \"Type\")])\n\nlog_histograms &lt;- lapply(names(Glass_transformed)[1:9], function(var) {\n  ggplot(Glass_transformed, aes_string(x = var)) + \n    geom_histogram(bins = 30) + \n    ggtitle(paste(\"Histogram of\", var)) + \n    theme_minimal() +\n    theme_classic()})\n\nsqrt_histograms &lt;- lapply(names(Glass_sqrt_transformed)[1:9], function(var) {\n  ggplot(Glass_sqrt_transformed, aes_string(x = var)) + \n    geom_histogram(bins = 30) + \n    ggtitle(paste(\"Histogram of\", var)) + \n    theme_minimal() +\n    theme_classic()})\n\nhistogram_combined &lt;- do.call(grid.arrange, c(histograms, ncol = 3))\n\n\n\nlog_histogram_combined &lt;- do.call(grid.arrange, c(log_histograms, ncol = 3))\n\n\n\nsqrt_histogram_combined &lt;- do.call(grid.arrange, c(sqrt_histograms, ncol = 3))\n\n\n\n\n\nset.seed(123)\ntrainIndex &lt;- createDataPartition(Glass$Type, p = 0.8, list = FALSE)\nGlassTrain &lt;- Glass_all_transformed[trainIndex, ]\nGlassTest &lt;- Glass_all_transformed[-trainIndex, ]\n\nmodel_transformed &lt;- train(Type ~ ., data = GlassTrain, method = 'rpart')\n\n\npred_transformed &lt;- predict(model_transformed, newdata = GlassTest)\n\n\ncm_transformed &lt;- confusionMatrix(pred_transformed, GlassTest$Type)\n\ncat(\"Transformed Model Performance:\\n\")\n\nTransformed Model Performance:\n\nprint(cm_transformed)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  1  2  3  5  6  7\n         1 11  8  2  0  0  0\n         2  3  7  1  1  1  0\n         3  0  0  0  0  0  0\n         5  0  0  0  0  0  0\n         6  0  0  0  0  0  0\n         7  0  0  0  1  0  5\n\nOverall Statistics\n                                          \n               Accuracy : 0.575           \n                 95% CI : (0.4089, 0.7296)\n    No Information Rate : 0.375           \n    P-Value [Acc &gt; NIR] : 0.008001        \n                                          \n                  Kappa : 0.371           \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: 1 Class: 2 Class: 3 Class: 5 Class: 6 Class: 7\nSensitivity            0.7857   0.4667    0.000     0.00    0.000   1.0000\nSpecificity            0.6154   0.7600    1.000     1.00    1.000   0.9714\nPos Pred Value         0.5238   0.5385      NaN      NaN      NaN   0.8333\nNeg Pred Value         0.8421   0.7037    0.925     0.95    0.975   1.0000\nPrevalence             0.3500   0.3750    0.075     0.05    0.025   0.1250\nDetection Rate         0.2750   0.1750    0.000     0.00    0.000   0.1250\nDetection Prevalence   0.5250   0.3250    0.000     0.00    0.000   0.1500\nBalanced Accuracy      0.7005   0.6133    0.500     0.50    0.500   0.9857\n\n\nAfter I applied the transformation and achieved accuracy of 57.5%, and the Kappa is 0.371. We can say there is a level of agreement between the variables. Now looking at the classes that were observed we can see that class 1 and 2 are strong in their classification while the rest need improvement. Maybe if we refine our focus on the variblae we observe we can improve out accurancy.\n\nFit SVM model (You may refer to Chapter 4 material for details) using the following R codes: (This code will be discussed in detail in the following chapters)\n\n\nset.seed(231) \nsigDist &lt;- sigest(Type~ ., data = Glass, frac = 1)\nsigDist \n\n       90%        50%        10% \n0.03407935 0.11297847 0.62767315 \n\n\n\nsvmTuneGrid &lt;- data.frame(sigma = as.vector(sigDist)[1], C = 2^(-2:10)) \nsvmTuneGrid \n\n        sigma       C\n1  0.03407935    0.25\n2  0.03407935    0.50\n3  0.03407935    1.00\n4  0.03407935    2.00\n5  0.03407935    4.00\n6  0.03407935    8.00\n7  0.03407935   16.00\n8  0.03407935   32.00\n9  0.03407935   64.00\n10 0.03407935  128.00\n11 0.03407935  256.00\n12 0.03407935  512.00\n13 0.03407935 1024.00\n\n\n\nset.seed(231)\n\nsigDist &lt;- sigest(Type ~ ., data = Glass, frac = 1)\n\nsvmTuneGrid &lt;- data.frame(sigma = as.vector(sigDist)[1], C = 2^(-2:10))\n\nsvmModel &lt;- ksvm(Type ~ ., data = Glass, type = \"C-svc\", kernel = \"rbfdot\", kpar = list(sigma = as.vector(sigDist)[1]), C = 2^(-2:10))\n\nprint(svmModel)\n\nSupport Vector Machine object of class \"ksvm\" \n\nSV type: C-svc  (classification) \n parameter : cost C = 0.25 \n  parameter : cost C = 0.5 \n  parameter : cost C = 1 \n  parameter : cost C = 2 \n  parameter : cost C = 4 \n  parameter : cost C = 8 \n  parameter : cost C = 16 \n  parameter : cost C = 32 \n  parameter : cost C = 64 \n  parameter : cost C = 128 \n  parameter : cost C = 256 \n  parameter : cost C = 512 \n  parameter : cost C = 1024 \n\nGaussian Radial Basis kernel function. \n Hyperparameter : sigma =  0.0340793487610772 \n\nNumber of Support Vectors : 205 \n\nObjective Function Value : -30.8971 -8.4786 -4.7642 -3.9839 -4.8778 -8.4545 -6.0399 -4.2908 -6.4624 -4.3643 -3.8975 -4.5021 -3.8506 -4.9211 -4.0869 \nTraining error : 0.439252 \n\n\n\nset.seed(1056)\n# Fit SVM model using 10-fold cross-validation\nsvmFit &lt;- train(Type ~ .,\n                data = Glass, method = \"svmRadial\",\n                preProcess = c(\"center\", \"scale\"),\n                tuneGrid = svmTuneGrid,\n                trControl = trainControl(method = \"repeatedcv\", repeats = 5))\n\n\nplot(svmFit, scales = list(x = list(log = 2)))\n\n\n\n\n\n\nPredicting Meat Moisture Content Using Infrared Spectroscopy: Model Comparison and Evaluation\nInfrared (IR) spectroscopy technology is used to determine the chemical makeup of a substance. The theory of IR spectroscopy holds that unique molecular structures absorb IR frequencies differently. In practice a spectrometer fires a series of IR frequencies into a sample material, and the device measures the absorbance of the sample at each individual frequency. This series of measurements creates a spectrum profile which can then be used to determine the chemical makeup of the sample material.\nA Tecator Infratec Food and Feed Analyzer instrument was used to analyze 215 samples of meat across 100 frequencies. A sample of these frequency profiles is displayed in Fig. 6.20. In addition to an IR profile, analytical chemistry determined the percent content of water, fat, and protein for each sample. If we can establish a predictive relationship between IR spectrum and fat content, then food scientists could predict a sample’s fat content with IR instead of using analytical chemistry. This would provide costs savings, since analytical chemistry is a more expensive, time-consuming process\na) Start R and use these commands to load the data:\n\nlibrary(caret)\ndata(tecator)\n\n# use ?tecator to see more details\n?tecator\n\nstarting httpd help server ... done\n\n# Check the structure of the data\nstr(absorp)\n\n num [1:215, 1:100] 2.62 2.83 2.58 2.82 2.79 ...\n\nstr(endpoints)\n\n num [1:215, 1:3] 60.5 46 71 72.8 58.3 44 44 69.3 61.4 61.4 ...\n\n\nThe matrix absorp contains the 100 absorbance values for the 215 samples, while matrix endpoints contain the percent of moisture, fat, and protein in columns 1–3, respectively. To be more specific\n\n# Assign the percent content to variables\nmoisture &lt;- endpoints[,1]\nfat &lt;- endpoints[,2]\nprotein &lt;- endpoints[,3]\n\n\nsummary(moisture)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  39.30   55.55   65.70   63.20   71.80   76.60 \n\n#print(moisture)\n\n\nsummary(fat)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.90    7.30   14.00   18.14   28.00   49.10 \n\n#print(fat)\n\n\nsummary(protein)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  11.00   15.35   18.70   17.68   20.10   21.80 \n\n#print(protein)\n\n\n# Check for missing values\nsum(is.na(absorp))\n\n[1] 0\n\nsum(is.na(moisture))\n\n[1] 0\n\nsum(is.na(fat))\n\n[1] 0\n\nsum(is.na(protein))\n\n[1] 0\n\n\nb) In this example the predictors are the measurements at the individual frequencies. Because the frequencies lie in a systematic order (850–1,050nm), the predictors have a high degree of correlation. Hence, the data lie in a smaller dimension than the total number of predictors (215). Use PCA to determine the effective dimension of these data. What is the effective dimension?\n\n#  PCA on the absorp data\npca_model &lt;- prcomp(absorp, center = TRUE, scale. = TRUE)\n\n# Summary of PCA \nsummary(pca_model)\n\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     9.9311 0.9847 0.52851 0.33827 0.08038 0.05123 0.02681\nProportion of Variance 0.9863 0.0097 0.00279 0.00114 0.00006 0.00003 0.00001\nCumulative Proportion  0.9863 0.9960 0.99875 0.99990 0.99996 0.99999 0.99999\n                           PC8      PC9     PC10     PC11     PC12     PC13\nStandard deviation     0.01961 0.008564 0.006739 0.004442 0.003361 0.001867\nProportion of Variance 0.00000 0.000000 0.000000 0.000000 0.000000 0.000000\nCumulative Proportion  1.00000 1.000000 1.000000 1.000000 1.000000 1.000000\n                           PC14      PC15      PC16      PC17      PC18\nStandard deviation     0.001377 0.0009449 0.0008641 0.0007558 0.0006977\nProportion of Variance 0.000000 0.0000000 0.0000000 0.0000000 0.0000000\nCumulative Proportion  1.000000 1.0000000 1.0000000 1.0000000 1.0000000\n                            PC19      PC20      PC21      PC22      PC23\nStandard deviation     0.0005884 0.0004628 0.0003897 0.0003341 0.0003123\nProportion of Variance 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\nCumulative Proportion  1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n                            PC24      PC25     PC26      PC27      PC28\nStandard deviation     0.0002721 0.0002616 0.000211 0.0001954 0.0001857\nProportion of Variance 0.0000000 0.0000000 0.000000 0.0000000 0.0000000\nCumulative Proportion  1.0000000 1.0000000 1.000000 1.0000000 1.0000000\n                            PC29      PC30      PC31      PC32      PC33\nStandard deviation     0.0001729 0.0001656 0.0001539 0.0001473 0.0001392\nProportion of Variance 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\nCumulative Proportion  1.0000000 1.0000000 1.0000000 1.0000000 1.0000000\n                            PC34      PC35      PC36     PC37     PC38\nStandard deviation     0.0001339 0.0001269 0.0001082 0.000104 9.98e-05\nProportion of Variance 0.0000000 0.0000000 0.0000000 0.000000 0.00e+00\nCumulative Proportion  1.0000000 1.0000000 1.0000000 1.000000 1.00e+00\n                            PC39      PC40      PC41      PC42     PC43\nStandard deviation     9.081e-05 8.668e-05 8.026e-05 7.762e-05 7.36e-05\nProportion of Variance 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.00e+00\nCumulative Proportion  1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.00e+00\n                            PC44      PC45     PC46      PC47      PC48\nStandard deviation     6.808e-05 6.541e-05 6.44e-05 5.897e-05 5.422e-05\nProportion of Variance 0.000e+00 0.000e+00 0.00e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00 1.00e+00 1.000e+00 1.000e+00\n                            PC49      PC50      PC51      PC52      PC53\nStandard deviation     5.027e-05 4.893e-05 4.608e-05 4.419e-05 4.037e-05\nProportion of Variance 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n                            PC54    PC55     PC56      PC57      PC58      PC59\nStandard deviation     3.854e-05 3.8e-05 3.64e-05 3.497e-05 3.443e-05 3.264e-05\nProportion of Variance 0.000e+00 0.0e+00 0.00e+00 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.0e+00 1.00e+00 1.000e+00 1.000e+00 1.000e+00\n                            PC60     PC61      PC62      PC63      PC64\nStandard deviation     3.104e-05 3.04e-05 2.959e-05 2.844e-05 2.699e-05\nProportion of Variance 0.000e+00 0.00e+00 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.00e+00 1.000e+00 1.000e+00 1.000e+00\n                            PC65      PC66      PC67      PC68      PC69\nStandard deviation     2.586e-05 2.388e-05 2.364e-05 2.284e-05 2.173e-05\nProportion of Variance 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n                            PC70      PC71     PC72      PC73      PC74\nStandard deviation     2.058e-05 1.997e-05 1.93e-05 1.854e-05 1.807e-05\nProportion of Variance 0.000e+00 0.000e+00 0.00e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00 1.00e+00 1.000e+00 1.000e+00\n                            PC75      PC76      PC77      PC78      PC79\nStandard deviation     1.728e-05 1.693e-05 1.612e-05 1.569e-05 1.516e-05\nProportion of Variance 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n                            PC80      PC81      PC82      PC83      PC84\nStandard deviation     1.445e-05 1.408e-05 1.356e-05 1.275e-05 1.224e-05\nProportion of Variance 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n                            PC85     PC86      PC87      PC88      PC89\nStandard deviation     1.178e-05 1.09e-05 1.045e-05 1.009e-05 9.396e-06\nProportion of Variance 0.000e+00 0.00e+00 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.00e+00 1.000e+00 1.000e+00 1.000e+00\n                            PC90     PC91      PC92     PC93      PC94\nStandard deviation     8.728e-06 8.27e-06 7.613e-06 6.83e-06 6.383e-06\nProportion of Variance 0.000e+00 0.00e+00 0.000e+00 0.00e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.00e+00 1.000e+00 1.00e+00 1.000e+00\n                            PC95      PC96      PC97      PC98      PC99\nStandard deviation     5.946e-06 5.478e-06 4.826e-06 4.521e-06 4.164e-06\nProportion of Variance 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\nCumulative Proportion  1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n                           PC100\nStandard deviation     4.122e-06\nProportion of Variance 0.000e+00\nCumulative Proportion  1.000e+00\n\n\n\n# Scree plot to visualize the variance explained by each component\nscreeplot(pca_model, type = \"lines\", main = \"PCA Model\")\n\n\n\n\nBased on both the PCA results above, first the pca summary output, and then screeplot visualization, tells me how much of the total variance is explained as PC are added. The following observations come from the output above:\n\nPC1 explains 98.63% of the total variance.\nPC2: 0.97% proportion variance, and 99.60% cumulative proportion.\nPC3: 0.279% proportion variance, and 99.875% cumulative proportion.\nPC4: 0.114% proportion variance, and 99.99% cumulative proportion.\n\nDecision:\nAs shown in the screeplot and the summary, PC1 explains a very high percentage (98.63%), majority of the variability is captured here. The rest of the pc’s explain additional variance, but they’re unlikely to provide meaningful information.\nc) Split the data into a training and a test set the response of the percentage of moisture, pre-process the data, and build at least three models described in this chapter (i.e., ordinary least squares, PCR, PLS, Ridge, and ENET). For those models with tuning parameters, what are the optimal values of the tuning parameter(s)?\n\nset.seed(123) \ntrainIndex &lt;- createDataPartition(moisture, p = 0.7, list = FALSE)\ntrainData &lt;- absorp[trainIndex, ]\ntestData &lt;- absorp[-trainIndex, ]\ntrainMoisture &lt;- moisture[trainIndex]\ntestMoisture &lt;- moisture[-trainIndex]\n\n\ntrainData &lt;- as.data.frame(trainData)\ntestData &lt;- as.data.frame(testData)\ncolnames(trainData) &lt;- paste0(\"V\", 1:ncol(trainData))\ncolnames(testData) &lt;- paste0(\"V\", 1:ncol(testData))\n\n\npreProcValues &lt;- preProcess(trainData, method = c(\"center\", \"scale\", \"pca\"))\ntrainTransformed &lt;- predict(preProcValues, trainData)\ntestTransformed &lt;- predict(preProcValues, testData)\n\n\nset.seed(123)\nols_model &lt;- train(trainTransformed, trainMoisture, method = \"lm\")\nols_model\n\nLinear Regression \n\n152 samples\n  2 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 152, 152, 152, 152, 152, 152, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  8.521083  0.3199381  6.646874\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\n\nset.seed(123)\npcr_model &lt;- train(trainTransformed, trainMoisture, method = \"pcr\", trControl = trainControl(method = \"cv\"))\npcr_model\n\nPrincipal Component Analysis \n\n152 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 136, 136, 137, 138, 136, 138, ... \nResampling results:\n\n  RMSE      Rsquared   MAE     \n  8.929481  0.2947719  7.356878\n\nTuning parameter 'ncomp' was held constant at a value of 1\n\n\n\nset.seed(123)\npls_model &lt;- train(trainTransformed, trainMoisture, method = \"pls\", trControl = trainControl(method = \"cv\"))\npls_model\n\nPartial Least Squares \n\n152 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 136, 136, 137, 138, 136, 138, ... \nResampling results:\n\n  RMSE      Rsquared   MAE    \n  8.918747  0.2967856  7.34553\n\nTuning parameter 'ncomp' was held constant at a value of 1\n\n\n\nset.seed(123)\nridge_model &lt;- train(trainTransformed, trainMoisture, method = \"ridge\", trControl = trainControl(method = \"cv\"))\nridge_model\n\nRidge Regression \n\n152 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 136, 136, 137, 138, 136, 138, ... \nResampling results across tuning parameters:\n\n  lambda  RMSE      Rsquared   MAE     \n  0e+00   8.543739  0.3903416  6.778162\n  1e-04   8.543732  0.3903418  6.778158\n  1e-01   8.536797  0.3904599  6.774300\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was lambda = 0.1.\n\n\n\nset.seed(123)\nglmnet &lt;- train(trainTransformed, trainMoisture, method = \"glmnet\", trControl = trainControl(method = \"cv\"))\nglmnet\n\nglmnet \n\n152 samples\n  2 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 136, 136, 137, 138, 136, 138, ... \nResampling results across tuning parameters:\n\n  alpha  lambda      RMSE      Rsquared   MAE     \n  0.10   0.01016872  8.541750  0.3903215  6.783427\n  0.10   0.10168717  8.540506  0.3903110  6.787156\n  0.10   1.01687174  8.533382  0.3900054  6.864132\n  0.55   0.01016872  8.542255  0.3902511  6.783899\n  0.55   0.10168717  8.541134  0.3901088  6.794327\n  0.55   1.01687174  8.569440  0.3871371  6.953858\n  1.00   0.01016872  8.542709  0.3902184  6.784242\n  1.00   0.10168717  8.541940  0.3898954  6.801561\n  1.00   1.01687174  8.628888  0.3822190  7.096575\n\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were alpha = 0.1 and lambda = 1.016872.\n\n\nSummary:\nOLS:\n\nRMSE: 8.521083\n\nRsquared: 0.3199381\n\nMAE: 6.646874\nTuning parameter held constant at a value of TRUE. (No tuning parameter)\n\nPCR:\n\nRMSE: 8.929481\n\nRsquared: 0.2947719\n\nMAE: 7.356878\n\nTuning parameter held constant at a value of 1.\n\nPLS:\n\nRMSE: 8.918747\n\nRsquared: 0.2967856\n\nMAE: 7.34553\n\nTuning parameter held constant at a value of 1.\n\nRidge Regression:\n\nRMSE: 8.536797\n\nRsquared: 0.3904599\n\nMAE: 6.774300\n\nTuning parameter the final value used for the model was lambda = 0.1.\n\nGlmnet:\n\nRMSE: 8.541750\n\nRsquared: 0.3903215\n\nMAE: 6.783427\nTuning parameters alpha = 0.1 and lambda = 0.1016872\n\nBased on the lowest RMSE, OLS and Ridge Regression are the better models. They also they have a high rquared explaining a higher variance proportion.\nd) Which model has the best predictive ability? Is any model significantly better or worse than the others?\nThe models are ordered from best to worst in terms of RMSE (predictive performance), based on the results provided above:\n1) OLS:\n\nRMSE: 8.521083\n\nRsquared: 0.3199381\n\nMAE: 6.646874\nTuning parameter held constant at a value of TRUE. (No tuning parameter)\n\n2) Ridge Regression:\n\nRMSE: 8.536797\n\nRsquared: 0.3904599\n\nMAE: 6.774300\n\nTuning parameter the final value used for the model was lambda = 0.1.\n\n3) Glmnet:\n\nRMSE: 8.541750\n\nRsquared: 0.3903215\n\nMAE: 6.783427\nTuning parameters alpha = 0.1 and lambda = 0.1016872\n\n4) PLS:\n\nRMSE: 8.918747\n\nRsquared: 0.2967856\n\nMAE: 7.34553\n\nTuning parameter held constant at a value of 1.\n\n5) PCR:\n\nRMSE: 8.929481\n\nRsquared: 0.2947719\n\nMAE: 7.356878\n\nTuning parameter held constant at a value of 1.\n\nThe models are ordered first by the lowest RMSE and then highest R-squared to determine their performance. Based on the criteria of lowest RMSE, the OLS model is the best, this tells me that the OLS model has the lowest prediction error. However, you can also get away with using the Ridge model because it has the second lowest RMSE, and highest rsquared, indicating minimal error and a large proportion of the variance is explained by this model.\ne) Explain which model you would use for predicting the percentage of moisture of a sample.\nThe model I would use to predict the percentage of moisture in a sample would be the one with the lowest RMSE because it has the lowest predictive error and a model with a high rsquared which explains variance proportion. In the outputs above, the model that best fits this is the Ridge model.\n\n\nComparative Performance of Machine Learning Models on Friedman’s Benchmark Data: Analyzing kNN, MARS, Neural Networks, and SVM\n7.2. Friedman (1991) introduced several benchmark data sets create by simulation. One of these simulations used the following nonlinear equation to create data:\ny = 10 sin(πx1x2) + 20(x3 − 0.5)2 + 10x4 + 5x5 + N(0, σ2)\nwhere the x values are random variables uniformly distributed between [0, 1] (there are also 5 other non-informative variables also created in the simulation). The package mlbench contains a function called mlbench.friedman1 that simulates these data:\nNote: For this exercise, you need to consider at least three of the following models: kNN, MARS, Neural Network, and Support vector machines with a specified kernel.\n\n#install.packages(\"caret\")\n# Loading libraries\nlibrary(mlbench)\nlibrary(caret)\nlibrary(earth)\n\nWarning: package 'earth' was built under R version 4.3.3\n\n\nLoading required package: Formula\n\n\nLoading required package: plotmo\n\n\nWarning: package 'plotmo' was built under R version 4.3.3\n\n\nLoading required package: plotrix\n\n\nWarning: package 'plotrix' was built under R version 4.3.2\n\nlibrary(e1071)\nlibrary(nnet)\n\n\nset.seed(200)\n\n# Generate training data\ntrainingData &lt;- mlbench.friedman1(200, sd = 1)\ntrainingData$x &lt;- data.frame(trainingData$x) # We convert the 'x' data from a matrix to a data frame. One reason is that this will give the columns names.\n\n\nfeaturePlot(trainingData$x, trainingData$y) # Visualize the data using featurePlot\n\n\n\n# Generate Test Data\ntestData &lt;- mlbench.friedman1(5000, sd = 1) # This creates a list with a vector 'y' and a matrix of predictors 'x'. Also simulate a large test set to estimate the true error rate with good precision\ntestData$x &lt;- data.frame(testData$x)\n\nScatterplot Observations:\n\nX1-X5: show a positive trend.\nX6-X10: show no specific trend, no correlation\n\n\n\nTuning several models: kNN, MARS, Neural Network, and SVM.\n\n# Train k-Nearest Neighbors (kNN) model\nknnModel &lt;- train(x = trainingData$x,\n                  y = trainingData$y,\n                  method = \"knn\",\n                  preProc = c(\"center\", \"scale\"),\n                  tuneLength = 10)\nprint(knnModel)\n\nk-Nearest Neighbors \n\n200 samples\n 10 predictor\n\nPre-processing: centered (10), scaled (10) \nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 200, 200, 200, 200, 200, 200, ... \nResampling results across tuning parameters:\n\n  k   RMSE      Rsquared   MAE     \n   5  3.466085  0.5121775  2.816838\n   7  3.349428  0.5452823  2.727410\n   9  3.264276  0.5785990  2.660026\n  11  3.214216  0.6024244  2.603767\n  13  3.196510  0.6176570  2.591935\n  15  3.184173  0.6305506  2.577482\n  17  3.183130  0.6425367  2.567787\n  19  3.198752  0.6483184  2.592683\n  21  3.188993  0.6611428  2.588787\n  23  3.200458  0.6638353  2.604529\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was k = 17.\n\n# Predict and evaluate kNN model\nknnPred &lt;- predict(knnModel, newdata = testData$x)\nknnResults &lt;- postResample(pred = knnPred, obs = testData$y)  # The function 'postResample' can be used to get the test set perforamnce values\ncat(\"\\n\\n\")\nprint(knnResults)\n\n     RMSE  Rsquared       MAE \n3.2040595 0.6819919 2.5683461 \n\n\n\n# Multivariate Adaptive Regression Splines (MARS)\nmarsModel &lt;- train(x = trainingData$x,\n                   y = trainingData$y,\n                   method = \"earth\",\n                   preProc = c(\"center\", \"scale\"),\n                   tuneLength = 10)\nprint(marsModel)\n\nMultivariate Adaptive Regression Spline \n\n200 samples\n 10 predictor\n\nPre-processing: centered (10), scaled (10) \nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 200, 200, 200, 200, 200, 200, ... \nResampling results across tuning parameters:\n\n  nprune  RMSE      Rsquared   MAE     \n   2      4.383438  0.2405683  3.597961\n   3      3.645469  0.4745962  2.930453\n   4      2.727602  0.7035031  2.184240\n   6      2.331605  0.7835496  1.833420\n   7      1.976830  0.8421599  1.562591\n   9      1.804342  0.8683110  1.410395\n  10      1.787676  0.8711960  1.386944\n  12      1.821005  0.8670619  1.419893\n  13      1.858688  0.8617344  1.445459\n  15      1.871033  0.8607099  1.457618\n\nTuning parameter 'degree' was held constant at a value of 1\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were nprune = 10 and degree = 1.\n\nmarsPred &lt;- predict(marsModel, newdata = testData$x)\nmarsResults &lt;- postResample(pred = marsPred, obs = testData$y)\ncat(\"\\n\\n\")\nprint(marsResults)\n\n    RMSE Rsquared      MAE \n1.776575 0.872700 1.358367 \n\n\n\n# Neural Network\nnnetModel &lt;- train(x = trainingData$x,\n                   y = trainingData$y,\n                   method = \"nnet\",\n                   preProc = c(\"center\", \"scale\"),\n                   tuneLength = 5,\n                   trace = FALSE,\n                   maxit = 500,\n                   linout = TRUE) # linout = TRUE for regression\nprint(nnetModel)\n\nNeural Network \n\n200 samples\n 10 predictor\n\nPre-processing: centered (10), scaled (10) \nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 200, 200, 200, 200, 200, 200, ... \nResampling results across tuning parameters:\n\n  size  decay  RMSE      Rsquared   MAE     \n  1     0e+00  2.519137  0.7353382  1.970543\n  1     1e-04  2.500942  0.7429904  1.944561\n  1     1e-03  2.475847  0.7489994  1.931115\n  1     1e-02  2.470504  0.7504027  1.922111\n  1     1e-01  2.439968  0.7557086  1.893173\n  3     0e+00  3.146237  0.6867740  2.246639\n  3     1e-04  3.123896  0.6514690  2.421103\n  3     1e-03  2.894276  0.6755563  2.274582\n  3     1e-02  2.766721  0.6975189  2.199090\n  3     1e-01  2.663439  0.7218025  2.102306\n  5     0e+00  6.450585  0.4720758  3.615483\n  5     1e-04  3.761009  0.5566309  2.708163\n  5     1e-03  3.651200  0.5926186  2.679819\n  5     1e-02  3.370460  0.6252829  2.614183\n  5     1e-01  3.052473  0.6601510  2.392260\n  7     0e+00  6.442198  0.4155727  3.821268\n  7     1e-04  4.787702  0.4624648  3.401147\n  7     1e-03  4.256500  0.5103711  3.207193\n  7     1e-02  3.819179  0.5480917  2.979782\n  7     1e-01  3.439917  0.6011807  2.741039\n  9     0e+00  5.131231  0.4728159  3.608050\n  9     1e-04  4.261980  0.4957059  3.306610\n  9     1e-03  4.014608  0.5250012  3.199011\n  9     1e-02  4.088546  0.5033594  3.233481\n  9     1e-01  3.436716  0.6038520  2.721582\n\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were size = 1 and decay = 0.1.\n\nnnetPred &lt;- predict(nnetModel, newdata = testData$x)\nnnetResults &lt;- postResample(pred = nnetPred, obs = testData$y)\ncat(\"\\n\\n\")\nprint(nnetResults)\n\n     RMSE  Rsquared       MAE \n2.6493149 0.7177213 2.0295230 \n\n\n\n# SVM\nsvmModel &lt;- train(x = trainingData$x,\n                  y = trainingData$y,\n                  method = \"svmRadial\",\n                  preProc = c(\"center\", \"scale\"),\n                  tuneLength = 10)\nprint(svmModel)\n\nSupport Vector Machines with Radial Basis Function Kernel \n\n200 samples\n 10 predictor\n\nPre-processing: centered (10), scaled (10) \nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 200, 200, 200, 200, 200, 200, ... \nResampling results across tuning parameters:\n\n  C       RMSE      Rsquared   MAE     \n    0.25  2.564825  0.7797760  2.011238\n    0.50  2.357718  0.7938560  1.837232\n    1.00  2.223469  0.8096320  1.723875\n    2.00  2.136798  0.8217596  1.659346\n    4.00  2.084793  0.8287955  1.622207\n    8.00  2.067316  0.8310680  1.611923\n   16.00  2.065727  0.8311623  1.610359\n   32.00  2.065727  0.8311623  1.610359\n   64.00  2.065727  0.8311623  1.610359\n  128.00  2.065727  0.8311623  1.610359\n\nTuning parameter 'sigma' was held constant at a value of 0.062404\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were sigma = 0.062404 and C = 16.\n\nsvmPred &lt;- predict(svmModel, newdata = testData$x)\nsvmResults &lt;- postResample(pred = svmPred, obs = testData$y)\ncat(\"\\n\\n\")\nprint(svmResults)\n\n     RMSE  Rsquared       MAE \n2.0723657 0.8258694 1.5741453 \n\n\nWhich models appear to give the best performance? Does MARS select the informative predictors (those named X1–X5)?\nPerformance Summary:\nMARS:\n\nOptimal nprune: 10\nRMSE: 1.776575\nR-squared: 0.872700\nMAE: 1.358367\n\nSVM:\n\nOptimal C: 16\nOptimal sigma: 0.068874\nRMSE: 2.0889248\nR-squared: 0.8232974\nMAE: 1.5874122\n\nNeural Network:\n\nOptimal size: 1\nOptimal decay: 0.1\nRMSE: 2.6493162\nR-squared: 0.7177209\nMAE: 2.0295251\n\nkNN:\n\nOptimal k: 19\nRMSE: 3.2286834\nR-squared: 0.6871735\nMAE: 2.5939727\n\nI have order the models from best performance to least, based on the following metric, low RMSE, high rsquared, and low MAE. In conclusion the MARS model outperforms the other models, it displays lowest RMSE, highest rsquared, lowest MAE. Now, the SVM model is also a strong contender following after the MARS model, it performs well. The Nueral Network performs alright it does have much higher RMSE than the previous two, lower rsquared and higher MAE, while the kNN model unperformed.\n\n# Variable Importance for MARS Model\ncat(\"Variable for MARS Model:\\n\")\n\nVariable for MARS Model:\n\nprint(varImp(marsModel))\n\nearth variable importance\n\n   Overall\nX1  100.00\nX4   82.78\nX2   64.18\nX5   40.21\nX3   28.14\nX6    0.00\n\n\nIn regards to the variables that are most informative they are as follows:\n\nX1: 100%\nX4: 83%\nX2: 64%\nX5: 40%\nX3: 28%\n\n\n\nEvaluating Predictor Importance in Simulated Data: A Comparative Study of Random Forest, Conditional Inference Trees, Boosted Trees, and Cubist Models\n8.1. Recreate the simulated data from Exercise 4:\n\nlibrary(mlbench)\nset.seed(200)\n\nsimulated &lt;- mlbench.friedman1(200, sd = 1)\nsimulated &lt;- cbind(simulated$x, simulated$y)\nsimulated &lt;- as.data.frame(simulated)\ncolnames(simulated)[ncol(simulated)] &lt;- \"y\"\n\n(a) Fit a random forest model to all of the predictors, then estimate the variable importance scores:\n\nlibrary(randomForest)\n\nWarning: package 'randomForest' was built under R version 4.3.3\n\n\nrandomForest 4.7-1.1\n\n\nType rfNews() to see new features/changes/bug fixes.\n\n\n\nAttaching package: 'randomForest'\n\n\nThe following object is masked from 'package:gridExtra':\n\n    combine\n\n\nThe following object is masked from 'package:ggplot2':\n\n    margin\n\nlibrary(caret)\n\n# Fit the random forest model\nrandom_forest &lt;- randomForest(y ~ ., data = simulated, importance = TRUE, ntree = 1000)\n\n# Estimate variable importance\nrandom_forest_var &lt;- varImp(random_forest, scale = FALSE)\nprint(random_forest_var)\n\n         Overall\nV1   8.732235404\nV2   6.415369387\nV3   0.763591825\nV4   7.615118809\nV5   2.023524577\nV6   0.165111172\nV7  -0.005961659\nV8  -0.166362581\nV9  -0.095292651\nV10 -0.074944788\n\n\n\nPositive Scores: V1, V2, V4, and V5 have positive scores. These are categorized as important predictors.\nNegative Scores: V6, V7, V8, V9, and V10 have negative scores. These are categorized as counterproductive predictions.\nV1 has the highest score (8.732) it is the most influential predictor in the model.\n\nDid the random forest model significantly use the uninformative predictors (V6 – V10)?\n\nNo, the variables of importance score for these predictors are either low or negative. As stated above these predictors are categorized as counterproductive, so the models performance is driven by the important predictors.\n\n(b) Now add an additional predictor that is highly correlated with one of the informative predictors. For example:\n\nsimulated$duplicate1 &lt;- simulated$V1 + rnorm(200) * .1\ncor(simulated$duplicate1, simulated$V1)\n\n[1] 0.9460206\n\n\nduplicate1 = 0.9460206\n\nset.seed(200)  \nsimulated$duplicate2 &lt;- simulated$V2 + rnorm(200) * 0.1\ncor(simulated$duplicate2, simulated$V2)\n\n[1] 0.9506982\n\n\nduplicate2 = 0.9506982\nFit another random forest model to these data. Did the importance score for V1 change? What happens when you add another predictor that is also highly correlated with V1?\n\nlibrary(randomForest)\nlibrary(caret)\n\nset.seed(200)\nmodel_of_duplicates &lt;- randomForest(y ~ ., data = simulated, importance = TRUE, ntree = 1000)\nImp_duplicates &lt;- varImp(model_of_duplicates, scale = FALSE)\nprint(Imp_duplicates)\n\n               Overall\nV1          5.60280192\nV2          5.68392894\nV3          0.46241755\nV4          7.27624754\nV5          1.72904882\nV6          0.15759142\nV7         -0.04038007\nV8         -0.08223050\nV9          0.01374080\nV10        -0.00844889\nduplicate1  4.24124904\nduplicate2  2.44934620\n\n\n\nAfter adding another predictor V1 decreased to 5.60.\nV1 and V2, ad V4 have the highest scores.\nDuplicate1, has a score of 4.24.\nDuplicate2, had a score of 2.45.\nV4 has the highest score (7.28) it is the most influential predictor in the model.\n\nAs we add more predictors that are highly correlated, it ends up balancing the distribution of the predictor and shifting the level of importance for the model.\n(c) Use the cforest function in the party package to fit a random forest model using conditional inference trees. The party package function varimp can calculate predictor importance. The conditional argument of that function toggles between the traditional importance measure and the modified version. Do these importances show the same pattern as the traditional random forest model?\n\nlibrary(party)\n\nWarning: package 'party' was built under R version 4.3.3\n\n\nLoading required package: grid\n\n\nLoading required package: mvtnorm\n\n\nLoading required package: modeltools\n\n\nLoading required package: stats4\n\n\n\nAttaching package: 'modeltools'\n\n\nThe following object is masked from 'package:kernlab':\n\n    prior\n\n\nLoading required package: strucchange\n\n\nWarning: package 'strucchange' was built under R version 4.3.3\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nLoading required package: sandwich\n\n\nWarning: package 'sandwich' was built under R version 4.3.3\n\nset.seed(200)  \ncforest_model &lt;- cforest(y ~ ., data = simulated, controls = cforest_unbiased(ntree = 1000))\ncforest_conditional &lt;- varimp(cforest_model, conditional = TRUE)\nprint(cforest_conditional)\n\n          V1           V2           V3           V4           V5           V6 \n 1.889829629  3.264726775  0.004320600  5.551127933  0.914778235  0.007554850 \n          V7           V8           V9          V10   duplicate1   duplicate2 \n 0.014874927 -0.008556608  0.005090607 -0.003604917  2.074263861  0.555345761 \n\n\n\nset.seed(200)  \ncforest_traditional &lt;- varimp(cforest_model, conditional = FALSE)\nprint(cforest_traditional)\n\n          V1           V2           V3           V4           V5           V6 \n 4.446790682  5.209317057  0.011349761  7.518264422  1.440050685 -0.007794484 \n          V7           V8           V9          V10   duplicate1   duplicate2 \n 0.032232290 -0.017239934  0.012003443 -0.007523583  4.992002245  1.845200350 \n\n\n\n\n\n\nTraditional\nConditional\n\n\n\n\nV1\n4.446790682\n1.889829629\n\n\nV2\n5.209317057\n3.264726775\n\n\nV3\n0.011349761\n0.004320600\n\n\nV4\n7.518264422\n5.551127933\n\n\nV5\n1.440050685\n0.914778235\n\n\nV6\n-0.007794484\n0.007554850\n\n\nV7\n0.032232290\n0.014874927\n\n\nV8\n-0.017239934\n-0.008556608\n\n\nV9\n0.012003443\n0.005090607\n\n\nV10\n-0.007523583\n-0.003604917\n\n\nduplicate1\n4.992002245\n2.074263861\n\n\nduplicate2\n1.845200350\n0.555345761\n\n\n\nIn summary, while the general pattern of importance is similar, where V4 has the highest score followed by V2. Additionally, V6 through V10 remain unimportant. The conditional model has a more balance dispenserment while the traditional highlights more the importance of some variables.\n(d) Repeat this process with different tree models, such as boosted trees and Cubist. Does the same pattern occur?\n\nBoosted Trees\n\n\nlibrary(gbm)\n\nWarning: package 'gbm' was built under R version 4.3.3\n\n\nLoaded gbm 2.2.2\n\n\nThis version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3\n\nset.seed(200)\ngbm_model &lt;- gbm(y ~ ., data = simulated, distribution = \"gaussian\", n.trees = 1000, interaction.depth = 3)\nsummary(gbm_model)\n\n\n\n\n                  var   rel.inf\nV4                 V4 27.564962\nV2                 V2 18.081613\nV1                 V1 13.457616\nduplicate1 duplicate1 11.043677\nV5                 V5 10.647818\nV3                 V3  7.468840\nduplicate2 duplicate2  4.082592\nV7                 V7  2.232902\nV6                 V6  1.579418\nV8                 V8  1.322100\nV10               V10  1.312882\nV9                 V9  1.205581\n\n\nIn short the Boosted Model, still considers the level of importance from V6-V10 to be unimportant. The most important variable here is V4 which matches the previous models, followed by V2 and V1. This seem to be in alignment with the other approaches where the level of importance lies within either V4, V1 or V2.\n\nCubist Model\n\n\nlibrary(Cubist)\n\nWarning: package 'Cubist' was built under R version 4.3.3\n\nlibrary(caret)\n\nset.seed(200)\ncubist_model &lt;- train(y ~ ., data = simulated, method = \"cubist\", trControl = trainControl(method = \"cv\"))\ncubist_varimp &lt;- varImp(cubist_model, scale = FALSE)\nprint(cubist_varimp)\n\ncubist variable importance\n\n           Overall\nV2            55.0\nV1            52.0\nV4            49.0\nduplicate1    39.0\nV5            38.0\nV3            32.5\nV6            21.5\nduplicate2     4.5\nV8             0.0\nV7             0.0\nV9             0.0\nV10            0.0\n\nsummary(cubist_varimp)\n\n           Length Class      Mode     \nimportance 1      data.frame list     \nmodel      1      -none-     character\ncalledFrom 1      -none-     character\n\n\nAs for the Cubist Model, you have some similarities, where V7-V10 remain unimportant and gives a slightly higher score to V6. However, in comparison to the other scores, V2 has the highest score followed by V1 then V4. This also aligns with the other methods where the level of importance is given to the op three variables either V1, V2, or V4.\nOverall the pattern remains almost unchanged you have the order of importance shift between variables, but most of the attention lies within V1, V2, V4. The counterproductive variables are pretty much the same besides in the last model, where it give V6 a higher score, but the pattern remains unchanges for the most part.\n\n\n[Exploring Predictive Modeling and Data Analysis: An Investigation into Housing Data, Soybean Disease Prediction, Oil Classification, and Statistical Concepts]\n\nlibrary(MASS)\n\nThis exercise involves the Boston housing data set.\na) To begin, load in the Boston data set. Since the Boston data set is part of the MASS library, you need to install the MASS package into R/Rstudio and then access the package as follows:\n\n#Boston\n\n?Boston #Read about the data set using \n\nHow many rows are in this Boston data set? How many columns? What do the rows and columns represent?\n\ndata(\"Boston\")\n\nBased on the information provided we have the following:\nRows:506-observations in the dataset for the Boston area.\nColumns: 14-variables, each column represents different variables. They are as followed:\n\ncrim: per capita crime rate by town.\nzn: proportion of residential land zoned for lots over 25,000 sq. ft.\nindus: proportion of non-retail business acres per town.\nchas: Charles River dummy variable (1 if tract bounds river; 0 otherwise).\nnox: nitrogen oxides concentration (parts per 10 million).\nrm: average number of rooms per dwelling.\nage: proportion of owner-occupied units built prior to 1940.\ndis: weighted mean of distances to five Boston employment centers.\nrad: index of accessibility to radial highways.\ntax: full-value property tax rate per $10,000.\nptratio: pupil-teacher ratio by town.\nblack: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\nlstat: percentage of lower status of the population.\nmedv: median value of owner-occupied homes in $1000s.\n\nb) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.\n\npairs(Boston, pch = 20, cex = 1, labels = colnames(Boston))\n\n\n\n\nOR\n\npredictors &lt;- colnames(Boston)\n\npar(mfrow = c(3, 3)) \n\nfor (i in 1:length(predictors)) \n  {for (j in (i+1):length(predictors))\n    {predictor_x &lt;- predictors[i]\n    predictor_y &lt;- predictors[j]\n    \n    if (!is.na(predictor_x) && !is.na(predictor_y)) \n    \n      {plot(Boston[[predictor_x]], Boston[[predictor_y]],\n           xlab = predictor_x, ylab = predictor_y,\n           main = paste(\"Scatterplot:\", predictor_x, \"and\", predictor_y))\n      \n      \n      abline(lm(Boston[[predictor_y]] ~ Boston[[predictor_x]]), col = \"red\")}}}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nI have provided two displays. The first combines all pairwise plots in a single view, the second organizes them for a better visualization. After reviewing the pairwise plots, I observed both positive and negative correlations, as well as some no-correlation, and some outliers. Below are a few observations, and as I proceed with the homework I will call out other observations:\n\nrad and zn: Negative correlation. Areas in Boston with more accessible radial highways have less residential zoning.\nage and lstat: Positive correlation. Older homes have higher proportions of lower status population.\nnox and tax: Positive correlation. Higher nitrogen oxides concentration levels are found in areas with higher property taxes.\nchas: No significant correlation. Most of the variables associated (chas) - proximity to the Charles River are not significantly.\nindus and tax: Positive correlation. Industrialized areas tend to have higher property taxes.\ncrim and medv: Negative correlation. Higher crime rates are found in areas with median home values.\n\nc) Are any of the predictors associated with per capita crime rate? If so, explain the relationship.\n\npar(mfrow = c(2, 2)) \n\n# Repeating the same as in 'b'\nfor (i in 1:length(predictors)) \n  {for (j in (i+1):length(predictors))\n    {predictor_x &lt;- predictors[i]\n    predictor_y &lt;- predictors[j]\n    \n    if (!is.na(predictor_x) && !is.na(predictor_y)) \n      \n      {if (predictor_x == \"crim\" || predictor_y == \"crim\")   # Checking for crim as a predictor\n       \n        # Repeating the same as in 'b'\n        {plot(Boston[[predictor_x]], Boston[[predictor_y]],\n             xlab = predictor_x, ylab = predictor_y,\n             main = paste(\"Scatterplot:\", predictor_x, \"and\", predictor_y))\n        \n        if (predictor_x == \"crim\") \n          {abline(lm(Boston[[predictor_y]] ~ Boston[[predictor_x]]), col = \"red\")}}}}}\n\n\n\n\n\n\n\n\n\n\n\n\n\nYes, there are predictors associated with the per capita crime rate. The following observation where made from the plots above, I also used the linear regression line to help me.\nNegative correlation: - crim and zn: There’s a slight negative correlation. - crim and dis: There’s a negative correlation. - crim and black: There’s a negative correlation. - crim and medv: There’s a negative correlation. - crim and indus: There’s a positive correlation.\nPositive correlation: - crim and nox: There’s a positive correlation.\n- crim and rm: There’s a slight negative correlation. - crim and age: There’s a positive correlation.\n- crim and rad: There’s a positive correlation.\n- crim and tax: There’s a positive correlation.\n- crim and ptratio: There’s a slight positive correlation. - crim and lstat: There’s a positive correlation.\nNo correlation: - crim and chas: There’s no clear correlation.\nIn conclusion the plots suggest that areas with higher nitrogen oxide/pollution, industrial areas, older homes, accessibility to radial highways, taxes, and lower status of population are likely to have higher crime rates. On the other hand, areas with more residential land zoning, larger homes, greater distance to five employment centers, and black population tend to have lower crime rates.\nd) Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Comment on the range of each predictor.\n\nhist(Boston$crim, main = \"Histogram: crim\", xlab = \"Per Capita Crime Rate\", col = \"blue\")\n\n\n\nhist(Boston$tax, main = \"Histogram: tax\", xlab = \"Tax Rate\", col = \"red\")\n\n\n\n# census tracts with high crime rates and tax rates\nhigh_crime = quantile(Boston$crim, 0.95)  \nhigh_tax = quantile(Boston$tax, 0.95)  \n\nhigh_crime_tracts = Boston[Boston$crim &gt; high_crime, ]\nhigh_tax_tracts = Boston[Boston$tax &gt; high_tax, ]\n\nrangecrim = range(Boston$crim, na.rm = TRUE)\nrangetax = range(Boston$tax, na.rm = TRUE)\n\nAccording to the output there exist census tracts with high crime rates and tax rates, especially in the top 5%. The per capita crime rate ranges from 0.00632 to 88.97620, and the property tax rate range from 187 to 711. This just tells me that there are significant variability for both predictors.\n\n# range of each predictor\npredictor_ranges &lt;- apply(Boston, 2, range)\npredictor_ranges\n\n         crim  zn indus chas   nox    rm   age     dis rad tax ptratio  black\n[1,]  0.00632   0  0.46    0 0.385 3.561   2.9  1.1296   1 187    12.6   0.32\n[2,] 88.97620 100 27.74    1 0.871 8.780 100.0 12.1265  24 711    22.0 396.90\n     lstat medv\n[1,]  1.73    5\n[2,] 37.97   50\n\n\nAccording to the output there exist census tracts with high crime rates and tax rates, especially in the top 5%. The per capita crime rate ranges from 0.00632 to 88.97620, and the property tax rate range from 187 to 711. This just tells me that there are significant variability for both predictors. In addition to the census tract associated with high crime rates and tax rates, we can also note the rest of the predictors.\n\nZn-proportion of residential land zoned rate range: 0 to 100\nindus-proportion of non-retail business acres per town rate range: 0.46 to 27.74\nchas-Charles River rate range: 0 to 1\nnox-nitrogen oxides concentration rate range: 0.385 to 0.871 (parts per 10 million)\nrm-average number of rooms rate range: 3.561 to 8.780\nage-older home rate range: 2.9 to 100\ndis-distances to employment centres rate range: 1.1296 to 12.1265\nrad-accessibility to radial highways: 1 to 24\nptratio-pupil-teacher ratio by town rate range: 12.6 to 22\nblack-proportion of black population: 0.32 to 396.90\nlstat-lower status of the population rate range: 1.73 to 37.97\nmedv-median value of owner-occupied rate range: 5 to 50\n\nJust reiterating what was previously stated, these predictors demonstrate that there is significant variability.\ne) How many of the census tracts in this data set bound the Charles river?\n\ntracts_chas = sum(Boston$chas == 1)\ntracts_chas\n\n[1] 35\n\n\nSince, the predictor chas - Charles River, 1 is if tract bounds river. They’re 35 census tracts.\n(f) What is the median pupil-teacher ratio among the towns in this data set?\n\nmedian_ptratio = median(Boston$ptratio)\nmedian_ptratio\n\n[1] 19.05\n\n\nThe median pupil-teacher ratio by town is 19.05.\n\n\nSoybean case study\nThe soybean data can also be found at the UC Irvine Machine Learning Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.\n\nlibrary(VIM)    \n\nWarning: package 'VIM' was built under R version 4.3.3\n\n\nLoading required package: colorspace\n\n\nVIM is ready to use.\n\n\nSuggestions and bug-reports can be submitted at: https://github.com/statistikat/VIM/issues\n\n\n\nAttaching package: 'VIM'\n\n\nThe following object is masked from 'package:datasets':\n\n    sleep\n\nlibrary(mice)   \n\nWarning: package 'mice' was built under R version 4.3.3\n\n\n\nAttaching package: 'mice'\n\n\nThe following object is masked from 'package:kernlab':\n\n    convergence\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following objects are masked from 'package:base':\n\n    cbind, rbind\n\nlibrary(mlbench) \ndata(Soybean) \n?Soybean\n\na) Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?\n\nstr(Soybean)\n\n'data.frame':   683 obs. of  36 variables:\n $ Class          : Factor w/ 19 levels \"2-4-d-injury\",..: 11 11 11 11 11 11 11 11 11 11 ...\n $ date           : Factor w/ 7 levels \"0\",\"1\",\"2\",\"3\",..: 7 5 4 4 7 6 6 5 7 5 ...\n $ plant.stand    : Ord.factor w/ 2 levels \"0\"&lt;\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ precip         : Ord.factor w/ 3 levels \"0\"&lt;\"1\"&lt;\"2\": 3 3 3 3 3 3 3 3 3 3 ...\n $ temp           : Ord.factor w/ 3 levels \"0\"&lt;\"1\"&lt;\"2\": 2 2 2 2 2 2 2 2 2 2 ...\n $ hail           : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n $ crop.hist      : Factor w/ 4 levels \"0\",\"1\",\"2\",\"3\": 2 3 2 2 3 4 3 2 4 3 ...\n $ area.dam       : Factor w/ 4 levels \"0\",\"1\",\"2\",\"3\": 2 1 1 1 1 1 1 1 1 1 ...\n $ sever          : Factor w/ 3 levels \"0\",\"1\",\"2\": 2 3 3 3 2 2 2 2 2 3 ...\n $ seed.tmt       : Factor w/ 3 levels \"0\",\"1\",\"2\": 1 2 2 1 1 1 2 1 2 1 ...\n $ germ           : Ord.factor w/ 3 levels \"0\"&lt;\"1\"&lt;\"2\": 1 2 3 2 3 2 1 3 2 3 ...\n $ plant.growth   : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n $ leaves         : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n $ leaf.halo      : Factor w/ 3 levels \"0\",\"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n $ leaf.marg      : Factor w/ 3 levels \"0\",\"1\",\"2\": 3 3 3 3 3 3 3 3 3 3 ...\n $ leaf.size      : Ord.factor w/ 3 levels \"0\"&lt;\"1\"&lt;\"2\": 3 3 3 3 3 3 3 3 3 3 ...\n $ leaf.shread    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ leaf.malf      : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ leaf.mild      : Factor w/ 3 levels \"0\",\"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n $ stem           : Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n $ lodging        : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 1 1 2 1 1 1 ...\n $ stem.cankers   : Factor w/ 4 levels \"0\",\"1\",\"2\",\"3\": 4 4 4 4 4 4 4 4 4 4 ...\n $ canker.lesion  : Factor w/ 4 levels \"0\",\"1\",\"2\",\"3\": 2 2 1 1 2 1 2 2 2 2 ...\n $ fruiting.bodies: Factor w/ 2 levels \"0\",\"1\": 2 2 2 2 2 2 2 2 2 2 ...\n $ ext.decay      : Factor w/ 3 levels \"0\",\"1\",\"2\": 2 2 2 2 2 2 2 2 2 2 ...\n $ mycelium       : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ int.discolor   : Factor w/ 3 levels \"0\",\"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n $ sclerotia      : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ fruit.pods     : Factor w/ 4 levels \"0\",\"1\",\"2\",\"3\": 1 1 1 1 1 1 1 1 1 1 ...\n $ fruit.spots    : Factor w/ 4 levels \"0\",\"1\",\"2\",\"4\": 4 4 4 4 4 4 4 4 4 4 ...\n $ seed           : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ mold.growth    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ seed.discolor  : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ seed.size      : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ shriveling     : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ roots          : Factor w/ 3 levels \"0\",\"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\nsummary(Soybean)\n\n                 Class          date     plant.stand  precip      temp    \n brown-spot         : 92   5      :149   0   :354    0   : 74   0   : 80  \n alternarialeaf-spot: 91   4      :131   1   :293    1   :112   1   :374  \n frog-eye-leaf-spot : 91   3      :118   NA's: 36    2   :459   2   :199  \n phytophthora-rot   : 88   2      : 93               NA's: 38   NA's: 30  \n anthracnose        : 44   6      : 90                                    \n brown-stem-rot     : 44   (Other):101                                    \n (Other)            :233   NA's   :  1                                    \n   hail     crop.hist  area.dam    sever     seed.tmt     germ     plant.growth\n 0   :435   0   : 65   0   :123   0   :195   0   :305   0   :165   0   :441    \n 1   :127   1   :165   1   :227   1   :322   1   :222   1   :213   1   :226    \n NA's:121   2   :219   2   :145   2   : 45   2   : 35   2   :193   NA's: 16    \n            3   :218   3   :187   NA's:121   NA's:121   NA's:112               \n            NA's: 16   NA's:  1                                                \n                                                                               \n                                                                               \n leaves  leaf.halo  leaf.marg  leaf.size  leaf.shread leaf.malf  leaf.mild \n 0: 77   0   :221   0   :357   0   : 51   0   :487    0   :554   0   :535  \n 1:606   1   : 36   1   : 21   1   :327   1   : 96    1   : 45   1   : 20  \n         2   :342   2   :221   2   :221   NA's:100    NA's: 84   2   : 20  \n         NA's: 84   NA's: 84   NA's: 84                          NA's:108  \n                                                                           \n                                                                           \n                                                                           \n   stem     lodging    stem.cankers canker.lesion fruiting.bodies ext.decay \n 0   :296   0   :520   0   :379     0   :320      0   :473        0   :497  \n 1   :371   1   : 42   1   : 39     1   : 83      1   :104        1   :135  \n NA's: 16   NA's:121   2   : 36     2   :177      NA's:106        2   : 13  \n                       3   :191     3   : 65                      NA's: 38  \n                       NA's: 38     NA's: 38                                \n                                                                            \n                                                                            \n mycelium   int.discolor sclerotia  fruit.pods fruit.spots   seed    \n 0   :639   0   :581     0   :625   0   :407   0   :345    0   :476  \n 1   :  6   1   : 44     1   : 20   1   :130   1   : 75    1   :115  \n NA's: 38   2   : 20     NA's: 38   2   : 14   2   : 57    NA's: 92  \n            NA's: 38                3   : 48   4   :100              \n                                    NA's: 84   NA's:106              \n                                                                     \n                                                                     \n mold.growth seed.discolor seed.size  shriveling  roots    \n 0   :524    0   :513      0   :532   0   :539   0   :551  \n 1   : 67    1   : 64      1   : 59   1   : 38   1   : 86  \n NA's: 92    NA's:106      NA's: 92   NA's:106   2   : 15  \n                                                 NA's: 31  \n                                                           \n                                                           \n                                                           \n\n\n\ndegenerate_predictors = sapply(Soybean, function(x) \n  {if (is.factor(x)) \n    {length(unique(x)) == 1}\n   else {FALSE}})\n\ndegenerate_predictors\n\n          Class            date     plant.stand          precip            temp \n          FALSE           FALSE           FALSE           FALSE           FALSE \n           hail       crop.hist        area.dam           sever        seed.tmt \n          FALSE           FALSE           FALSE           FALSE           FALSE \n           germ    plant.growth          leaves       leaf.halo       leaf.marg \n          FALSE           FALSE           FALSE           FALSE           FALSE \n      leaf.size     leaf.shread       leaf.malf       leaf.mild            stem \n          FALSE           FALSE           FALSE           FALSE           FALSE \n        lodging    stem.cankers   canker.lesion fruiting.bodies       ext.decay \n          FALSE           FALSE           FALSE           FALSE           FALSE \n       mycelium    int.discolor       sclerotia      fruit.pods     fruit.spots \n          FALSE           FALSE           FALSE           FALSE           FALSE \n           seed     mold.growth   seed.discolor       seed.size      shriveling \n          FALSE           FALSE           FALSE           FALSE           FALSE \n          roots \n          FALSE \n\n\nAccording to the result, the Soybean dataset has no predictors that are degenerate.\nb) Roughly 18 % of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?\n\naggr_plot &lt;- aggr(Soybean, col=c('blue', 'red'), numbers=TRUE, sortVars=TRUE, \n                  labels=names(Soybean), cex.axis=.85, gap=3, \n                  ylab=c(\"Missing data\",\"Pattern\"))\n\n\n\n\n\n Variables sorted by number of missings: \n        Variable       Count\n            hail 0.177159590\n           sever 0.177159590\n        seed.tmt 0.177159590\n         lodging 0.177159590\n            germ 0.163982430\n       leaf.mild 0.158125915\n fruiting.bodies 0.155197657\n     fruit.spots 0.155197657\n   seed.discolor 0.155197657\n      shriveling 0.155197657\n     leaf.shread 0.146412884\n            seed 0.134699854\n     mold.growth 0.134699854\n       seed.size 0.134699854\n       leaf.halo 0.122986823\n       leaf.marg 0.122986823\n       leaf.size 0.122986823\n       leaf.malf 0.122986823\n      fruit.pods 0.122986823\n          precip 0.055636896\n    stem.cankers 0.055636896\n   canker.lesion 0.055636896\n       ext.decay 0.055636896\n        mycelium 0.055636896\n    int.discolor 0.055636896\n       sclerotia 0.055636896\n     plant.stand 0.052708638\n           roots 0.045387994\n            temp 0.043923865\n       crop.hist 0.023426061\n    plant.growth 0.023426061\n            stem 0.023426061\n            date 0.001464129\n        area.dam 0.001464129\n           Class 0.000000000\n          leaves 0.000000000\n\n\nThe highest proportion of missing data are as follows:\nHail - 17.7% Server - 17.7% Seed.tmt - 17.7% Germ - 16.4% lodging - 17.7% There are other in the 15% range, and in the 14%, and so on.\nMost of the variables seem to be missing data, you have some variables that are not missing (class, and leaves). After looking at the visualization, the missing data does not have a relationship to classes.\nc) Develop a strategy for handling missing data, either by eliminating predictors or imputation.\n\nmissing_na &lt;- colSums(is.na(Soybean))\n\n# Identify predictors with more than 99 missing values\npredictors_removed &lt;- names(missing_na[missing_na &gt; 99])\n\n# Create a new dataset, now without the predictors  that have more than 99 missing values\nSoybean_new &lt;- Soybean[, !(names(Soybean) %in% predictors_removed)]\n\n# Summary of the cleaned dataset\nsummary(Soybean_new)\n\n                 Class          date     plant.stand  precip      temp    \n brown-spot         : 92   5      :149   0   :354    0   : 74   0   : 80  \n alternarialeaf-spot: 91   4      :131   1   :293    1   :112   1   :374  \n frog-eye-leaf-spot : 91   3      :118   NA's: 36    2   :459   2   :199  \n phytophthora-rot   : 88   2      : 93               NA's: 38   NA's: 30  \n anthracnose        : 44   6      : 90                                    \n brown-stem-rot     : 44   (Other):101                                    \n (Other)            :233   NA's   :  1                                    \n crop.hist  area.dam   plant.growth leaves  leaf.halo  leaf.marg  leaf.size \n 0   : 65   0   :123   0   :441     0: 77   0   :221   0   :357   0   : 51  \n 1   :165   1   :227   1   :226     1:606   1   : 36   1   : 21   1   :327  \n 2   :219   2   :145   NA's: 16             2   :342   2   :221   2   :221  \n 3   :218   3   :187                        NA's: 84   NA's: 84   NA's: 84  \n NA's: 16   NA's:  1                                                        \n                                                                            \n                                                                            \n leaf.malf    stem     stem.cankers canker.lesion ext.decay  mycelium  \n 0   :554   0   :296   0   :379     0   :320      0   :497   0   :639  \n 1   : 45   1   :371   1   : 39     1   : 83      1   :135   1   :  6  \n NA's: 84   NA's: 16   2   : 36     2   :177      2   : 13   NA's: 38  \n                       3   :191     3   : 65      NA's: 38             \n                       NA's: 38     NA's: 38                           \n                                                                       \n                                                                       \n int.discolor sclerotia  fruit.pods   seed     mold.growth seed.size \n 0   :581     0   :625   0   :407   0   :476   0   :524    0   :532  \n 1   : 44     1   : 20   1   :130   1   :115   1   : 67    1   : 59  \n 2   : 20     NA's: 38   2   : 14   NA's: 92   NA's: 92    NA's: 92  \n NA's: 38                3   : 48                                    \n                         NA's: 84                                    \n                                                                     \n                                                                     \n  roots    \n 0   :551  \n 1   : 86  \n 2   : 15  \n NA's: 31  \n           \n           \n           \n\n\nExplanation: Initially during part a, when I ran this model, I noticed variables that higher amounts of missing data. I decided to remove predictors that had more than 100 missing values and removing them accordingly. This way I can remove any variables that exceeds a specific number of missing values. Finally I create a new dataset that can be used for future analysis.\nBrodnjak-Vonina et al. (2005) develop a methodology for food laboratories to determine the type of oil from a sample. In their procedure, they used a gas chromatograph (an instrument that separates chemicals in a sample) to measure seven different fatty acids in an oil. These measurements would then be used to predict the type of oil in food samples. To create their model, they used 96 samples2 of seven types of oils.\nThese data can be found in the caret package using data(oil). The oil types are contained in a factor variable called oilType. The types are pumpkin (coded as A), sunflower (B), peanut (C), olive (D), soybean (E), rapeseed (F) and corn (G). In R,\n\n#install.packages(\"caret\")\nlibrary(caret)\ndata(oil)\n\n\nstr(oilType)\n\n Factor w/ 7 levels \"A\",\"B\",\"C\",\"D\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\ncat(\"\\n\\n\")\ntable(oilType)\n\noilType\n A  B  C  D  E  F  G \n37 26  3  7 11 10  2 \n\n\na) Use the sample function in base R to create a completely random sample of 60 oils. How closely do the frequencies of the random sample match the original samples? Repeat this procedure several times of understand the variation in the sampling process.\n\nset.seed(123)\n\nrandom_sample &lt;- sample(oilType, 60, replace = FALSE)\n\n# Comparing frequencies for the random sample to the original data\nrandom &lt;- table(random_sample)\noriginal &lt;- table(oilType)\n\nprint(\"Original Frequencies:\")\n\n[1] \"Original Frequencies:\"\n\nprint(original)\n\noilType\n A  B  C  D  E  F  G \n37 26  3  7 11 10  2 \n\ncat(\"\\n\\n\")\nprint(\"Random Sample Frequencies:\")\n\n[1] \"Random Sample Frequencies:\"\n\nprint(random)\n\nrandom_sample\n A  B  C  D  E  F  G \n24 17  3  3  6  5  2 \n\n\n\nThe random sample has the same frequencies for: C and G.\nThe random sample has lower frequencies for: A,B, D, E and F.\n\nb) Use the caret package function createDataPartition to create a stratified random sample. How does this compare to completely random samples?\n\n# Following same process as \"a\"\nset.seed(123)\n\n# creating stratified random sample\nstratified_sample &lt;- createDataPartition(y = oilType, p = 0.1, list = FALSE)\nstratified_data &lt;- oilType[stratified_sample]  \n\nstratified_data_df &lt;- as.data.frame(stratified_data)\n\n\ntable_stratified &lt;- table(stratified_data_df)\n\ncat(\"Original Frequencies:\\n\")\n\nOriginal Frequencies:\n\nprint(random)\n\nrandom_sample\n A  B  C  D  E  F  G \n24 17  3  3  6  5  2 \n\n# Print some space\ncat(\"\\n\\n\")\n# Print stratified sample frequencies\ncat(\"Stratified Sample Frequencies:\\n\")\n\nStratified Sample Frequencies:\n\nprint(table_stratified)\n\nstratified_data\nA B C D E F G \n4 3 1 1 2 1 1 \n\n\nThe values in the statified sample are much lower than the random sample. This is in part due to the stratified refined method, while the random sample looks at all variables within the dataset at random.\nc) With such a small samples size, what are the options for determining performance of the model? Should a test set be used?\nMethods such as K-fold, along with the train-test split method, could be an option for determining performance of the refined model.\nd) One method for understanding the uncertainty of a test set is to use a confidence interval. To obtain a confidence interval for the overall accuracy, the based R function binom.test can be used. It requires the user to input the number of samples and the number correctly classified to calculate the interval. For example, suppose a test set sample of 20 oil samples was set aside and 76 were used for model training. For this test set size and a model that is about 80 % accurate (16 out of 20 correct), the confidence interval would be computed using\n\nbinomial_result1 = binom.test(16, 20)\nprint(binomial_result1)\n\n\n    Exact binomial test\n\ndata:  16 and 20\nnumber of successes = 16, number of trials = 20, p-value = 0.01182\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.563386 0.942666\nsample estimates:\nprobability of success \n                   0.8 \n\n\nIn this case, the width of the 95% confidence interval is 37.9 %, and accuracy 80%.\nTry different samples sizes and accuracy rates to understand the trade-off between the uncertainty in the results, the model performance, and the test set size.\n\nbinom_result2 &lt;- binom.test(41, 50)\n\nprint(binom_result2)\n\n\n    Exact binomial test\n\ndata:  41 and 50\nnumber of successes = 41, number of trials = 50, p-value = 5.614e-06\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.6856306 0.9142379\nsample estimates:\nprobability of success \n                  0.82 \n\n\nIn this case, the width of the 95% confidence interval is 22.9% and accuracy 82%\n\nbinom_result3 &lt;- binom.test(90, 100)\n\nprint(binom_result3)\n\n\n    Exact binomial test\n\ndata:  90 and 100\nnumber of successes = 90, number of trials = 100, p-value &lt; 2.2e-16\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.8237774 0.9509953\nsample estimates:\nprobability of success \n                   0.9 \n\n\nIn this case, the width of the 95% confidence interval is 12.7%, and accuracy 90%\nIn conclusion, I noticed that as the sample size increases it reduces the confidence level. However, as the accuracy rate increases it tends to reduce the interval width.\nBriefly discuss what is the bias-variance tradeoff in statistics and predictive modeling.\nThe bias-variance tradeoff is when we chose lower bias which increases variance or lower variance increases bias. The objective is to find a good balance meaning that both bias and variance are at minimal.\n\n\nPredicting Fat Content in Meat Using IR Spectroscopy and Machine Learning: A Comparative Study of Predictive Models\nInfrared (IR) spectroscopy technology is used to determine the chemical makeup of a substance. The theory of IR spectroscopy holds that unique molecular structures absorb IR frequencies differently. In practice a spectrometer fires a series of IR frequencies into a sample material, and the device measures the absorbance of the sample at each individual frequency. This series of measurements creates a spectrum profile which can then be used to determine the chemical makeup of the sample material.\nA Tecator Infratec Food and Feed Analyzer instrument was used to analyze 215 samples of meat across 100 frequencies. A sample of these frequency profiles is displayed in Fig. 6.20. In addition to an IR profile, analytical chemistry determined the percent content of water, fat, and protein for each sample. If we can establish a predictive relationship between IR spectrum and fat content, then food scientists could predict a sample’s fat content with IR instead of using analytical chemistry. This would provide costs savings, since analytical chemistry is a more expensive, time-consuming process.\na) Start R and use these commands to load the data:\n\nlibrary(caret)\nlibrary(e1071)\nlibrary(nnet)\nlibrary(earth)\nlibrary(kernlab)\nlibrary(pls)\n\n\nAttaching package: 'pls'\n\n\nThe following object is masked from 'package:caret':\n\n    R2\n\n\nThe following object is masked from 'package:corrplot':\n\n    corrplot\n\n\nThe following object is masked from 'package:LearnBayes':\n\n    predplot\n\n\nThe following object is masked from 'package:stats':\n\n    loadings\n\nlibrary(kknn)\n\nWarning: package 'kknn' was built under R version 4.3.3\n\n\n\nAttaching package: 'kknn'\n\n\nThe following object is masked from 'package:caret':\n\n    contr.dummy\n\ndata(tecator)\n?tecator \nstr(absorp)\n\n num [1:215, 1:100] 2.62 2.83 2.58 2.82 2.79 ...\n\n\n\nstr(endpoints)\n\n num [1:215, 1:3] 60.5 46 71 72.8 58.3 44 44 69.3 61.4 61.4 ...\n\n\nThe matrix absorp contains the 100 absorbance values for the 215 samples, while matrix endpoints contain the percent of moisture, fat, and protein in columns 1–3, respectively. To be more specific\n\nmoisture = endpoints[,1]\nfat = endpoints[,2]\nprotein = endpoints[,3]\n\nb) Split the data into a training and a test set the response of the percentage of protein, pre-process the data as appropriate.\n\nset.seed(123)\nindex &lt;- createDataPartition(protein, p = 0.7, list = FALSE)\ntrain_data &lt;- data.frame(absorp[index, ], protein = protein[index])\ntest_data &lt;- data.frame(absorp[-index, ], protein = protein[-index])\n\n\ncombined_data &lt;- rbind(train_data, test_data)\npreProcess &lt;- preProcess(combined_data[, -ncol(combined_data)], method = \"pca\", pcaComp = 20)\ncombined_pca &lt;- predict(preProcess, combined_data[, -ncol(combined_data)])\n\n\nn_train &lt;- nrow(train_data)\ntrain_pca &lt;- cbind(combined_pca[1:n_train, ], protein = train_data$protein)\ntest_pca &lt;- cbind(combined_pca[(n_train + 1):nrow(combined_data), ], protein = test_data$protein)\n\nc) Build at least three models described Chapter 6: ordinary least squares, PCR, PLS, Ridge, and ENET. For those models with tuning parameters, what are the optimal values of the tuning parameter(s)?\nOrdinary Least Squares (OLS):\n\nset.seed(123)\nols_model &lt;- train(protein ~ ., data = train_pca, method = \"lm\")\nols_model\n\nLinear Regression \n\n152 samples\n 20 predictor\n\nNo pre-processing\nResampling: Bootstrapped (25 reps) \nSummary of sample sizes: 152, 152, 152, 152, 152, 152, ... \nResampling results:\n\n  RMSE       Rsquared   MAE      \n  0.7268192  0.9450398  0.5549019\n\nTuning parameter 'intercept' was held constant at a value of TRUE\n\n\nOLS - Linear Regression:\n\nRMSE: 0.7268192\nR-squared: 0.9450398\nMAE: 0.5549019\n\nPrincipal Component Regression (PCR):\n\nset.seed(123)\npcr_model &lt;- train(protein ~ ., data = train_data, method = \"pcr\", trControl = trainControl(method = \"cv\"))\npcr_model\n\nPrincipal Component Analysis \n\n152 samples\n100 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 136, 137, 137, 137, 136, 138, ... \nResampling results across tuning parameters:\n\n  ncomp  RMSE      Rsquared   MAE     \n  1      2.966122  0.1570382  2.520911\n  2      2.854048  0.1843866  2.337577\n  3      2.316586  0.4545058  1.848556\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was ncomp = 3.\n\n\nPrincipal Component Regression (PCR):\n\nncomp: 3\nRMSE: 2.316586\nR-squared: 0.4545058\nMAE: 1.848556\n\nPartial Least Squares (PLS):\n\nset.seed(123)\npls_model &lt;- train(protein ~ ., data = train_data, method = \"pls\", trControl = trainControl(method = \"cv\"))\npls_model\n\nPartial Least Squares \n\n152 samples\n100 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 136, 137, 137, 137, 136, 138, ... \nResampling results across tuning parameters:\n\n  ncomp  RMSE      Rsquared   MAE     \n  1      2.959109  0.1580897  2.511023\n  2      2.256430  0.5094219  1.788162\n  3      1.743833  0.6963113  1.291007\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was ncomp = 3.\n\n\nPartial Least Squares (PLS):\n\nncomp: 3\nRMSE: 1.743833\nR-squared: 0.6963113\nMAE: 1.291007\n\nd) Build nonlinear models in Chapter 7: SVM, neural network, MARS, and KNN models. Since neural networks are especially sensitive to highly correlated predictors, does pre-processing using PCA help the model? For those models with tuning parameters, what are the optimal values of the tuning parameter(s)?\nSupport Vector Machine (SVM)\n\nset.seed(123)\nsvm_model &lt;- train(protein ~ ., data = train_data, method = \"svmRadial\", trControl = trainControl(method = \"cv\"), tuneLength = 10)\nsvm_model\n\nSupport Vector Machines with Radial Basis Function Kernel \n\n152 samples\n100 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 136, 137, 137, 137, 136, 138, ... \nResampling results across tuning parameters:\n\n  C       RMSE      Rsquared   MAE     \n    0.25  2.683055  0.3123868  2.126981\n    0.50  2.431071  0.4167549  1.921845\n    1.00  2.135216  0.5501123  1.656360\n    2.00  1.934330  0.6259364  1.506506\n    4.00  1.812361  0.6698215  1.397221\n    8.00  1.744285  0.6980792  1.333155\n   16.00  1.717343  0.7085363  1.317764\n   32.00  1.676734  0.7178974  1.272854\n   64.00  1.805285  0.6829508  1.316395\n  128.00  1.910920  0.6787767  1.327847\n\nTuning parameter 'sigma' was held constant at a value of 0.05200074\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were sigma = 0.05200074 and C = 32.\n\n\nSVM:\n\nC: 32\nsigma: 0.05200074\nRMSE: 1.676734\nR-squared: 0.7178974\nMAE: 1.272854\n\nNeural Network (NN):\n\nset.seed(123)\nnn_model &lt;- train(protein ~ ., data = train_pca, method = \"nnet\", trControl = trainControl(method = \"cv\"), tuneLength = 10, trace = FALSE)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nnn_model\n\nNeural Network \n\n152 samples\n 20 predictor\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 136, 137, 137, 137, 136, 138, ... \nResampling results across tuning parameters:\n\n  size  decay         RMSE      Rsquared    MAE     \n   1    0.0000000000  16.94374         NaN  16.66501\n   1    0.0001000000  16.94374  0.14978364  16.66501\n   1    0.0002371374  16.94374  0.12567105  16.66501\n   1    0.0005623413  16.94374  0.23506957  16.66501\n   1    0.0013335214  16.94374  0.22929835  16.66501\n   1    0.0031622777  16.94375  0.36449463  16.66502\n   1    0.0074989421  16.94376  0.28410002  16.66503\n   1    0.0177827941  16.94378  0.37009598  16.66505\n   1    0.0421696503  16.94383  0.43328160  16.66510\n   1    0.1000000000  16.94394  0.43121217  16.66522\n   3    0.0000000000  16.94374         NaN  16.66501\n   3    0.0001000000  16.94374  0.05872841  16.66501\n   3    0.0002371374  16.94374  0.06529164  16.66501\n   3    0.0005623413  16.94374  0.13473509  16.66501\n   3    0.0013335214  16.94374  0.08244673  16.66501\n   3    0.0031622777  16.94374  0.28238535  16.66501\n   3    0.0074989421  16.94375  0.41696389  16.66502\n   3    0.0177827941  16.94376  0.42169655  16.66503\n   3    0.0421696503  16.94380  0.36330231  16.66507\n   3    0.1000000000  16.94387  0.39924295  16.66515\n   5    0.0000000000  16.94374         NaN  16.66501\n   5    0.0001000000  16.94374  0.04037119  16.66501\n   5    0.0002371374  16.94374  0.09412361  16.66501\n   5    0.0005623413  16.94374  0.12538589  16.66501\n   5    0.0013335214  16.94374  0.10079018  16.66501\n   5    0.0031622777  16.94374  0.22678453  16.66501\n   5    0.0074989421  16.94375  0.40603294  16.66502\n   5    0.0177827941  16.94376  0.39381393  16.66503\n   5    0.0421696503  16.94379  0.39586586  16.66506\n   5    0.1000000000  16.94385  0.38722123  16.66512\n   7    0.0000000000  16.94374         NaN  16.66501\n   7    0.0001000000  16.94374  0.05679502  16.66501\n   7    0.0002371374  16.94374  0.10851811  16.66501\n   7    0.0005623413  16.94374  0.06909546  16.66501\n   7    0.0013335214  16.94374  0.13320589  16.66501\n   7    0.0031622777  16.94374  0.23264217  16.66501\n   7    0.0074989421  16.94375  0.29182176  16.66502\n   7    0.0177827941  16.94376  0.35007398  16.66503\n   7    0.0421696503  16.94378  0.28503256  16.66505\n   7    0.1000000000  16.94384  0.32305081  16.66511\n   9    0.0000000000  16.94374         NaN  16.66501\n   9    0.0001000000  16.94374  0.07837021  16.66501\n   9    0.0002371374  16.94374  0.08903918  16.66501\n   9    0.0005623413  16.94374  0.08068229  16.66501\n   9    0.0013335214  16.94374  0.08148662  16.66501\n   9    0.0031622777  16.94374  0.22060992  16.66501\n   9    0.0074989421  16.94374  0.24151517  16.66501\n   9    0.0177827941  16.94375  0.36487565  16.66502\n   9    0.0421696503  16.94378  0.39001464  16.66505\n   9    0.1000000000  16.94382  0.40309815  16.66510\n  11    0.0000000000  16.94374         NaN  16.66501\n  11    0.0001000000  16.94374  0.08534898  16.66501\n  11    0.0002371374  16.94374  0.05052102  16.66501\n  11    0.0005623413  16.94374  0.05491134  16.66501\n  11    0.0013335214  16.94374  0.07165259  16.66501\n  11    0.0031622777  16.94374  0.10939340  16.66501\n  11    0.0074989421  16.94374  0.28477069  16.66501\n  11    0.0177827941  16.94375  0.27494690  16.66502\n  11    0.0421696503  16.94377  0.35160466  16.66504\n  11    0.1000000000  16.94382  0.32616988  16.66509\n  13    0.0000000000  16.94374         NaN  16.66501\n  13    0.0001000000  16.94374  0.07206534  16.66501\n  13    0.0002371374  16.94374  0.03338102  16.66501\n  13    0.0005623413  16.94374  0.10199462  16.66501\n  13    0.0013335214  16.94374  0.07235802  16.66501\n  13    0.0031622777  16.94374  0.07123641  16.66501\n  13    0.0074989421  16.94374  0.16657309  16.66501\n  13    0.0177827941  16.94375  0.30535658  16.66502\n  13    0.0421696503  16.94377  0.27075772  16.66504\n  13    0.1000000000  16.94381  0.29249124  16.66508\n  15    0.0000000000  16.94374         NaN  16.66501\n  15    0.0001000000  16.94374  0.16676854  16.66501\n  15    0.0002371374  16.94374  0.12257102  16.66501\n  15    0.0005623413  16.94374  0.09188033  16.66501\n  15    0.0013335214  16.94374  0.10349543  16.66501\n  15    0.0031622777  16.94374  0.04629168  16.66501\n  15    0.0074989421  16.94374  0.12201012  16.66501\n  15    0.0177827941  16.94375  0.24663325  16.66502\n  15    0.0421696503  16.94377  0.29833891  16.66504\n  15    0.1000000000  16.94381  0.32152626  16.66508\n  17    0.0000000000  16.94374         NaN  16.66501\n  17    0.0001000000  16.94374  0.11594246  16.66501\n  17    0.0002371374  16.94374  0.07114745  16.66501\n  17    0.0005623413  16.94374  0.09299386  16.66501\n  17    0.0013335214  16.94374  0.06510852  16.66501\n  17    0.0031622777  16.94374  0.06360193  16.66501\n  17    0.0074989421  16.94375  0.11744827  16.66501\n  17    0.0177827941  16.94375  0.24397690  16.66502\n  17    0.0421696503  16.94377  0.16413717  16.66504\n  17    0.1000000000  16.94380  0.28505783  16.66507\n  19    0.0000000000  16.94374         NaN  16.66501\n  19    0.0001000000  16.94374  0.08433063  16.66501\n  19    0.0002371374  16.94374  0.09962678  16.66501\n  19    0.0005623413  16.94374  0.09972566  16.66501\n  19    0.0013335214  16.94374  0.19852311  16.66501\n  19    0.0031622777  16.94374  0.04105236  16.66501\n  19    0.0074989421  16.94374  0.14339708  16.66501\n  19    0.0177827941  16.94375  0.19480536  16.66502\n  19    0.0421696503  16.94377  0.29160005  16.66504\n  19    0.1000000000  16.94380  0.25396402  16.66507\n\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were size = 1 and decay = 0.\n\n\nNeural Network:\n\nsize: 1\ndecay: 0\nRMSE: 16.94374\nR-squared: NA\nMAE: 16.66501\n\nMultivariate Adaptive Regression Splines (MARS):\n\nset.seed(123)\nmars_model &lt;- train(protein ~ ., data = train_data, method = \"earth\", trControl = trainControl(method = \"cv\"))\nmars_model\n\nMultivariate Adaptive Regression Spline \n\n152 samples\n100 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 136, 137, 137, 137, 136, 138, ... \nResampling results across tuning parameters:\n\n  nprune  RMSE      Rsquared   MAE      \n   2      2.854067  0.1814645  2.3879962\n  15      1.212796  0.8517258  0.9221938\n  28      1.357497  0.8313863  0.9689615\n\nTuning parameter 'degree' was held constant at a value of 1\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were nprune = 15 and degree = 1.\n\n\nMARS:\n\nnprune: 15\ndegree: 1\nRMSE: 1.212796\nR-squared: 0.8517258\nMAE: 0.9221938\n\nk-Nearest Neighbors (kNN):\n\nset.seed(123)\nknn_model &lt;- train(protein ~ ., data = train_data, method = \"knn\", trControl = trainControl(method = \"cv\"), tuneLength = 10)\nknn_model\n\nk-Nearest Neighbors \n\n152 samples\n100 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 136, 137, 137, 137, 136, 138, ... \nResampling results across tuning parameters:\n\n  k   RMSE      Rsquared   MAE     \n   5  2.300655  0.4780876  1.909033\n   7  2.453521  0.3946490  2.032608\n   9  2.511651  0.3773289  2.075073\n  11  2.550576  0.3616637  2.087163\n  13  2.619400  0.3441352  2.145169\n  15  2.654108  0.3080377  2.188281\n  17  2.716741  0.2774054  2.239304\n  19  2.716383  0.2760947  2.254749\n  21  2.773202  0.2384392  2.297991\n  23  2.789007  0.2262430  2.320930\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was k = 5.\n\n\nkNN:\n\nk: 5\nRMSE: 2.300655\nsquared: 0.4780876\nMAE: 1.909033\n\ne) Which model from parts c) and d) has the best predictive ability? Is any model significantly better or worse than the others?\n\n\n\n\n\n\n\n\n\n\nModel\nRMSE\nRsquared\nMAE\nRank\n\n\n\n\nOLS - Linear Regression\n0.7268192\n0.9450398\n0.5549019\n1\n\n\nPrincipal Component Regression (PCR)\n2.316586\n0.4545058\n1.848556\n6\n\n\nPartial Least Squares (PLS)\n1.743833\n0.6963113\n1.291007\n4\n\n\nSVM\n1.676734\n0.7178974\n1.272854\n3\n\n\nNeural Network\n16.94374\nNA\n16.66501\n7\n\n\nMARS\n1.212796\n0.8517258\n0.9221938\n2\n\n\nkNN\n2.300655\n0.478087\n1.909033\n5\n\n\n\nIn conclusion, I have ranked the models according to the criteria of lowest RMSE, and lowest MAE, and high rsquared. The OLS - Linear Regression outperforms all other models, followed by the MARS. The Nueral Network model performs the worst out of all the other models as it has the highest RMSE and the rsquared is not available.\nDeveloping a model to predict permeability (see Sect. 1.4 of the textbook) could save significant resources for a pharmaceutical company, while at the same time more rapidly identifying molecules that have a sufficient permeability to become a drug:\na) Start R and use these commands to load the data:\n\nlibrary(AppliedPredictiveModeling) \ndata(permeability)\nstr(fingerprints)\n\n num [1:165, 1:1107] 0 0 0 0 0 0 0 0 0 0 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:165] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ : chr [1:1107] \"X1\" \"X2\" \"X3\" \"X4\" ...\n\nstr(permeability)\n\n num [1:165, 1] 12.52 1.12 19.41 1.73 1.68 ...\n - attr(*, \"dimnames\")=List of 2\n  ..$ : chr [1:165] \"1\" \"2\" \"3\" \"4\" ...\n  ..$ : chr \"permeability\"\n\n\nThe matrix fingerprints contains the 1,107 binary molecular predictors for the 165 compounds, while permeability contains permeability response:\nb) The fingerprint predictors indicate the presence or absence of substructures of a molecule and are often sparse meaning that relatively few of the molecules contain each substructure. Filter out the predictors that have low frequencies using the nearZeroVar function from the caret package. How many predictors are left for modeling?\n\nnzv &lt;- nearZeroVar(fingerprints, saveMetrics = TRUE)\nfiltered_fingerprints &lt;- fingerprints[, !nzv$nzv]\npredictors_left &lt;- ncol(filtered_fingerprints)\npredictors_left\n\n[1] 388\n\n\nThere are 388 predictors left.\nc) Split the data into a training and a test set, pre-process the data, and tune a PLS model. How many latent variables are optimal and what is the corresponding resampled estimate of R2?\n\nset.seed(123)\nindex &lt;- createDataPartition(permeability, p = 0.7, list = FALSE)\ntrain_data &lt;- filtered_fingerprints[index, ]\ntrain_permeability &lt;- permeability[index]\ntest_data &lt;- filtered_fingerprints[-index, ]\ntest_permeability &lt;- permeability[-index]\n\n# Preprocess the data (center and scale)\npreProcValues &lt;- preProcess(train_data, method = c(\"center\", \"scale\"))\ntrain_data_transformed &lt;- predict(preProcValues, train_data)\ntest_data_transformed &lt;- predict(preProcValues, test_data)\n\nPLS\n\nset.seed(123)\npls2_model &lt;- train(train_data_transformed, train_permeability, method = \"pls\", trControl = trainControl(method = \"cv\"), tuneLength = 10)\npls2_model\n\nPartial Least Squares \n\n117 samples\n388 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 105, 105, 106, 105, 105, 105, ... \nResampling results across tuning parameters:\n\n  ncomp  RMSE      Rsquared   MAE      \n   1     13.36436  0.3433889  10.474224\n   2     12.30920  0.4595424   8.621998\n   3     12.79841  0.4713902   9.518968\n   4     13.01506  0.4586135   9.759753\n   5     13.50115  0.4188773   9.868189\n   6     13.28765  0.4391301   9.680872\n   7     12.89540  0.4604643   9.314659\n   8     12.82966  0.4653079   9.399587\n   9     12.94528  0.4583512   9.434668\n  10     13.30683  0.4341421   9.892463\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was ncomp = 2.\n\n\n\nresampled_r2 &lt;- max(pls2_model$results$Rsquared)\nresampled_r2\n\n[1] 0.4713902\n\n\nPLS Model:\n\nncomp: 2\nRMSE: 12.30920\nRsquared: 0.4595424\nMAE: 8.621998\nresampled R2: 0.4713902\n\nThere are 2 optimal latent variables and the resampled estimate rsquared is 0.4713902.\nd) Predict the response for the test set. What is the test set estimate of R2?\n\npls2_pred &lt;- predict(pls2_model, test_data_transformed)\npls2_r2 &lt;- cor(pls2_pred, test_permeability)^2\npls2_r2\n\n[1] 0.3819407\n\n\nThe test set of rsquared is 0.3819407.\ne) Try building other models discussed in this chapter. Do any have better predictive performance?\nSupport Vector Machine (SVM)\n\nset.seed(123)\nsvm2_model &lt;- train(train_data, train_permeability, method = \"svmRadial\", trControl = trainControl(method = \"cv\"), tuneLength = 10)\nsvm2_model\n\nSupport Vector Machines with Radial Basis Function Kernel \n\n117 samples\n388 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 105, 105, 106, 105, 105, 105, ... \nResampling results across tuning parameters:\n\n  C       RMSE      Rsquared   MAE     \n    0.25  13.31509  0.4937223  8.559747\n    0.50  12.03042  0.5011860  7.954428\n    1.00  11.71028  0.5103354  7.664866\n    2.00  11.87293  0.4929464  7.786952\n    4.00  12.17146  0.4658056  8.201456\n    8.00  12.33716  0.4474256  8.447651\n   16.00  12.33661  0.4453930  8.476095\n   32.00  12.29978  0.4482620  8.468869\n   64.00  12.27952  0.4499855  8.465674\n  128.00  12.27952  0.4499855  8.465674\n\nTuning parameter 'sigma' was held constant at a value of 0.003241275\nRMSE was used to select the optimal model using the smallest value.\nThe final values used for the model were sigma = 0.003241275 and C = 1.\n\nsvm2_pred &lt;- predict(svm2_model, test_data)\nsvm2_r2 &lt;- cor(svm2_pred, test_permeability)^2\ncat(\"\\n\\n\")\nsvm2_r2\n\n[1] 0.4394321\n\n\nSupport Vector Machine (SVM):\n\nsigma: 0.003241275\nC: 1\nRMSE: 11.71028\nrsquared: 0.5103354\nMAE: 7.664866\n\nThe Test Set of rsquared: 0.4394321\nRidge Regression\n\nset.seed(123)\nridge2_model &lt;- train(train_data, train_permeability, method = \"ridge\", trControl = trainControl(method = \"cv\"), tuneLength = 10)\n\nWarning: model fit failed for Fold04: lambda=0.0000000 Error in if (zmin &lt; gamhat) { : missing value where TRUE/FALSE needed\n\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nridge2_model\n\nRidge Regression \n\n117 samples\n388 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 105, 105, 106, 105, 105, 105, ... \nResampling results across tuning parameters:\n\n  lambda        RMSE          Rsquared    MAE         \n  0.0000000000      22.42497  0.27702987      15.70016\n  0.0001000000    6615.25285  0.08396519    3563.99122\n  0.0002371374   99671.84144  0.08457611   62604.59123\n  0.0005623413  170244.42077  0.14306870  104949.49993\n  0.0013335214   13949.49087  0.14095413    8819.60573\n  0.0031622777    1338.29409  0.09027590     926.17683\n  0.0074989421    4869.57307  0.19911169    3391.43119\n  0.0177827941      17.66500  0.25592818      12.61336\n  0.0421696503      15.69511  0.31773516      11.39005\n  0.1000000000      14.70937  0.37493137      10.76181\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was lambda = 0.1.\n\nridge2_pred &lt;- predict(ridge2_model, test_data)\nridge2_r2 &lt;- cor(ridge2_pred, test_permeability)^2\ncat(\"\\n\\n\")\nridge2_r2\n\n[1] 0.5311375\n\n\nRidge Rigression:\n\nlambda = 0.1\nRMSE: 14.70937\nrsquared: 0.37493137\nMAE: 10.76181\n\nThe Test Set of rsquared: 0.5311375\nLasso Regression\n\nset.seed(123)\nlasso2_model &lt;- train(train_data, train_permeability, method = \"lasso\", trControl = trainControl(method = \"cv\"), tuneLength = 10)\n\nWarning: model fit failed for Fold04: fraction=0.9 Error in if (zmin &lt; gamhat) { : missing value where TRUE/FALSE needed\n\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nlasso2_model\n\nThe lasso \n\n117 samples\n388 predictors\n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 105, 105, 106, 105, 105, 105, ... \nResampling results across tuning parameters:\n\n  fraction   RMSE      Rsquared   MAE      \n  0.1000000  12.76949  0.4681541   9.532476\n  0.1888889  13.75236  0.4031430   9.879639\n  0.2777778  14.64878  0.3661843  10.403824\n  0.3666667  15.57692  0.3421552  11.066913\n  0.4555556  16.62051  0.3177241  11.791049\n  0.5444444  17.77760  0.3013368  12.521844\n  0.6333333  18.87893  0.2964326  13.284076\n  0.7222222  20.07676  0.2902689  14.162004\n  0.8111111  21.22992  0.2871456  14.928141\n  0.9000000  21.91715  0.2806466  15.326569\n\nRMSE was used to select the optimal model using the smallest value.\nThe final value used for the model was fraction = 0.1.\n\nlasso2_pred &lt;- predict(lasso2_model, test_data)\nlasso2_r2 &lt;- cor(lasso2_pred, test_permeability)^2\ncat(\"\\n\\n\")\nlasso2_r2\n\n[1] 0.4573928\n\n\nLasso Regression:\n\nfraction: 0.1\nRMSE: 12.76949\nrsquared: 0.4681541\nMAE: 9.532476\n\nThe Test Set of rsquared: 0.4573928\nTable Summary:\n\n\n\n\n\n\n\n\n\n\nModel\nRMSE\nRsquared\nMAE\nTest_set_rsquared\n\n\n\n\nPLS Model\n12.30920\n0.4595424\n8.621998\n0.3819407\n\n\nSVM\n11.71028\n0.5103354\n7.664866\n0.4394321\n\n\nRidge Rigression\n14.70937\n0.3749313\n10.76181\n0.5311375\n\n\nLasso Regression\n12.76949\n0.4681541\n9.532476\n0.4573928\n\n\n\nf) Would you recommend any of your models to replace the permeability laboratory experiment?\nAccording to the table above, SVM has the lowest RMSE, and MAE, this model has less of a possibility to give us an error. However, the Ridge Regression model has the highest rquared, which simply means that this model can explain the highest variance out of the other models. Now, my recommendation, would be to use the SVM model for this laboratory experiment, the reason is because it has the lowest RMSE and MAE, and a decent amount of variance can be explained by this model.\nReturn to the permeability problem outlined in Problem 2. Train several nonlinear regression models and evaluate the resampling and test set performance.\nSupport Vector Machines (SVM)\n\nset.seed(123)\nsvm3_model &lt;- train(train_data_transformed, train_permeability, method = \"svmRadial\",\n                    trControl = trainControl(method = \"cv\", number = 10),\n                    tuneLength = 10)\n\nsvm3_pred &lt;- predict(svm3_model, newdata = test_data_transformed)\nsvm3_result &lt;- postResample(svm3_pred, test_permeability)\nsvm3_result\n\n      RMSE   Rsquared        MAE \n10.5483639  0.4394321  7.1154118 \n\n\nNeural Network\n\nset.seed(123)\nnn3_model &lt;- train(train_data_transformed, train_permeability, method = \"nnet\",\n                  trControl = trainControl(method = \"cv\", number = 10),\n                  tuneLength = 10, trace = FALSE, linout = TRUE)\n\nWarning: model fit failed for Fold01: size= 3, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold01: size= 5, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold01: size= 7, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold01: size= 9, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold01: size=11, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold01: size=13, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold01: size=15, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold01: size=17, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold01: size=19, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold01: size= 3, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold01: size= 5, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold01: size= 7, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold01: size= 9, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold01: size=11, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold01: size=13, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold01: size=15, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold01: size=17, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold01: size=19, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold01: size= 3, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold01: size= 5, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold01: size= 7, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold01: size= 9, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold01: size=11, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold01: size=13, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold01: size=15, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold01: size=17, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold01: size=19, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold01: size= 3, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold01: size= 5, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold01: size= 7, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold01: size= 9, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold01: size=11, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold01: size=13, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold01: size=15, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold01: size=17, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold01: size=19, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold01: size= 3, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold01: size= 5, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold01: size= 7, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold01: size= 9, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold01: size=11, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold01: size=13, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold01: size=15, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold01: size=17, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold01: size=19, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold01: size= 3, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold01: size= 5, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold01: size= 7, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold01: size= 9, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold01: size=11, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold01: size=13, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold01: size=15, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold01: size=17, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold01: size=19, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold01: size= 3, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold01: size= 5, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold01: size= 7, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold01: size= 9, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold01: size=11, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold01: size=13, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold01: size=15, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold01: size=17, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold01: size=19, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold01: size= 3, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold01: size= 5, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold01: size= 7, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold01: size= 9, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold01: size=11, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold01: size=13, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold01: size=15, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold01: size=17, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold01: size=19, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold01: size= 3, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold01: size= 5, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold01: size= 7, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold01: size= 9, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold01: size=11, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold01: size=13, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold01: size=15, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold01: size=17, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold01: size=19, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold01: size= 3, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold01: size= 5, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold01: size= 7, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold01: size= 9, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold01: size=11, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold01: size=13, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold01: size=15, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold01: size=17, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold01: size=19, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold02: size= 3, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold02: size= 5, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold02: size= 7, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold02: size= 9, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold02: size=11, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold02: size=13, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold02: size=15, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold02: size=17, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold02: size=19, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold02: size= 3, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold02: size= 5, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold02: size= 7, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold02: size= 9, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold02: size=11, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold02: size=13, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold02: size=15, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold02: size=17, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold02: size=19, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold02: size= 3, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold02: size= 5, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold02: size= 7, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold02: size= 9, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold02: size=11, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold02: size=13, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold02: size=15, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold02: size=17, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold02: size=19, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold02: size= 3, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold02: size= 5, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold02: size= 7, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold02: size= 9, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold02: size=11, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold02: size=13, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold02: size=15, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold02: size=17, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold02: size=19, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold02: size= 3, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold02: size= 5, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold02: size= 7, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold02: size= 9, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold02: size=11, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold02: size=13, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold02: size=15, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold02: size=17, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold02: size=19, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold02: size= 3, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold02: size= 5, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold02: size= 7, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold02: size= 9, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold02: size=11, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold02: size=13, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold02: size=15, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold02: size=17, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold02: size=19, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold02: size= 3, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold02: size= 5, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold02: size= 7, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold02: size= 9, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold02: size=11, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold02: size=13, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold02: size=15, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold02: size=17, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold02: size=19, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold02: size= 3, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold02: size= 5, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold02: size= 7, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold02: size= 9, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold02: size=11, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold02: size=13, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold02: size=15, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold02: size=17, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold02: size=19, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold02: size= 3, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold02: size= 5, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold02: size= 7, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold02: size= 9, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold02: size=11, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold02: size=13, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold02: size=15, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold02: size=17, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold02: size=19, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold02: size= 3, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold02: size= 5, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold02: size= 7, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold02: size= 9, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold02: size=11, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold02: size=13, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold02: size=15, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold02: size=17, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold02: size=19, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold03: size= 3, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold03: size= 5, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold03: size= 7, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold03: size= 9, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold03: size=11, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold03: size=13, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold03: size=15, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold03: size=17, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold03: size=19, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold03: size= 3, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold03: size= 5, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold03: size= 7, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold03: size= 9, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold03: size=11, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold03: size=13, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold03: size=15, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold03: size=17, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold03: size=19, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold03: size= 3, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold03: size= 5, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold03: size= 7, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold03: size= 9, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold03: size=11, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold03: size=13, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold03: size=15, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold03: size=17, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold03: size=19, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold03: size= 3, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold03: size= 5, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold03: size= 7, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold03: size= 9, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold03: size=11, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold03: size=13, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold03: size=15, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold03: size=17, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold03: size=19, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold03: size= 3, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold03: size= 5, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold03: size= 7, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold03: size= 9, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold03: size=11, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold03: size=13, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold03: size=15, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold03: size=17, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold03: size=19, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold03: size= 3, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold03: size= 5, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold03: size= 7, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold03: size= 9, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold03: size=11, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold03: size=13, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold03: size=15, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold03: size=17, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold03: size=19, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold03: size= 3, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold03: size= 5, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold03: size= 7, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold03: size= 9, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold03: size=11, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold03: size=13, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold03: size=15, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold03: size=17, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold03: size=19, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold03: size= 3, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold03: size= 5, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold03: size= 7, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold03: size= 9, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold03: size=11, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold03: size=13, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold03: size=15, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold03: size=17, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold03: size=19, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold03: size= 3, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold03: size= 5, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold03: size= 7, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold03: size= 9, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold03: size=11, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold03: size=13, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold03: size=15, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold03: size=17, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold03: size=19, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold03: size= 3, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold03: size= 5, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold03: size= 7, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold03: size= 9, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold03: size=11, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold03: size=13, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold03: size=15, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold03: size=17, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold03: size=19, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold04: size= 3, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold04: size= 5, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold04: size= 7, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold04: size= 9, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold04: size=11, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold04: size=13, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold04: size=15, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold04: size=17, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold04: size=19, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold04: size= 3, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold04: size= 5, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold04: size= 7, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold04: size= 9, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold04: size=11, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold04: size=13, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold04: size=15, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold04: size=17, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold04: size=19, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold04: size= 3, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold04: size= 5, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold04: size= 7, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold04: size= 9, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold04: size=11, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold04: size=13, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold04: size=15, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold04: size=17, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold04: size=19, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold04: size= 3, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold04: size= 5, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold04: size= 7, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold04: size= 9, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold04: size=11, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold04: size=13, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold04: size=15, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold04: size=17, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold04: size=19, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold04: size= 3, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold04: size= 5, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold04: size= 7, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold04: size= 9, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold04: size=11, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold04: size=13, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold04: size=15, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold04: size=17, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold04: size=19, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold04: size= 3, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold04: size= 5, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold04: size= 7, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold04: size= 9, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold04: size=11, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold04: size=13, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold04: size=15, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold04: size=17, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold04: size=19, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold04: size= 3, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold04: size= 5, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold04: size= 7, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold04: size= 9, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold04: size=11, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold04: size=13, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold04: size=15, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold04: size=17, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold04: size=19, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold04: size= 3, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold04: size= 5, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold04: size= 7, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold04: size= 9, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold04: size=11, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold04: size=13, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold04: size=15, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold04: size=17, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold04: size=19, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold04: size= 3, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold04: size= 5, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold04: size= 7, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold04: size= 9, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold04: size=11, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold04: size=13, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold04: size=15, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold04: size=17, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold04: size=19, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold04: size= 3, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold04: size= 5, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold04: size= 7, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold04: size= 9, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold04: size=11, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold04: size=13, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold04: size=15, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold04: size=17, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold04: size=19, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold05: size= 3, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold05: size= 5, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold05: size= 7, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold05: size= 9, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold05: size=11, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold05: size=13, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold05: size=15, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold05: size=17, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold05: size=19, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold05: size= 3, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold05: size= 5, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold05: size= 7, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold05: size= 9, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold05: size=11, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold05: size=13, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold05: size=15, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold05: size=17, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold05: size=19, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold05: size= 3, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold05: size= 5, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold05: size= 7, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold05: size= 9, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold05: size=11, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold05: size=13, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold05: size=15, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold05: size=17, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold05: size=19, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold05: size= 3, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold05: size= 5, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold05: size= 7, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold05: size= 9, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold05: size=11, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold05: size=13, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold05: size=15, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold05: size=17, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold05: size=19, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold05: size= 3, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold05: size= 5, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold05: size= 7, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold05: size= 9, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold05: size=11, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold05: size=13, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold05: size=15, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold05: size=17, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold05: size=19, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold05: size= 3, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold05: size= 5, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold05: size= 7, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold05: size= 9, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold05: size=11, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold05: size=13, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold05: size=15, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold05: size=17, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold05: size=19, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold05: size= 3, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold05: size= 5, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold05: size= 7, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold05: size= 9, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold05: size=11, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold05: size=13, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold05: size=15, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold05: size=17, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold05: size=19, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold05: size= 3, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold05: size= 5, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold05: size= 7, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold05: size= 9, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold05: size=11, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold05: size=13, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold05: size=15, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold05: size=17, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold05: size=19, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold05: size= 3, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold05: size= 5, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold05: size= 7, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold05: size= 9, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold05: size=11, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold05: size=13, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold05: size=15, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold05: size=17, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold05: size=19, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold05: size= 3, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold05: size= 5, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold05: size= 7, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold05: size= 9, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold05: size=11, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold05: size=13, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold05: size=15, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold05: size=17, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold05: size=19, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold06: size= 3, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold06: size= 5, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold06: size= 7, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold06: size= 9, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold06: size=11, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold06: size=13, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold06: size=15, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold06: size=17, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold06: size=19, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold06: size= 3, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold06: size= 5, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold06: size= 7, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold06: size= 9, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold06: size=11, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold06: size=13, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold06: size=15, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold06: size=17, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold06: size=19, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold06: size= 3, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold06: size= 5, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold06: size= 7, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold06: size= 9, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold06: size=11, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold06: size=13, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold06: size=15, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold06: size=17, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold06: size=19, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold06: size= 3, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold06: size= 5, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold06: size= 7, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold06: size= 9, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold06: size=11, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold06: size=13, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold06: size=15, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold06: size=17, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold06: size=19, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold06: size= 3, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold06: size= 5, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold06: size= 7, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold06: size= 9, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold06: size=11, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold06: size=13, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold06: size=15, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold06: size=17, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold06: size=19, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold06: size= 3, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold06: size= 5, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold06: size= 7, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold06: size= 9, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold06: size=11, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold06: size=13, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold06: size=15, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold06: size=17, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold06: size=19, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold06: size= 3, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold06: size= 5, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold06: size= 7, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold06: size= 9, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold06: size=11, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold06: size=13, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold06: size=15, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold06: size=17, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold06: size=19, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold06: size= 3, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold06: size= 5, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold06: size= 7, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold06: size= 9, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold06: size=11, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold06: size=13, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold06: size=15, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold06: size=17, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold06: size=19, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold06: size= 3, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold06: size= 5, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold06: size= 7, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold06: size= 9, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold06: size=11, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold06: size=13, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold06: size=15, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold06: size=17, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold06: size=19, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold06: size= 3, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold06: size= 5, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold06: size= 7, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold06: size= 9, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold06: size=11, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold06: size=13, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold06: size=15, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold06: size=17, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold06: size=19, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold07: size= 3, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold07: size= 5, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold07: size= 7, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold07: size= 9, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold07: size=11, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold07: size=13, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold07: size=15, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold07: size=17, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold07: size=19, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold07: size= 3, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold07: size= 5, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold07: size= 7, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold07: size= 9, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold07: size=11, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold07: size=13, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold07: size=15, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold07: size=17, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold07: size=19, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold07: size= 3, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold07: size= 5, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold07: size= 7, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold07: size= 9, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold07: size=11, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold07: size=13, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold07: size=15, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold07: size=17, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold07: size=19, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold07: size= 3, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold07: size= 5, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold07: size= 7, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold07: size= 9, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold07: size=11, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold07: size=13, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold07: size=15, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold07: size=17, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold07: size=19, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold07: size= 3, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold07: size= 5, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold07: size= 7, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold07: size= 9, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold07: size=11, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold07: size=13, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold07: size=15, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold07: size=17, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold07: size=19, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold07: size= 3, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold07: size= 5, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold07: size= 7, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold07: size= 9, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold07: size=11, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold07: size=13, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold07: size=15, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold07: size=17, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold07: size=19, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold07: size= 3, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold07: size= 5, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold07: size= 7, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold07: size= 9, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold07: size=11, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold07: size=13, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold07: size=15, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold07: size=17, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold07: size=19, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold07: size= 3, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold07: size= 5, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold07: size= 7, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold07: size= 9, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold07: size=11, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold07: size=13, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold07: size=15, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold07: size=17, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold07: size=19, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold07: size= 3, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold07: size= 5, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold07: size= 7, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold07: size= 9, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold07: size=11, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold07: size=13, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold07: size=15, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold07: size=17, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold07: size=19, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold07: size= 3, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold07: size= 5, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold07: size= 7, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold07: size= 9, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold07: size=11, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold07: size=13, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold07: size=15, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold07: size=17, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold07: size=19, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold08: size= 3, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold08: size= 5, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold08: size= 7, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold08: size= 9, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold08: size=11, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold08: size=13, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold08: size=15, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold08: size=17, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold08: size=19, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold08: size= 3, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold08: size= 5, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold08: size= 7, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold08: size= 9, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold08: size=11, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold08: size=13, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold08: size=15, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold08: size=17, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold08: size=19, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold08: size= 3, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold08: size= 5, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold08: size= 7, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold08: size= 9, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold08: size=11, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold08: size=13, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold08: size=15, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold08: size=17, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold08: size=19, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold08: size= 3, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold08: size= 5, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold08: size= 7, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold08: size= 9, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold08: size=11, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold08: size=13, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold08: size=15, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold08: size=17, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold08: size=19, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold08: size= 3, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold08: size= 5, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold08: size= 7, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold08: size= 9, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold08: size=11, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold08: size=13, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold08: size=15, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold08: size=17, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold08: size=19, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold08: size= 3, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold08: size= 5, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold08: size= 7, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold08: size= 9, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold08: size=11, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold08: size=13, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold08: size=15, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold08: size=17, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold08: size=19, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold08: size= 3, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold08: size= 5, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold08: size= 7, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold08: size= 9, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold08: size=11, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold08: size=13, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold08: size=15, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold08: size=17, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold08: size=19, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold08: size= 3, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold08: size= 5, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold08: size= 7, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold08: size= 9, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold08: size=11, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold08: size=13, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold08: size=15, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold08: size=17, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold08: size=19, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold08: size= 3, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold08: size= 5, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold08: size= 7, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold08: size= 9, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold08: size=11, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold08: size=13, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold08: size=15, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold08: size=17, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold08: size=19, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold08: size= 3, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold08: size= 5, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold08: size= 7, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold08: size= 9, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold08: size=11, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold08: size=13, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold08: size=15, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold08: size=17, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold08: size=19, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold09: size= 3, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold09: size= 5, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold09: size= 7, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold09: size= 9, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold09: size=11, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold09: size=13, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold09: size=15, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold09: size=17, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold09: size=19, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold09: size= 3, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold09: size= 5, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold09: size= 7, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold09: size= 9, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold09: size=11, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold09: size=13, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold09: size=15, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold09: size=17, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold09: size=19, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold09: size= 3, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold09: size= 5, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold09: size= 7, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold09: size= 9, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold09: size=11, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold09: size=13, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold09: size=15, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold09: size=17, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold09: size=19, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold09: size= 3, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold09: size= 5, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold09: size= 7, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold09: size= 9, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold09: size=11, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold09: size=13, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold09: size=15, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold09: size=17, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold09: size=19, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold09: size= 3, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold09: size= 5, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold09: size= 7, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold09: size= 9, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold09: size=11, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold09: size=13, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold09: size=15, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold09: size=17, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold09: size=19, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold09: size= 3, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold09: size= 5, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold09: size= 7, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold09: size= 9, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold09: size=11, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold09: size=13, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold09: size=15, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold09: size=17, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold09: size=19, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold09: size= 3, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold09: size= 5, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold09: size= 7, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold09: size= 9, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold09: size=11, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold09: size=13, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold09: size=15, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold09: size=17, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold09: size=19, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold09: size= 3, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold09: size= 5, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold09: size= 7, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold09: size= 9, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold09: size=11, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold09: size=13, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold09: size=15, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold09: size=17, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold09: size=19, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold09: size= 3, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold09: size= 5, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold09: size= 7, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold09: size= 9, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold09: size=11, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold09: size=13, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold09: size=15, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold09: size=17, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold09: size=19, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold09: size= 3, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold09: size= 5, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold09: size= 7, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold09: size= 9, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold09: size=11, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold09: size=13, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold09: size=15, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold09: size=17, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold09: size=19, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold10: size= 3, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold10: size= 5, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold10: size= 7, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold10: size= 9, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold10: size=11, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold10: size=13, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold10: size=15, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold10: size=17, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold10: size=19, decay=0.0000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold10: size= 3, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold10: size= 5, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold10: size= 7, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold10: size= 9, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold10: size=11, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold10: size=13, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold10: size=15, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold10: size=17, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold10: size=19, decay=0.1000000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold10: size= 3, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold10: size= 5, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold10: size= 7, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold10: size= 9, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold10: size=11, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold10: size=13, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold10: size=15, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold10: size=17, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold10: size=19, decay=0.0421697 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold10: size= 3, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold10: size= 5, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold10: size= 7, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold10: size= 9, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold10: size=11, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold10: size=13, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold10: size=15, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold10: size=17, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold10: size=19, decay=0.0177828 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold10: size= 3, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold10: size= 5, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold10: size= 7, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold10: size= 9, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold10: size=11, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold10: size=13, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold10: size=15, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold10: size=17, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold10: size=19, decay=0.0074989 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold10: size= 3, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold10: size= 5, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold10: size= 7, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold10: size= 9, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold10: size=11, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold10: size=13, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold10: size=15, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold10: size=17, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold10: size=19, decay=0.0031623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold10: size= 3, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold10: size= 5, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold10: size= 7, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold10: size= 9, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold10: size=11, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold10: size=13, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold10: size=15, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold10: size=17, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold10: size=19, decay=0.0013335 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold10: size= 3, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold10: size= 5, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold10: size= 7, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold10: size= 9, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold10: size=11, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold10: size=13, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold10: size=15, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold10: size=17, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold10: size=19, decay=0.0005623 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold10: size= 3, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold10: size= 5, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold10: size= 7, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold10: size= 9, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold10: size=11, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold10: size=13, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold10: size=15, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold10: size=17, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold10: size=19, decay=0.0002371 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning: model fit failed for Fold10: size= 3, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1171) weights\n\n\nWarning: model fit failed for Fold10: size= 5, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (1951) weights\n\n\nWarning: model fit failed for Fold10: size= 7, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (2731) weights\n\n\nWarning: model fit failed for Fold10: size= 9, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (3511) weights\n\n\nWarning: model fit failed for Fold10: size=11, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (4291) weights\n\n\nWarning: model fit failed for Fold10: size=13, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5071) weights\n\n\nWarning: model fit failed for Fold10: size=15, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (5851) weights\n\n\nWarning: model fit failed for Fold10: size=17, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (6631) weights\n\n\nWarning: model fit failed for Fold10: size=19, decay=0.0001000 Error in nnet.default(x, y, w, ...) : too many (7411) weights\n\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\n\nWarning in train.default(train_data_transformed, train_permeability, method =\n\"nnet\", : missing values found in aggregated results\n\nnn3_pred &lt;- predict(nn3_model, newdata = test_data_transformed)\nnn3_result &lt;- postResample(nn3_pred, test_permeability)\nnn3_result\n\n     RMSE  Rsquared       MAE \n12.731659  0.197259  9.597258 \n\n\nMARS\n\nset.seed(123)\nmars3_model &lt;- train(train_data_transformed, train_permeability, method = \"earth\",\n                    trControl = trainControl(method = \"cv\", number = 10),\n                    tuneLength = 10)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nmars3_pred &lt;- predict(mars3_model, newdata = test_data_transformed)\nmars3_result &lt;- postResample(mars3_pred, test_permeability)\nmars3_result\n\n      RMSE   Rsquared        MAE \n11.8764033  0.3239788  7.5933958 \n\n\nk-Nearest Neighbors (kNN)\n\nset.seed(123)\nknn3_model &lt;- train(train_data_transformed, train_permeability, method = \"kknn\",\n                   trControl = trainControl(method = \"cv\", number = 10),\n                   tuneLength = 10)\n\nknn3_pred &lt;- predict(knn3_model, newdata = test_data_transformed)\nknn3_result &lt;- postResample(knn3_pred, test_permeability)\nknn3_result\n\n      RMSE   Rsquared        MAE \n10.5121448  0.4547602  6.9931969 \n\n\na) Which nonlinear regression model that we learned in Chapter 7 gives the optimal resampling and test set performance?\n\n\n\nNONLINEAR\nRMSE\nRsquared\nMAE\n\n\n\n\nSVM\n10.5483639\n0.4394321\n7.1154118\n\n\nNeural Network\n12.731659\n0.197259\n9.597258\n\n\nMARS\n11.8764033\n0.3239788\n7.5933958\n\n\nkNN\n10.5121448\n0.4547602\n6.9931969\n\n\n\nThe kNN Model gives the optimal resampling and test set performance, because it has the lowest RMSE and the highest R-squared compared to the others.\nb) Do any of the nonlinear models outperform the optimal linear model you previously developed in Problem 2? If so, what might this tell you about the underlying relationship between the predictors and the response?\n\n\n\nNONLINEAR\nRMSE\nRsquared\nMAE\n\n\n\n\nSVM\n10.5483639\n0.4394321\n7.1154118\n\n\nNeural Network\n12.731659\n0.197259\n9.597258\n\n\nMARS\n11.8764033\n0.3239788\n7.5933958\n\n\nkNN\n10.5121448\n0.4547602\n6.9931969\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther Models Ran in Q2\nRMSE\nRsquared\nMAE\nTest_set_rsquared\n\n\n\n\nPLS Model\n12.30920\n0.4595424\n8.621998\n0.3819407\n\n\nRidge Rigression\n14.70937\n0.3749313\n10.76181\n0.5311375\n\n\nLasso Regression\n12.76949\n0.4681541\n9.532476\n0.4573928\n\n\nSVM\n11.71028\n0.5103354\n7.664866\n0.4394321\n\n\n\nYes, the best model so far is the kNN model, which seems to outperform all the other models, this model has the lowest RMSE and MAE, and its rsquared value is not the lowest. This highlights the relationship between predictors and the response variables are nonlinear.\nc) Would you recommend any of the models you have developed to replace the permeability laboratory experiment?\nBased on the results, I recommend using the kNN model. This model demonstrates the lowest RMSE and MAE, indicating it has the smallest error and provides the most accurate predictions. Additionally, it has one of the higher R-squared values, suggesting it effectively explains the variability in the data.\n\n\nAnalyzing and Predicting Oil Types and Customer Churn Using Machine Learning Techniques\n\n# Load necessary libraries\nlibrary(caret)\nlibrary(tidyverse)\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ ggplot2::alpha()       masks kernlab::alpha()\n✖ stringr::boundary()    masks strucchange::boundary()\n✖ dplyr::combine()       masks randomForest::combine(), gridExtra::combine()\n✖ purrr::cross()         masks kernlab::cross()\n✖ dplyr::filter()        masks mice::filter(), stats::filter()\n✖ dplyr::lag()           masks stats::lag()\n✖ purrr::lift()          masks caret::lift()\n✖ randomForest::margin() masks ggplot2::margin()\n✖ dplyr::select()        masks MASS::select()\n✖ dplyr::where()         masks party::where()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(e1071) \nlibrary(randomForest) \nlibrary(nnet) \nlibrary(modeldata)\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(corrplot)\nlibrary(GGally)\nlibrary(rsample)\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\n\nAttaching package: 'rsample'\n\nThe following object is masked from 'package:e1071':\n\n    permutations\n\nlibrary(recipes)\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\n\nAttaching package: 'recipes'\n\nThe following object is masked from 'package:stringr':\n\n    fixed\n\nThe following object is masked from 'package:VIM':\n\n    prepare\n\nThe following object is masked from 'package:stats4':\n\n    update\n\nThe following object is masked from 'package:stats':\n\n    step\n\nlibrary(rpart)\nlibrary(ranger)\n\nWarning: package 'ranger' was built under R version 4.3.3\n\n\n\nAttaching package: 'ranger'\n\nThe following object is masked from 'package:randomForest':\n\n    importance\n\nlibrary(nnet)\nlibrary(caret)\nlibrary(yardstick)\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n\nAttaching package: 'yardstick'\n\nThe following object is masked from 'package:readr':\n\n    spec\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall, sensitivity, specificity\n\nlibrary(yardstick)\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(xgboost)\n\nWarning: package 'xgboost' was built under R version 4.3.3\n\n\n\nAttaching package: 'xgboost'\n\nThe following object is masked from 'package:dplyr':\n\n    slice\n\n\nIn Homework 1, Problem 3, we described a data set which contained 96 oil samples each from one of seven types of oils (pumpkin, sunflower, peanut, olive, soybean, rapeseed, and corn). Gas chromatography was performed on each sample and the percentage of each type of 7 fatty acids was determined. We would like to use these data to build a model that predicts the type of oil based on a sample’s fatty acid percentages. These data can be found in the caret package using data(oil). The oil types are contained in a factor variable called oilType. The types are pumpkin (coded as A), sunflower (B), peanut (C), olive (D), soybean (E), rapeseed (F) and corn (G). In R\n\n# Load the data\n?oil\ndata(oil)\n\n\nstr(oilType)\n\n Factor w/ 7 levels \"A\",\"B\",\"C\",\"D\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\n\n\ntable(oilType)\n\noilType\n A  B  C  D  E  F  G \n37 26  3  7 11 10  2 \n\n\n\nGiven the classification imbalance in oil Type, describe how you would create a training and testing set.\n\n\n# Convert the fatty acid compositions into a data frame\noil_data &lt;- as.data.frame(fattyAcids)\noil_data$oilType &lt;- oilType\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Split the data using stratified sampling\ntrain_index &lt;- createDataPartition(oil_data$oilType, p = 0.7, list = FALSE)\ntrain_data &lt;- oil_data[train_index, ]\ntest_data &lt;- oil_data[-train_index, ]\n\n# Pre-process the data: Centering and Scaling\npreProcValues &lt;- preProcess(train_data[,-ncol(train_data)], method = c(\"center\", \"scale\"))\ntrain_data[,-ncol(train_data)] &lt;- predict(preProcValues, train_data[,-ncol(train_data)])\ntest_data[,-ncol(test_data)] &lt;- predict(preProcValues, test_data[,-ncol(test_data)])\n\n# Control for cross-validation\nctrl &lt;- trainControl(method = \"cv\", number = 10, classProbs = TRUE, summaryFunction = multiClassSummary)\n\n# Check for missing values in the dataset\ncolSums(is.na(train_data))\n\n  Palmitic    Stearic      Oleic   Linoleic  Linolenic Eicosanoic Eicosenoic \n         0          0          0          0          0          0          0 \n   oilType \n         0 \n\n\n\n# Remove rows with missing values\ntrain_data_clean &lt;- na.omit(train_data)\n\n# Impute missing values using the median\npreProcess_missing &lt;- preProcess(train_data, method = 'medianImpute')\ntrain_data_clean &lt;- predict(preProcess_missing, train_data)\n\n\nWhich classification statistic would you choose to optimize for this problem and why?\n\n\nI would choose the F1 score in cases where the classes are imbalanced. The objective is to achieve a balance between precision and recall.\n\n\nSplit the data into a training and a testing set, pre-process the data, and build models and tune them via resampling described in Chapter 12. Clearly list the models under consideration and the corresponding tuning parameters of the models.\n\nk-Nearest Neighbors (k-NN):\n\n# k-Nearest Neighbors (k-NN)\nset.seed(123)\nknn_model &lt;- train(oilType ~ ., data = train_data_clean, method = \"knn\", trControl = ctrl, tuneLength = 10)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nknn_model\n\nk-Nearest Neighbors \n\n70 samples\n 7 predictor\n 7 classes: 'A', 'B', 'C', 'D', 'E', 'F', 'G' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 62, 61, 62, 65, 63, 62, ... \nResampling results across tuning parameters:\n\n  k   logLoss    AUC        prAUC       Accuracy   Kappa      Mean_F1\n   5  0.5583384  0.9919921  0.01388889  0.9375000  0.9195499  NaN    \n   7  0.2130499  0.9974206  0.01708333  0.9138889  0.8913839  NaN    \n   9  0.2846983  0.9926091  0.01504630  0.8902778  0.8606707  NaN    \n  11  0.3528189  0.9904927  0.01430556  0.8500000  0.7986789  NaN    \n  13  0.4069273  0.9866567  0.04625000  0.8333333  0.7649739  NaN    \n  15  0.4571756  0.9891865  0.06750000  0.8208333  0.7493668  NaN    \n  17  0.5034627  0.9847983  0.07888889  0.7629365  0.6623458  NaN    \n  19  0.5718785  0.9844444  0.06222222  0.7629365  0.6633254  NaN    \n  21  0.6450366  0.9800893  0.10847222  0.7179365  0.5943018  NaN    \n  23  0.7030457  0.9753274  0.15388889  0.7054365  0.5779753  NaN    \n  Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value\n  NaN               0.9879592         NaN                  NaN                \n  NaN               0.9852891         NaN                  NaN                \n  NaN               0.9805272         NaN                  NaN                \n  NaN               0.9712585         NaN                  NaN                \n  NaN               0.9652041         NaN                  NaN                \n  NaN               0.9632993         NaN                  NaN                \n  NaN               0.9493878         NaN                  NaN                \n  NaN               0.9497279         NaN                  NaN                \n  NaN               0.9405612         NaN                  NaN                \n  NaN               0.9381803         NaN                  NaN                \n  Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy\n  NaN             NaN          0.1339286            NaN                   \n  NaN             NaN          0.1305556            NaN                   \n  NaN             NaN          0.1271825            NaN                   \n  NaN             NaN          0.1214286            NaN                   \n  NaN             NaN          0.1190476            NaN                   \n  NaN             NaN          0.1172619            NaN                   \n  NaN             NaN          0.1089909            NaN                   \n  NaN             NaN          0.1089909            NaN                   \n  NaN             NaN          0.1025624            NaN                   \n  NaN             NaN          0.1007766            NaN                   \n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was k = 5.\n\n\nk-NN\n\nk = 5\nLog Loss: 0.5583384\nAUC: 0.9919921\nprAUC: 0.01388889\nAccuracy: 0.9375000\nKappa: 0.9195499\nMean Specificity: 0.9879592\nMean_Detection_Rate: 0.1339286\n\nLogistic Regression:\n\n# Multinomial Logistic Regression\nset.seed(123)\nlog_reg_model &lt;- train(oilType ~ ., data = train_data_clean, method = \"multinom\", trControl = ctrl)\n\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 0.230492\niter  20 value 0.009305\niter  30 value 0.005935\niter  40 value 0.003696\niter  50 value 0.000954\niter  60 value 0.000826\niter  70 value 0.000260\niter  80 value 0.000245\nfinal  value 0.000060 \nconverged\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 13.646415\niter  20 value 12.851693\nfinal  value 12.849291 \nconverged\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 0.426250\niter  20 value 0.207953\niter  30 value 0.168443\niter  40 value 0.149413\niter  50 value 0.139491\niter  60 value 0.133972\niter  70 value 0.127703\niter  80 value 0.124725\niter  90 value 0.122750\niter 100 value 0.121841\nfinal  value 0.121841 \nstopped after 100 iterations\n# weights:  63 (48 variable)\ninitial  value 118.700519 \niter  10 value 0.649572\niter  20 value 0.072909\niter  30 value 0.019004\niter  40 value 0.003326\niter  50 value 0.000645\niter  60 value 0.000422\niter  70 value 0.000364\nfinal  value 0.000098 \nconverged\n# weights:  63 (48 variable)\ninitial  value 118.700519 \niter  10 value 13.670469\niter  20 value 12.717571\nfinal  value 12.717563 \nconverged\n# weights:  63 (48 variable)\ninitial  value 118.700519 \niter  10 value 0.686614\niter  20 value 0.158090\niter  30 value 0.125404\niter  40 value 0.118784\niter  50 value 0.114113\niter  60 value 0.112865\niter  70 value 0.110695\niter  80 value 0.109572\niter  90 value 0.109285\niter 100 value 0.109183\nfinal  value 0.109183 \nstopped after 100 iterations\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 1.152097\niter  20 value 0.109926\niter  30 value 0.001534\niter  40 value 0.000261\nfinal  value 0.000067 \nconverged\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 14.882114\niter  20 value 13.391351\nfinal  value 13.391006 \nconverged\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 1.193110\niter  20 value 0.224979\niter  30 value 0.168419\niter  40 value 0.160797\niter  50 value 0.156729\niter  60 value 0.153777\niter  70 value 0.150874\niter  80 value 0.149621\niter  90 value 0.148691\niter 100 value 0.148334\nfinal  value 0.148334 \nstopped after 100 iterations\n# weights:  63 (48 variable)\ninitial  value 126.484160 \niter  10 value 1.375234\niter  20 value 0.124856\niter  30 value 0.012044\niter  40 value 0.002572\nfinal  value 0.000097 \nconverged\n# weights:  63 (48 variable)\ninitial  value 126.484160 \niter  10 value 15.628076\niter  20 value 13.989692\nfinal  value 13.989554 \nconverged\n# weights:  63 (48 variable)\ninitial  value 126.484160 \niter  10 value 1.417908\niter  20 value 0.245160\niter  30 value 0.180883\niter  40 value 0.170537\niter  50 value 0.167035\niter  60 value 0.162098\niter  70 value 0.157875\niter  80 value 0.156208\niter  90 value 0.154991\niter 100 value 0.154037\nfinal  value 0.154037 \nstopped after 100 iterations\n# weights:  63 (48 variable)\ninitial  value 122.592339 \niter  10 value 1.160417\niter  20 value 0.162686\niter  30 value 0.015871\niter  40 value 0.001935\niter  50 value 0.000669\nfinal  value 0.000081 \nconverged\n# weights:  63 (48 variable)\ninitial  value 122.592339 \niter  10 value 15.295508\niter  20 value 13.937812\nfinal  value 13.936771 \nconverged\n# weights:  63 (48 variable)\ninitial  value 122.592339 \niter  10 value 1.211200\niter  20 value 0.266794\niter  30 value 0.174726\niter  40 value 0.165736\niter  50 value 0.162175\niter  60 value 0.157912\niter  70 value 0.155364\niter  80 value 0.153778\niter  90 value 0.152625\niter 100 value 0.151735\nfinal  value 0.151735 \nstopped after 100 iterations\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 0.982411\niter  20 value 0.066584\niter  30 value 0.011904\niter  40 value 0.002347\niter  50 value 0.001356\nfinal  value 0.000097 \nconverged\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 17.094756\niter  20 value 13.621311\nfinal  value 13.620050 \nconverged\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 1.025922\niter  20 value 0.188570\niter  30 value 0.160388\niter  40 value 0.154288\niter  50 value 0.149946\niter  60 value 0.145432\niter  70 value 0.143887\niter  80 value 0.141985\niter  90 value 0.140837\niter 100 value 0.140255\nfinal  value 0.140255 \nstopped after 100 iterations\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 0.201740\niter  20 value 0.036329\niter  30 value 0.015300\niter  40 value 0.006737\niter  50 value 0.001509\niter  60 value 0.001034\niter  70 value 0.000403\niter  80 value 0.000390\nfinal  value 0.000095 \nconverged\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 15.237850\niter  20 value 13.504443\nfinal  value 13.503651 \nconverged\n# weights:  63 (48 variable)\ninitial  value 120.646429 \niter  10 value 0.345888\niter  20 value 0.205508\niter  30 value 0.195911\niter  40 value 0.175069\niter  50 value 0.161029\niter  60 value 0.154607\niter  70 value 0.150725\niter  80 value 0.149049\niter  90 value 0.147205\niter 100 value 0.146646\nfinal  value 0.146646 \nstopped after 100 iterations\n# weights:  63 (48 variable)\ninitial  value 124.538250 \niter  10 value 1.423335\niter  20 value 0.097869\niter  30 value 0.013049\niter  40 value 0.000310\nfinal  value 0.000099 \nconverged\n# weights:  63 (48 variable)\ninitial  value 124.538250 \niter  10 value 15.998770\niter  20 value 13.988523\nfinal  value 13.987954 \nconverged\n# weights:  63 (48 variable)\ninitial  value 124.538250 \niter  10 value 1.461579\niter  20 value 0.221927\niter  30 value 0.182446\niter  40 value 0.172022\niter  50 value 0.166376\niter  60 value 0.158815\niter  70 value 0.155713\niter  80 value 0.154033\niter  90 value 0.153355\niter 100 value 0.152889\nfinal  value 0.152889 \nstopped after 100 iterations\n# weights:  63 (48 variable)\ninitial  value 126.484160 \niter  10 value 1.036502\niter  20 value 0.114252\niter  30 value 0.014528\niter  40 value 0.002648\niter  50 value 0.000355\nfinal  value 0.000100 \nconverged\n# weights:  63 (48 variable)\ninitial  value 126.484160 \niter  10 value 14.854834\niter  20 value 13.259516\nfinal  value 13.259460 \nconverged\n# weights:  63 (48 variable)\ninitial  value 126.484160 \niter  10 value 1.076774\niter  20 value 0.208832\niter  30 value 0.157131\niter  40 value 0.147462\niter  50 value 0.144709\niter  60 value 0.143059\niter  70 value 0.140412\niter  80 value 0.139310\niter  90 value 0.138768\niter 100 value 0.138344\nfinal  value 0.138344 \nstopped after 100 iterations\n# weights:  63 (48 variable)\ninitial  value 124.538250 \niter  10 value 1.127422\niter  20 value 0.120905\niter  30 value 0.019095\niter  40 value 0.000191\nfinal  value 0.000076 \nconverged\n# weights:  63 (48 variable)\ninitial  value 124.538250 \niter  10 value 16.170644\niter  20 value 14.131864\nfinal  value 14.130439 \nconverged\n# weights:  63 (48 variable)\ninitial  value 124.538250 \niter  10 value 1.178679\niter  20 value 0.235910\niter  30 value 0.180086\niter  40 value 0.170725\niter  50 value 0.165276\niter  60 value 0.160358\niter  70 value 0.157981\niter  80 value 0.156756\niter  90 value 0.155457\niter 100 value 0.154352\nfinal  value 0.154352 \nstopped after 100 iterations\n\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\n\n# weights:  63 (48 variable)\ninitial  value 136.213710 \niter  10 value 18.641421\niter  20 value 14.337670\nfinal  value 14.337620 \nconverged\n\nlog_reg_model\n\nPenalized Multinomial Regression \n\n70 samples\n 7 predictor\n 7 classes: 'A', 'B', 'C', 'D', 'E', 'F', 'G' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 62, 61, 62, 65, 63, 62, ... \nResampling results across tuning parameters:\n\n  decay  logLoss    AUC        prAUC      Accuracy   Kappa      Mean_F1\n  0e+00  0.5777232  0.9892857  0.2177480  0.9313889  0.9115174  NaN    \n  1e-04  0.4544908  0.9930556  0.2457738  0.9513889  0.9387330  NaN    \n  1e-01  0.1882385  0.9972222  0.2473611  0.9513889  0.9387330  NaN    \n  Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value\n  NaN               0.9882993         NaN                  NaN                \n  NaN               0.9920918         NaN                  NaN                \n  NaN               0.9920918         NaN                  NaN                \n  Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy\n  NaN             NaN          0.1330556            NaN                   \n  NaN             NaN          0.1359127            NaN                   \n  NaN             NaN          0.1359127            NaN                   \n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was decay = 0.1.\n\n\nLogistic Regression\n\ndecay: 0.1\nlogLoss: 0.1882385\nAUC: 0.9972222\nprAUC: 0.2473611\nAccuracy: 0.9513889\nKappa: 0.9387330\nMean_Specificity: 0.9920918\nMean_Detection_Rate: 0.1359127\n\nRandom Forest:\n\n# Random Forest\nset.seed(123)\nrf_grid &lt;- expand.grid(mtry = seq(1, ncol(train_data) - 1, length.out = 6))\nrf_model &lt;- train(oilType ~ ., data = train_data, method = \"rf\", trControl = ctrl, tuneGrid = rf_grid)\n\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n\nrf_model\n\nRandom Forest \n\n70 samples\n 7 predictor\n 7 classes: 'A', 'B', 'C', 'D', 'E', 'F', 'G' \n\nNo pre-processing\nResampling: Cross-Validated (10 fold) \nSummary of sample sizes: 62, 61, 62, 65, 63, 62, ... \nResampling results across tuning parameters:\n\n  mtry  logLoss    AUC  prAUC      Accuracy  Kappa      Mean_F1\n  1.0   0.2490844  1    0.2522222  0.9625    0.9523009  NaN    \n  2.2   0.1537911  1    0.2188889  0.9750    0.9679487  NaN    \n  3.4   0.1361475  1    0.2147222  0.9750    0.9679487  NaN    \n  4.6   0.1333570  1    0.1791667  0.9550    0.9405111  NaN    \n  5.8   0.1378782  1    0.1583333  0.9550    0.9405111  NaN    \n  7.0   0.1509535  1    0.1455556  0.9550    0.9405111  NaN    \n  Mean_Sensitivity  Mean_Specificity  Mean_Pos_Pred_Value  Mean_Neg_Pred_Value\n  NaN               0.9934524         NaN                  NaN                \n  NaN               0.9959184         NaN                  NaN                \n  NaN               0.9959184         NaN                  NaN                \n  NaN               0.9933163         NaN                  NaN                \n  NaN               0.9933163         NaN                  NaN                \n  NaN               0.9933163         NaN                  NaN                \n  Mean_Precision  Mean_Recall  Mean_Detection_Rate  Mean_Balanced_Accuracy\n  NaN             NaN          0.1375000            NaN                   \n  NaN             NaN          0.1392857            NaN                   \n  NaN             NaN          0.1392857            NaN                   \n  NaN             NaN          0.1364286            NaN                   \n  NaN             NaN          0.1364286            NaN                   \n  NaN             NaN          0.1364286            NaN                   \n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 2.2.\n\n\nRandom Forest:\n\nmtry = 2.2\nLogLoss: 0.1537911\nAUC: 1\nprAUC: 0.2188889\nAccuracy: 0.9750\nKappa: 0.9679487\nMean_Specificity: 0.9959184\nMean_Detection_Rate: 0.1392857\n\nSummary:\n\n\n\n\n\n\n\n\n\n\n\nModel\nTuning Parameter\nAccuracy\nKappa\nAUC\nprAUC\n\n\n\n\nk-NN\nk = 5\n0.9375\n0.9195499\n0.9920\n0.0139\n\n\nLogistic Regression\ndecay = 0.1\n0.9514\n0.9387330\n0.9972\n0.2474\n\n\nRandom Forest\nmtry = 2.2\n0.9750\n0.9679487\n1.0000\n0.2189\n\n\n\nConclusion: Out of all the models the Random Forest performs the best with the highest accuracy, Kappa, and AUC score. Then followed by the Logistic Regression model, and the model that performed the worst is the k-NN model. Altogether these models performed well, but the Random Forest out performs the rest of the models.\n\nOf the models presented in this chapter, which performs best on these data? Which oil type does the model most accurately predict? Least accurately predict?\n\n\n# Generate predictions for each model\nknn_pred &lt;- predict(knn_model, test_data)\nlog_reg_pred &lt;- predict(log_reg_model, test_data)\nrf_pred &lt;- predict(rf_model, test_data)\n\n# Create confusion matrices\nknn_cm &lt;- confusionMatrix(knn_pred, test_data$oilType)\nlog_reg_cm &lt;- confusionMatrix(log_reg_pred, test_data$oilType)\nrf_cm &lt;- confusionMatrix(rf_pred, test_data$oilType)\n\n\n# Print confusion matrices\nprint(knn_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  A  B  C  D  E  F  G\n         A 10  0  0  0  0  0  0\n         B  1  7  0  0  0  0  0\n         C  0  0  0  0  0  0  0\n         D  0  0  0  2  0  0  0\n         E  0  0  0  0  3  0  0\n         F  0  0  0  0  0  3  0\n         G  0  0  0  0  0  0  0\n\nOverall Statistics\n                                         \n               Accuracy : 0.9615         \n                 95% CI : (0.8036, 0.999)\n    No Information Rate : 0.4231         \n    P-Value [Acc &gt; NIR] : 7.058e-09      \n                                         \n                  Kappa : 0.9467         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: A Class: B Class: C Class: D Class: E Class: F\nSensitivity            0.9091   1.0000       NA  1.00000   1.0000   1.0000\nSpecificity            1.0000   0.9474        1  1.00000   1.0000   1.0000\nPos Pred Value         1.0000   0.8750       NA  1.00000   1.0000   1.0000\nNeg Pred Value         0.9375   1.0000       NA  1.00000   1.0000   1.0000\nPrevalence             0.4231   0.2692        0  0.07692   0.1154   0.1154\nDetection Rate         0.3846   0.2692        0  0.07692   0.1154   0.1154\nDetection Prevalence   0.3846   0.3077        0  0.07692   0.1154   0.1154\nBalanced Accuracy      0.9545   0.9737       NA  1.00000   1.0000   1.0000\n                     Class: G\nSensitivity                NA\nSpecificity                 1\nPos Pred Value             NA\nNeg Pred Value             NA\nPrevalence                  0\nDetection Rate              0\nDetection Prevalence        0\nBalanced Accuracy          NA\n\n\nkNN:\n\nAccuracy: 0.9615\nKappa: 0.9467\nClass A, B, D, E, and F are good.\nClass C, and G have no predictions\n\n\nprint(log_reg_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  A  B  C  D  E  F  G\n         A 10  0  0  0  0  0  0\n         B  1  7  0  0  0  0  0\n         C  0  0  0  0  0  0  0\n         D  0  0  0  2  0  0  0\n         E  0  0  0  0  3  0  0\n         F  0  0  0  0  0  3  0\n         G  0  0  0  0  0  0  0\n\nOverall Statistics\n                                         \n               Accuracy : 0.9615         \n                 95% CI : (0.8036, 0.999)\n    No Information Rate : 0.4231         \n    P-Value [Acc &gt; NIR] : 7.058e-09      \n                                         \n                  Kappa : 0.9467         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: A Class: B Class: C Class: D Class: E Class: F\nSensitivity            0.9091   1.0000       NA  1.00000   1.0000   1.0000\nSpecificity            1.0000   0.9474        1  1.00000   1.0000   1.0000\nPos Pred Value         1.0000   0.8750       NA  1.00000   1.0000   1.0000\nNeg Pred Value         0.9375   1.0000       NA  1.00000   1.0000   1.0000\nPrevalence             0.4231   0.2692        0  0.07692   0.1154   0.1154\nDetection Rate         0.3846   0.2692        0  0.07692   0.1154   0.1154\nDetection Prevalence   0.3846   0.3077        0  0.07692   0.1154   0.1154\nBalanced Accuracy      0.9545   0.9737       NA  1.00000   1.0000   1.0000\n                     Class: G\nSensitivity                NA\nSpecificity                 1\nPos Pred Value             NA\nNeg Pred Value             NA\nPrevalence                  0\nDetection Rate              0\nDetection Prevalence        0\nBalanced Accuracy          NA\n\n\nLogistic Regression:\n\nAccuracy: 0.9615\nKappa: 0.9467\nClass A, B, D, E, and F are predicted well\nClass C and G have the worst outcome.\n\n\nprint(rf_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  A  B  C  D  E  F  G\n         A 11  0  0  0  0  0  0\n         B  0  7  0  0  0  0  0\n         C  0  0  0  0  0  0  0\n         D  0  0  0  2  0  0  0\n         E  0  0  0  0  3  0  0\n         F  0  0  0  0  0  3  0\n         G  0  0  0  0  0  0  0\n\nOverall Statistics\n                                     \n               Accuracy : 1          \n                 95% CI : (0.8677, 1)\n    No Information Rate : 0.4231     \n    P-Value [Acc &gt; NIR] : 1.936e-10  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: A Class: B Class: C Class: D Class: E Class: F\nSensitivity            1.0000   1.0000       NA  1.00000   1.0000   1.0000\nSpecificity            1.0000   1.0000        1  1.00000   1.0000   1.0000\nPos Pred Value         1.0000   1.0000       NA  1.00000   1.0000   1.0000\nNeg Pred Value         1.0000   1.0000       NA  1.00000   1.0000   1.0000\nPrevalence             0.4231   0.2692        0  0.07692   0.1154   0.1154\nDetection Rate         0.4231   0.2692        0  0.07692   0.1154   0.1154\nDetection Prevalence   0.4231   0.2692        0  0.07692   0.1154   0.1154\nBalanced Accuracy      1.0000   1.0000       NA  1.00000   1.0000   1.0000\n                     Class: G\nSensitivity                NA\nSpecificity                 1\nPos Pred Value             NA\nNeg Pred Value             NA\nPrevalence                  0\nDetection Rate              0\nDetection Prevalence        0\nBalanced Accuracy          NA\n\n\nRandom Forest:\n\nAccuracy: 1\nKappa: 1\nClass A, B, D, E, and F are predicted well\nClass C and G have no predictions.\n\n\n# Compute F1 Scores\nf1_score &lt;- function(conf_matrix) {\n  precision &lt;- conf_matrix$byClass[, \"Precision\"]\n  recall &lt;- conf_matrix$byClass[, \"Recall\"]\n  f1 &lt;- 2 * (precision * recall) / (precision + recall)\n  f1[is.na(f1)] &lt;- 0 # Handle cases where precision or recall is zero\n  return(f1)}\n\n# Calculate F1 Scores for each model\nknn_f1 &lt;- f1_score(knn_cm)\nlog_reg_f1 &lt;- f1_score(log_reg_cm)\nrf_f1 &lt;- f1_score(rf_cm)\n\n# Summary Table\nsummary_table &lt;- data.frame(\n  Model = c(\"k-NN\", \"Logistic Regression\", \"Random Forest\"),\n  Accuracy = c(knn_cm$overall['Accuracy'], log_reg_cm$overall['Accuracy'], rf_cm$overall['Accuracy']),\n  Kappa = c(knn_cm$overall['Kappa'], log_reg_cm$overall['Kappa'], rf_cm$overall['Kappa']),\n  F1_Score = c(mean(knn_f1), mean(log_reg_f1), mean(rf_f1)))\n\nprint(summary_table)\n\n                Model  Accuracy     Kappa  F1_Score\n1                k-NN 0.9615385 0.9467213 0.6979592\n2 Logistic Regression 0.9615385 0.9467213 0.6979592\n3       Random Forest 1.0000000 1.0000000 0.7142857\n\n\nBest Predictive Model: Random Forest:\n\nAccuracy: 1\nKappa: 1\nF1 score: 0.7142857\n\nSummary: Although the SVM and k-NN are identical in the confusion matrices, we would need further testing for these models. However, as previously stated the Random Forest proves to be the best performing almost perfectly. In terms of class, Class A (pumpkin) predicted the most accurate, wile the least accurate classes are Class C (peanut) and G (corn).\nUse the fatty acid data from Problem 1 above.\n\nUse the same data splitting approach (if any) and pre-processing steps that you did Problem 1. Using the same classification statistic as before, build models described in Chapter 13: Nonlinear Classification Models for these data. Which model has the best predictive ability? How does this optimal model’s performance compare to the best linear model’s performance?\n\n\n# Set up cross-validation control\nctrl &lt;- trainControl(method = \"cv\", number = 10)\n\n# Define a grid of hyperparameters\ntune_grid &lt;- expand.grid(sigma = c(0.01, 0.05, 0.1, 0.2), \n                         C = c(0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128))\n\nSupport Vector Machines (SVM):\n\n# Train SVM model\nsvm_model &lt;- train(oilType ~ ., data = train_data, method = \"svmRadial\",\n                   trControl = ctrl, tuneGrid = tune_grid)\n\n# Generate predictions for SVM model\nsvm_pred &lt;- predict(svm_model, test_data)\n\n# Create confusion matrix and calculate F1 Score\nsvm_cm &lt;- confusionMatrix(svm_pred, test_data$oilType)\n\nsvm_f1 &lt;- f_meas(svm_cm$table, truth = \"reference\", estimate = \"prediction\", event_level = \"second\")\n\nWarning: While computing multiclass `precision()`, some levels had no predicted events\n(i.e. `true_positive + false_positive = 0`).\nPrecision is undefined in this case, and those levels will be removed from the\naveraged result.\nNote that the following number of true events actually occurred for each\nproblematic event level:\n'C': 0, 'G': 0\n\n\nWarning: While computing multiclass `recall()`, some levels had no true events (i.e.\n`true_positive + false_negative = 0`).\nRecall is undefined in this case, and those levels will be removed from the\naveraged result.\nNote that the following number of predicted events actually occurred for each\nproblematic event level:\n'C': 0, 'G': 0\n\n# Print results\nprint(svm_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  A  B  C  D  E  F  G\n         A 10  0  0  0  0  0  0\n         B  1  7  0  0  0  0  0\n         C  0  0  0  0  0  0  0\n         D  0  0  0  2  0  0  0\n         E  0  0  0  0  3  0  0\n         F  0  0  0  0  0  3  0\n         G  0  0  0  0  0  0  0\n\nOverall Statistics\n                                         \n               Accuracy : 0.9615         \n                 95% CI : (0.8036, 0.999)\n    No Information Rate : 0.4231         \n    P-Value [Acc &gt; NIR] : 7.058e-09      \n                                         \n                  Kappa : 0.9467         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: A Class: B Class: C Class: D Class: E Class: F\nSensitivity            0.9091   1.0000       NA  1.00000   1.0000   1.0000\nSpecificity            1.0000   0.9474        1  1.00000   1.0000   1.0000\nPos Pred Value         1.0000   0.8750       NA  1.00000   1.0000   1.0000\nNeg Pred Value         0.9375   1.0000       NA  1.00000   1.0000   1.0000\nPrevalence             0.4231   0.2692        0  0.07692   0.1154   0.1154\nDetection Rate         0.3846   0.2692        0  0.07692   0.1154   0.1154\nDetection Prevalence   0.3846   0.3077        0  0.07692   0.1154   0.1154\nBalanced Accuracy      0.9545   0.9737       NA  1.00000   1.0000   1.0000\n                     Class: G\nSensitivity                NA\nSpecificity                 1\nPos Pred Value             NA\nNeg Pred Value             NA\nPrevalence                  0\nDetection Rate              0\nDetection Prevalence        0\nBalanced Accuracy          NA\n\n\n\nprint(svm_f1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  macro          0.977\n\n\nsvm:\n\nKappa: 0.9467\nAccuracy: 0.9615\nClasses E and F performed the best, followed by D then A and B.\nClasses D and G continue to perform bad.\nF1 score: 0.9771429\n\nGBM Model:\n\n# Train GBM model\ngbm_model &lt;- train(oilType ~ ., data = train_data, method = \"gbm\",\n                   trControl = ctrl, verbose = FALSE)\n\n# Generate predictions for GBM model\ngbm_pred &lt;- predict(gbm_model, test_data)\n\n# Create confusion matrix and calculate F1 Score\ngbm_cm &lt;- confusionMatrix(gbm_pred, test_data$oilType)\ngbm_f1 &lt;- f_meas(gbm_cm$table, truth = \"reference\", estimate = \"prediction\", event_level = \"second\")\n\nWarning: While computing multiclass `precision()`, some levels had no predicted events\n(i.e. `true_positive + false_positive = 0`).\nPrecision is undefined in this case, and those levels will be removed from the\naveraged result.\nNote that the following number of true events actually occurred for each\nproblematic event level:\n'C': 0, 'G': 0\n\n\nWarning: While computing multiclass `recall()`, some levels had no true events (i.e.\n`true_positive + false_negative = 0`).\nRecall is undefined in this case, and those levels will be removed from the\naveraged result.\nNote that the following number of predicted events actually occurred for each\nproblematic event level:\n'C': 0, 'G': 0\n\n# Print results\nprint(gbm_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  A  B  C  D  E  F  G\n         A 11  0  0  0  0  0  0\n         B  0  7  0  0  0  0  0\n         C  0  0  0  0  0  0  0\n         D  0  0  0  2  0  0  0\n         E  0  0  0  0  3  0  0\n         F  0  0  0  0  0  3  0\n         G  0  0  0  0  0  0  0\n\nOverall Statistics\n                                     \n               Accuracy : 1          \n                 95% CI : (0.8677, 1)\n    No Information Rate : 0.4231     \n    P-Value [Acc &gt; NIR] : 1.936e-10  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: A Class: B Class: C Class: D Class: E Class: F\nSensitivity            1.0000   1.0000       NA  1.00000   1.0000   1.0000\nSpecificity            1.0000   1.0000        1  1.00000   1.0000   1.0000\nPos Pred Value         1.0000   1.0000       NA  1.00000   1.0000   1.0000\nNeg Pred Value         1.0000   1.0000       NA  1.00000   1.0000   1.0000\nPrevalence             0.4231   0.2692        0  0.07692   0.1154   0.1154\nDetection Rate         0.4231   0.2692        0  0.07692   0.1154   0.1154\nDetection Prevalence   0.4231   0.2692        0  0.07692   0.1154   0.1154\nBalanced Accuracy      1.0000   1.0000       NA  1.00000   1.0000   1.0000\n                     Class: G\nSensitivity                NA\nSpecificity                 1\nPos Pred Value             NA\nNeg Pred Value             NA\nPrevalence                  0\nDetection Rate              0\nDetection Prevalence        0\nBalanced Accuracy          NA\n\n\n\nprint(gbm_f1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  macro              1\n\n\nGBM:\n\nKappa: 1\nAccuracy: 1\nClasses A, B, D, E and F in this order, predicted well.\nClasses D and G continue to perform bad\nF1 score: 1\n\nNeural Network (NN):\n\n# Train Neural Network model\nnn_model &lt;- train(oilType ~ ., data = train_data, method = \"nnet\",\n                  trControl = ctrl, tuneLength = 3, linout = TRUE, trace = FALSE)\n\n# Generate predictions for Neural Network model\nnn_pred &lt;- predict(nn_model, test_data)\n\n# Create confusion matrix and calculate F1 Score\nnn_cm &lt;- confusionMatrix(nn_pred, test_data$oilType)\nnn_f1 &lt;- f_meas(nn_cm$table, truth = \"reference\", estimate = \"prediction\", event_level = \"second\")\n\nWarning: While computing multiclass `precision()`, some levels had no predicted events\n(i.e. `true_positive + false_positive = 0`).\nPrecision is undefined in this case, and those levels will be removed from the\naveraged result.\nNote that the following number of true events actually occurred for each\nproblematic event level:\n'C': 0, 'G': 0\n\n\nWarning: While computing multiclass `recall()`, some levels had no true events (i.e.\n`true_positive + false_negative = 0`).\nRecall is undefined in this case, and those levels will be removed from the\naveraged result.\nNote that the following number of predicted events actually occurred for each\nproblematic event level:\n'C': 0, 'G': 0\n\n# Print results\nprint(nn_cm)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  A  B  C  D  E  F  G\n         A 11  0  0  0  0  0  0\n         B  0  7  0  0  0  0  0\n         C  0  0  0  0  0  0  0\n         D  0  0  0  2  0  0  0\n         E  0  0  0  0  3  1  0\n         F  0  0  0  0  0  2  0\n         G  0  0  0  0  0  0  0\n\nOverall Statistics\n                                         \n               Accuracy : 0.9615         \n                 95% CI : (0.8036, 0.999)\n    No Information Rate : 0.4231         \n    P-Value [Acc &gt; NIR] : 7.058e-09      \n                                         \n                  Kappa : 0.9463         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: A Class: B Class: C Class: D Class: E Class: F\nSensitivity            1.0000   1.0000       NA  1.00000   1.0000  0.66667\nSpecificity            1.0000   1.0000        1  1.00000   0.9565  1.00000\nPos Pred Value         1.0000   1.0000       NA  1.00000   0.7500  1.00000\nNeg Pred Value         1.0000   1.0000       NA  1.00000   1.0000  0.95833\nPrevalence             0.4231   0.2692        0  0.07692   0.1154  0.11538\nDetection Rate         0.4231   0.2692        0  0.07692   0.1154  0.07692\nDetection Prevalence   0.4231   0.2692        0  0.07692   0.1538  0.07692\nBalanced Accuracy      1.0000   1.0000       NA  1.00000   0.9783  0.83333\n                     Class: G\nSensitivity                NA\nSpecificity                 1\nPos Pred Value             NA\nNeg Pred Value             NA\nPrevalence                  0\nDetection Rate              0\nDetection Prevalence        0\nBalanced Accuracy          NA\n\n\n\nprint(nn_f1)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 f_meas  macro          0.931\n\n\nNeural Network:\n\nAccuracy : 0.9615\nKappa : 0.9453\nF1 score: 0.9246377\n\n\n\n\nMatrices Models\nAccuracy\nKappa\nF1 Score\n\n\n\n\nSVM\n0.9615\n0.9615\n0.9771429\n\n\nGBM\n1\n1\n1\n\n\nNeural Network\n0.9615\n0.9453\n0.9314286\n\n\n\nConclusion: The GBM performed the best here performing exceptionally well with perfect scores in accuracy, Kappa and F1 score. This model accurately predict all classes except for peanut and corn which had no predictions. Comparing the GBM and the linear model from question 1 (Random Forest), these two models accurately class A, B, D, E, F and also failed to predict Class C and G. However, in terms of metrics the GBM model offers a higher F1 score indicating it can better distinguish true positive and avoids false negative, making this model more reliable.\n\nWould you infer that the data have nonlinear separation boundaries based on this comparison?\n\n\nGiven that the GBM, and Random Forest models, are both performing exceptionally well, indicates these models are best suited to handle non-linear data sets. Thus, confirming that the data does have nonlinear separation boundaries\n\n\nWhich oil type does the optimal model most accurately predict? Least accurately predict?\n\n\nThe GBM model accurately predicted the Class (A, B, D, E, F) the best while not accurately predicting Class (C) the peanut and Class (G) corn oil type.\n\nThe “churn” data set was developed to predict telecom customer churn based on information about their account. The data files state that the data are “artificial based on claims similar to real world.” The data consist of 19 predictors related to the customer account, such as the number of customer service calls, the area code, and the number of minutes. The outcome is whether the customer churned:\n\nStart R and use these commands to load the data\n\n\ndata(mlc_churn) \nstr(mlc_churn)\n\ntibble [5,000 × 20] (S3: tbl_df/tbl/data.frame)\n $ state                        : Factor w/ 51 levels \"AK\",\"AL\",\"AR\",..: 17 36 32 36 37 2 20 25 19 50 ...\n $ account_length               : int [1:5000] 128 107 137 84 75 118 121 147 117 141 ...\n $ area_code                    : Factor w/ 3 levels \"area_code_408\",..: 2 2 2 1 2 3 3 2 1 2 ...\n $ international_plan           : Factor w/ 2 levels \"no\",\"yes\": 1 1 1 2 2 2 1 2 1 2 ...\n $ voice_mail_plan              : Factor w/ 2 levels \"no\",\"yes\": 2 2 1 1 1 1 2 1 1 2 ...\n $ number_vmail_messages        : int [1:5000] 25 26 0 0 0 0 24 0 0 37 ...\n $ total_day_minutes            : num [1:5000] 265 162 243 299 167 ...\n $ total_day_calls              : int [1:5000] 110 123 114 71 113 98 88 79 97 84 ...\n $ total_day_charge             : num [1:5000] 45.1 27.5 41.4 50.9 28.3 ...\n $ total_eve_minutes            : num [1:5000] 197.4 195.5 121.2 61.9 148.3 ...\n $ total_eve_calls              : int [1:5000] 99 103 110 88 122 101 108 94 80 111 ...\n $ total_eve_charge             : num [1:5000] 16.78 16.62 10.3 5.26 12.61 ...\n $ total_night_minutes          : num [1:5000] 245 254 163 197 187 ...\n $ total_night_calls            : int [1:5000] 91 103 104 89 121 118 118 96 90 97 ...\n $ total_night_charge           : num [1:5000] 11.01 11.45 7.32 8.86 8.41 ...\n $ total_intl_minutes           : num [1:5000] 10 13.7 12.2 6.6 10.1 6.3 7.5 7.1 8.7 11.2 ...\n $ total_intl_calls             : int [1:5000] 3 3 5 7 3 6 7 6 4 5 ...\n $ total_intl_charge            : num [1:5000] 2.7 3.7 3.29 1.78 2.73 1.7 2.03 1.92 2.35 3.02 ...\n $ number_customer_service_calls: int [1:5000] 1 1 0 2 3 0 3 0 1 0 ...\n $ churn                        : Factor w/ 2 levels \"yes\",\"no\": 2 2 2 2 2 2 2 2 2 2 ...\n\n?mlc_churn\n\n\ncolnames(mlc_churn)\n\n [1] \"state\"                         \"account_length\"               \n [3] \"area_code\"                     \"international_plan\"           \n [5] \"voice_mail_plan\"               \"number_vmail_messages\"        \n [7] \"total_day_minutes\"             \"total_day_calls\"              \n [9] \"total_day_charge\"              \"total_eve_minutes\"            \n[11] \"total_eve_calls\"               \"total_eve_charge\"             \n[13] \"total_night_minutes\"           \"total_night_calls\"            \n[15] \"total_night_charge\"            \"total_intl_minutes\"           \n[17] \"total_intl_calls\"              \"total_intl_charge\"            \n[19] \"number_customer_service_calls\" \"churn\"                        \n\n\n\nExplore the data by visualizing the relationship between the predictors and the outcome. Are there important features of the predictor data themselves, such as between-predictor correlations or degenerate distributions? Can functions of more than one predictor be used to model the data more effectively?\n\n\n# Density plot of account length\nggplot(mlc_churn, aes(x = account_length, fill = churn)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Density: Account Length by Churn Status\", x = \"Account Length\", y = \"Density\")\n\n\n\n\n\nDensity Plot: Although there is some overlap between churn and nonchurn customers, their are some difference. It looks like the churned customer is skewed left, while the nonchurn customer are skewed right.\n\n\n# Boxplot of total day minutes by churn\nggplot(mlc_churn, aes(x = churn, y = total_day_minutes, fill = churn)) +\n  geom_boxplot() +\n  labs(title = \"Total Day Minutes by Churn Status\", x = \"Churn\", y = \"Total Day Minutes\")\n\n\n\n\nBoxplot: Churned customers have slightly higher total day minutes compared to nonchurned customers. Nonchurn customers show to have outliers, and at first glance both churn and nonchurn look to be normally distributed, but after carefully evaluating the nonchurn, we can determine its normal distributed, while churned is negatively distributed, only confirming the Density graph.\n\nggplot(mlc_churn, aes(x = total_day_minutes, y = total_night_minutes, color = churn)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Scatter Plot of Total Day vs. Total Night Minutes\", x = \"Total Day Minutes\", y = \"Total Night Minutes\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nScatter Plot: There is a slight positive correlation and a linear relationship between day minutes and night minutes.\n\n\n# Calculate the correlation matrix\nnumeric_vars &lt;- mlc_churn %&gt;% select_if(is.numeric)\ncor_matrix &lt;- cor(numeric_vars, use = \"pairwise.complete.obs\")\n\n# Convert correlation matrix to long format\ncor_long &lt;- cor_matrix %&gt;%\n  as.data.frame() %&gt;%\n  rownames_to_column(var = \"Var1\") %&gt;%\n  pivot_longer(-Var1, names_to = \"Var2\", values_to = \"value\")\n\n# Plot the heatmap\nggplot(cor_long, aes(Var1, Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1, 1)) +\n  labs(title = \"Correlation Heatmap of Numeric Variables\", x = \"\", y = \"\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\nCorrelation Heat Map:\n\nPositive correlation: day minutes and day charge\nPositive correlation: eve minutes and eve charge\nPositive correlation: night minutes and night charge\nPositive correlation: intl minutes and intl charge\n\nConclusion: The visualizations suggest, that creating interaction based on relationships of predictors could create more effective models. For instance, aggregating totals across different times, or between account_length and total_day_minutes, would capture more patterns. Ultimately improving the predictive performance of the model.\n\nSplit the data into a training and a testing set, pre-process the data if appropriate.\n\n\n# Split the data\nset.seed(123)\nsplit &lt;- initial_split(mlc_churn, prop = 0.7)\ntrain_data &lt;- training(split)\ntest_data &lt;- testing(split)\n\n# Preprocess the data\nrecipe &lt;- recipe(churn ~ ., data = train_data) %&gt;%\n  step_normalize(all_numeric()) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  prep(training = train_data, retain = TRUE)\n\ntrain_prepped &lt;- bake(recipe, new_data = train_data)\ntest_prepped &lt;- bake(recipe, new_data = test_data)\n\n\nTry building other models discussed in this chapter. Do any have better predictive performance?\n\n\n# Check column names in the preprocessed data\n#colnames(train_prepped)\n\n\n#colnames(test_prepped)\n\n\n# Decision Tree Model\ntree_model &lt;- rpart(churn ~ ., data = train_prepped, method = \"class\")\n\n# Predict on test data\ntree_preds &lt;- predict(tree_model, newdata = test_prepped, type = \"class\")\n\n# Evaluate performance\ntest_churn &lt;- factor(test_data$churn, levels = levels(tree_preds))\ntree_metrics &lt;- confusionMatrix(tree_preds, test_churn)\nprint(tree_metrics)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  yes   no\n       yes  142   22\n       no    64 1272\n                                          \n               Accuracy : 0.9427          \n                 95% CI : (0.9297, 0.9539)\n    No Information Rate : 0.8627          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.7353          \n                                          \n Mcnemar's Test P-Value : 9.818e-06       \n                                          \n            Sensitivity : 0.68932         \n            Specificity : 0.98300         \n         Pos Pred Value : 0.86585         \n         Neg Pred Value : 0.95210         \n             Prevalence : 0.13733         \n         Detection Rate : 0.09467         \n   Detection Prevalence : 0.10933         \n      Balanced Accuracy : 0.83616         \n                                          \n       'Positive' Class : yes             \n                                          \n\n\nDecision Tree Model:\n\nAccuracy: 0.9427\nKappa : 0.7353\nMcnemar’s Test P-Value : 9.818e-06\nBalanced Accuracy : 0.83616\n\n\n# Random Forest Model\nrf_model &lt;- ranger(churn ~ ., data = train_prepped, classification = TRUE)\n\n# Predict on test data\nrf_preds &lt;- predict(rf_model, data = test_prepped)$predictions\n\n# Evaluate performance\nrf_metrics &lt;- confusionMatrix(factor(rf_preds, levels = levels(test_data$churn)), test_churn)\nprint(rf_metrics)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  yes   no\n       yes  153    4\n       no    53 1290\n                                         \n               Accuracy : 0.962          \n                 95% CI : (0.951, 0.9711)\n    No Information Rate : 0.8627         \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16      \n                                         \n                  Kappa : 0.8218         \n                                         \n Mcnemar's Test P-Value : 2.047e-10      \n                                         \n            Sensitivity : 0.7427         \n            Specificity : 0.9969         \n         Pos Pred Value : 0.9745         \n         Neg Pred Value : 0.9605         \n             Prevalence : 0.1373         \n         Detection Rate : 0.1020         \n   Detection Prevalence : 0.1047         \n      Balanced Accuracy : 0.8698         \n                                         \n       'Positive' Class : yes            \n                                         \n\n\nRandom Forest Model:\n\nAccuracy : 0.962\nKappa : 0.8128\nMcnemar’s Test P-Value : &lt; 2.2e-16\nBalanced Accuracy : 0.8698\n\n\n# SVM Model\nsvm_model &lt;- svm(churn ~ ., data = train_prepped, kernel = \"linear\")\n\n# Predict on test data\nsvm_preds &lt;- predict(svm_model, newdata = test_prepped)\n\n# Evaluate performance\nsvm_metrics &lt;- confusionMatrix(factor(svm_preds, levels = levels(test_data$churn)), test_churn)\nprint(svm_metrics)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  yes   no\n       yes    0    0\n       no   206 1294\n                                          \n               Accuracy : 0.8627          \n                 95% CI : (0.8442, 0.8797)\n    No Information Rate : 0.8627          \n    P-Value [Acc &gt; NIR] : 0.5186          \n                                          \n                  Kappa : 0               \n                                          \n Mcnemar's Test P-Value : &lt;2e-16          \n                                          \n            Sensitivity : 0.0000          \n            Specificity : 1.0000          \n         Pos Pred Value :    NaN          \n         Neg Pred Value : 0.8627          \n             Prevalence : 0.1373          \n         Detection Rate : 0.0000          \n   Detection Prevalence : 0.0000          \n      Balanced Accuracy : 0.5000          \n                                          \n       'Positive' Class : yes             \n                                          \n\n\nSVM Model:\n\nAccuracy: 0.8627\nKappa : 0\nMcnemar’s Test P-Value : &lt; 2e-16\nBalanced Accuracy : 0.5000\n\nConclusion: The Random Forest performs the best with the highest accuracy and Kappa, and balance accuracy. Additionally, the Decision Tree performs well but not like the Random Forest model, and the svm model performed the worst failing to detect any churned customers.\nUse the fatty acid data from Homework 1 Problem 3 above.\n\nUse the same data splitting approach (if any) and pre-processing steps that you did in Homework 1 Problem 3.\n\n\n# Re-load the data if needed\ndata(oil)\noil_data &lt;- as.data.frame(fattyAcids)\noil_data$oilType &lt;- oilType\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Split the data using stratified sampling\ntrain_index &lt;- createDataPartition(oil_data$oilType, p = 0.7, list = FALSE)\ntrain_data &lt;- oil_data[train_index, ]\ntest_data &lt;- oil_data[-train_index, ]\n\n# Pre-process the data: Centering and Scaling\npreProcValues &lt;- preProcess(train_data[,-ncol(train_data)], method = c(\"center\", \"scale\"))\ntrain_data[,-ncol(train_data)] &lt;- predict(preProcValues, train_data[,-ncol(train_data)])\ntest_data[,-ncol(test_data)] &lt;- predict(preProcValues, test_data[,-ncol(test_data)])\n\n\nFit a few basic trees to the training set.\n\nDecision tree model:\n\n# Fit tree model\nset.seed(123)\ntree_model &lt;- train(oilType ~ ., data = train_data, method = \"rpart\",\n                    trControl = trainControl(method = \"cv\", number = 10))\n\n# Predict on the test set\ntest_predictions &lt;- predict(tree_model, newdata = test_data)\n\n# Confusion matrix\nconf_matrix &lt;- confusionMatrix(test_predictions, test_data$oilType)\nprint(conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  A  B  C  D  E  F  G\n         A 10  0  0  0  0  0  0\n         B  0  7  0  0  0  0  0\n         C  0  0  0  0  0  0  0\n         D  0  0  0  0  0  0  0\n         E  1  0  0  2  3  3  0\n         F  0  0  0  0  0  0  0\n         G  0  0  0  0  0  0  0\n\nOverall Statistics\n                                          \n               Accuracy : 0.7692          \n                 95% CI : (0.5635, 0.9103)\n    No Information Rate : 0.4231          \n    P-Value [Acc &gt; NIR] : 0.000358        \n                                          \n                  Kappa : 0.6816          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: A Class: B Class: C Class: D Class: E Class: F\nSensitivity            0.9091   1.0000       NA  0.00000   1.0000   0.0000\nSpecificity            1.0000   1.0000        1  1.00000   0.7391   1.0000\nPos Pred Value         1.0000   1.0000       NA      NaN   0.3333      NaN\nNeg Pred Value         0.9375   1.0000       NA  0.92308   1.0000   0.8846\nPrevalence             0.4231   0.2692        0  0.07692   0.1154   0.1154\nDetection Rate         0.3846   0.2692        0  0.00000   0.1154   0.0000\nDetection Prevalence   0.3846   0.2692        0  0.00000   0.3462   0.0000\nBalanced Accuracy      0.9545   1.0000       NA  0.50000   0.8696   0.5000\n                     Class: G\nSensitivity                NA\nSpecificity                 1\nPos Pred Value             NA\nNeg Pred Value             NA\nPrevalence                  0\nDetection Rate              0\nDetection Prevalence        0\nBalanced Accuracy          NA\n\n\ndecision tree model:\n\nAccuracy : 0.7692\nKappa : 0.6816\nP-Value: 0.000358\n\n\nDoes bagging improve the performance of the trees? What about boosting?\n\nBagging:\n\n# Fit a bagged decision tree model\nset.seed(123)\nbagged_tree_model &lt;- train(oilType ~ ., data = train_data, method = \"treebag\",\n                           trControl = trainControl(method = \"cv\", number = 10))\n\n# Predict on the test set\nbagged_predictions &lt;- predict(bagged_tree_model, newdata = test_data)\n\n# Confusion matrix\nbagged_conf_matrix &lt;- confusionMatrix(bagged_predictions, test_data$oilType)\nprint(bagged_conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  A  B  C  D  E  F  G\n         A 10  0  0  0  0  0  0\n         B  0  7  0  0  0  0  0\n         C  0  0  0  0  0  0  0\n         D  0  0  0  2  0  0  0\n         E  1  0  0  0  3  0  0\n         F  0  0  0  0  0  3  0\n         G  0  0  0  0  0  0  0\n\nOverall Statistics\n                                         \n               Accuracy : 0.9615         \n                 95% CI : (0.8036, 0.999)\n    No Information Rate : 0.4231         \n    P-Value [Acc &gt; NIR] : 7.058e-09      \n                                         \n                  Kappa : 0.9472         \n                                         \n Mcnemar's Test P-Value : NA             \n\nStatistics by Class:\n\n                     Class: A Class: B Class: C Class: D Class: E Class: F\nSensitivity            0.9091   1.0000       NA  1.00000   1.0000   1.0000\nSpecificity            1.0000   1.0000        1  1.00000   0.9565   1.0000\nPos Pred Value         1.0000   1.0000       NA  1.00000   0.7500   1.0000\nNeg Pred Value         0.9375   1.0000       NA  1.00000   1.0000   1.0000\nPrevalence             0.4231   0.2692        0  0.07692   0.1154   0.1154\nDetection Rate         0.3846   0.2692        0  0.07692   0.1154   0.1154\nDetection Prevalence   0.3846   0.2692        0  0.07692   0.1538   0.1154\nBalanced Accuracy      0.9545   1.0000       NA  1.00000   0.9783   1.0000\n                     Class: G\nSensitivity                NA\nSpecificity                 1\nPos Pred Value             NA\nNeg Pred Value             NA\nPrevalence                  0\nDetection Rate              0\nDetection Prevalence        0\nBalanced Accuracy          NA\n\n\nBagging (decision tree mode):\n\nAccuracy : 0.9615\nKappa : 0.9472\nP-Value: 7.058e-09\n\nBoosting:\n\n# Fit a boosted decision tree model\nset.seed(123)\nboosted_tree_model &lt;- train(oilType ~ ., data = train_data, method = \"gbm\",\n                            trControl = trainControl(method = \"cv\", number = 10),\n                            verbose = FALSE)\n\n# Predict on the test set\nboosted_predictions &lt;- predict(boosted_tree_model, newdata = test_data)\n\n# Confusion matrix\nboosted_conf_matrix &lt;- confusionMatrix(boosted_predictions, test_data$oilType)\nprint(boosted_conf_matrix)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  A  B  C  D  E  F  G\n         A 11  0  0  0  0  0  0\n         B  0  7  0  0  0  0  0\n         C  0  0  0  0  0  0  0\n         D  0  0  0  2  0  0  0\n         E  0  0  0  0  3  0  0\n         F  0  0  0  0  0  3  0\n         G  0  0  0  0  0  0  0\n\nOverall Statistics\n                                     \n               Accuracy : 1          \n                 95% CI : (0.8677, 1)\n    No Information Rate : 0.4231     \n    P-Value [Acc &gt; NIR] : 1.936e-10  \n                                     \n                  Kappa : 1          \n                                     \n Mcnemar's Test P-Value : NA         \n\nStatistics by Class:\n\n                     Class: A Class: B Class: C Class: D Class: E Class: F\nSensitivity            1.0000   1.0000       NA  1.00000   1.0000   1.0000\nSpecificity            1.0000   1.0000        1  1.00000   1.0000   1.0000\nPos Pred Value         1.0000   1.0000       NA  1.00000   1.0000   1.0000\nNeg Pred Value         1.0000   1.0000       NA  1.00000   1.0000   1.0000\nPrevalence             0.4231   0.2692        0  0.07692   0.1154   0.1154\nDetection Rate         0.4231   0.2692        0  0.07692   0.1154   0.1154\nDetection Prevalence   0.4231   0.2692        0  0.07692   0.1154   0.1154\nBalanced Accuracy      1.0000   1.0000       NA  1.00000   1.0000   1.0000\n                     Class: G\nSensitivity                NA\nSpecificity                 1\nPos Pred Value             NA\nNeg Pred Value             NA\nPrevalence                  0\nDetection Rate              0\nDetection Prevalence        0\nBalanced Accuracy          NA\n\n\nBoosting (decision tree mode):\n\nAccuracy : 1\nKappa : 1\nP-Value: 1.936e-10\n\nIn conclusion, both bagging and boosting, enchance the decision tree from the basic tree model, you can observe that the predicting class levels have drastically improve. The accuracy and Kappa have also improved from the basic decision tree to bagging and then to boosting which resulted in accuracy of 100% and Kappa of 100%. The specific classes accuracy have also increase with Class A being the best and Class C and G being the worst..\nSide note: All classes (A,B,D,E,F) had an accuracy of 100%.\n\nWhich model has better performance, and what are the corresponding tuning parameters?\n\n\nThe Boosted decision tree performs the best resulting in perfect accuracy and Kappa.\n\n\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nKappa\nNo Information Rate\nP-Value\n\n\n\n\nDecision Tree\n0.7692\n0.6816\n0.4231\n0.000358\n\n\nBagged Decision Tree\n0.9615\n0.9472\n0.4231\n7.058e-09\n\n\nBoosted Decision Tree\n1.0000\n1.0000\n0.4231\n1.936e-10\n\n\n\n\n\nNow for the corresponding tuning parameter, they are as follows:\n\n\n# Print the best tuning parameters\nboosted_tree_model$bestTune\n\n  n.trees interaction.depth shrinkage n.minobsinnode\n1      50                 1       0.1             10\n\n\n\n\n\nParameter\nValue\n\n\n\n\nn.trees\n50\n\n\ninteraction.depth\n1\n\n\nshrinkage\n0.1\n\n\nn.minobsinnode\n10"
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/Joaquin/School/DA - 6833 (Summer 2024)/Joaquin_Ramriez_Portfolio_II\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.3.3\n\nlibrary(here) #to set paths\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/Joaquin/School/DA - 6833 (Summer 2024)/Joaquin_Ramriez_Portfolio_II\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Joaquin Ramirez",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "This report analyzes the American Idol dataset to explore gender representation among finalists and winners and to investigate any trends or patterns over different seasons. The dataset includes information on eliminations, finalists, seasons, and can be found - Tidy Tuesday Exercise.\nQuestions: Who is more likely to win the next seasons, a male or a female?"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#introduction",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#introduction",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "This report analyzes the American Idol dataset to explore gender representation among finalists and winners and to investigate any trends or patterns over different seasons. The dataset includes information on eliminations, finalists, seasons, and can be found - Tidy Tuesday Exercise.\nQuestions: Who is more likely to win the next seasons, a male or a female?"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#load-libraries-and-data",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#load-libraries-and-data",
    "title": "Tidy Tuesday Exercise",
    "section": "Load Libraries and Data",
    "text": "Load Libraries and Data\n\n# Load necessary libraries\nlibrary(rlang)\n\nWarning: package 'rlang' was built under R version 4.3.3\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ purrr::%@%()         masks rlang::%@%()\n✖ dplyr::filter()      masks stats::filter()\n✖ purrr::flatten()     masks rlang::flatten()\n✖ purrr::flatten_chr() masks rlang::flatten_chr()\n✖ purrr::flatten_dbl() masks rlang::flatten_dbl()\n✖ purrr::flatten_int() masks rlang::flatten_int()\n✖ purrr::flatten_lgl() masks rlang::flatten_lgl()\n✖ purrr::flatten_raw() masks rlang::flatten_raw()\n✖ purrr::invoke()      masks rlang::invoke()\n✖ dplyr::lag()         masks stats::lag()\n✖ purrr::splice()      masks rlang::splice()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.3.3\n\n\nhere() starts at C:/Users/Joaquin/School/DA - 6833 (Summer 2024)/Joaquin_Ramriez_Portfolio_II\n\nlibrary(janitor)\n\nWarning: package 'janitor' was built under R version 4.3.3\n\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(gt)\nlibrary(tidymodels)\n\nWarning: package 'tidymodels' was built under R version 4.3.3\n\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.5      ✔ rsample      1.2.1 \n✔ dials        1.2.1      ✔ tune         1.2.1 \n✔ infer        1.0.7      ✔ workflows    1.1.4 \n✔ modeldata    1.4.0      ✔ workflowsets 1.1.0 \n✔ parsnip      1.2.1      ✔ yardstick    1.3.1 \n✔ recipes      1.0.10     \n\n\nWarning: package 'dials' was built under R version 4.3.3\n\n\nWarning: package 'scales' was built under R version 4.3.3\n\n\nWarning: package 'infer' was built under R version 4.3.3\n\n\nWarning: package 'modeldata' was built under R version 4.3.3\n\n\nWarning: package 'parsnip' was built under R version 4.3.3\n\n\nWarning: package 'recipes' was built under R version 4.3.3\n\n\nWarning: package 'rsample' was built under R version 4.3.3\n\n\nWarning: package 'tune' was built under R version 4.3.3\n\n\nWarning: package 'workflows' was built under R version 4.3.3\n\n\nWarning: package 'workflowsets' was built under R version 4.3.3\n\n\nWarning: package 'yardstick' was built under R version 4.3.3\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ purrr::%@%()         masks rlang::%@%()\n✖ scales::discard()    masks purrr::discard()\n✖ dplyr::filter()      masks stats::filter()\n✖ recipes::fixed()     masks stringr::fixed()\n✖ purrr::flatten()     masks rlang::flatten()\n✖ purrr::flatten_chr() masks rlang::flatten_chr()\n✖ purrr::flatten_dbl() masks rlang::flatten_dbl()\n✖ purrr::flatten_int() masks rlang::flatten_int()\n✖ purrr::flatten_lgl() masks rlang::flatten_lgl()\n✖ purrr::flatten_raw() masks rlang::flatten_raw()\n✖ purrr::invoke()      masks rlang::invoke()\n✖ dplyr::lag()         masks stats::lag()\n✖ yardstick::spec()    masks readr::spec()\n✖ purrr::splice()      masks rlang::splice()\n✖ recipes::step()      masks stats::step()\n• Learn how to get started at https://www.tidymodels.org/start/\n\nlibrary(rsample)  # For data splitting\nlibrary(caret)    # For model training\n\nWarning: package 'caret' was built under R version 4.3.3\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(glmnet)   # For Ridge and Elastic Net\n\nWarning: package 'glmnet' was built under R version 4.3.3\n\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(nnet)     # For Neural Network\nlibrary(earth)    # For MARS\n\nWarning: package 'earth' was built under R version 4.3.3\n\n\nLoading required package: Formula\nLoading required package: plotmo\n\n\nWarning: package 'plotmo' was built under R version 4.3.3\n\n\nLoading required package: plotrix\n\n\nWarning: package 'plotrix' was built under R version 4.3.2\n\n\n\nAttaching package: 'plotrix'\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\nlibrary(e1071)    # For SVM\n\n\nAttaching package: 'e1071'\n\nThe following object is masked from 'package:tune':\n\n    tune\n\nThe following object is masked from 'package:rsample':\n\n    permutations\n\nThe following object is masked from 'package:parsnip':\n\n    tune\n\nlibrary(pROC)     # For evaluation metrics\n\nWarning: package 'pROC' was built under R version 4.3.3\n\n\nType 'citation(\"pROC\")' for a citation.\n\nAttaching package: 'pROC'\n\nThe following objects are masked from 'package:stats':\n\n    cov, smooth, var\n\n\n\n# Define file paths and load data\neliminations &lt;- read_csv(here::here(\"tidytuesday-exercise\", \"eliminations.csv\"))\n\nRows: 456 Columns: 46\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (44): place, gender, contestant, top_36, top_36_2, top_36_3, top_36_4, t...\ndbl  (1): season\nlgl  (1): comeback\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfinalists &lt;- read_csv(here::here(\"tidytuesday-exercise\", \"finalists.csv\"))\n\nRows: 190 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): Contestant, Birthday, Birthplace, Hometown, Description, Contestant...\ndbl (1): Season\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nseasons &lt;- read_csv(here::here(\"tidytuesday-exercise\", \"seasons.csv\"))\n\nRows: 18 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (10): winner, runner_up, original_release, original_network, hosted_by, ...\ndbl  (2): season, no_of_episodes\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Clean column names\neliminations &lt;- clean_names(eliminations)\nfinalists &lt;- clean_names(finalists)\nseasons &lt;- clean_names(seasons)\n\n# Display the first few rows and summary of each dataset\nhead(eliminations)\n\n# A tibble: 6 × 46\n  season place gender contestant        top_36 top_36_2 top_36_3 top_36_4 top_32\n   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;             &lt;chr&gt;  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt; \n1      1 1     Female Kelly Clarkson    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n2      1 2     Male   Justin Guarini    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n3      1 3     Female Nikki McKibbin    &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n4      1 4     Female Tamyra Gray       &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n5      1 5     Male   R. J. Helton      &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n6      1 6     Female Christina Christ… &lt;NA&gt;   &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;  \n# ℹ 37 more variables: top_32_2 &lt;chr&gt;, top_32_3 &lt;chr&gt;, top_32_4 &lt;chr&gt;,\n#   top_30 &lt;chr&gt;, top_30_2 &lt;chr&gt;, top_30_3 &lt;chr&gt;, top_25 &lt;chr&gt;, top_25_2 &lt;chr&gt;,\n#   top_25_3 &lt;chr&gt;, top_24 &lt;chr&gt;, top_24_2 &lt;chr&gt;, top_24_3 &lt;chr&gt;, top_20 &lt;chr&gt;,\n#   top_20_2 &lt;chr&gt;, top_16 &lt;chr&gt;, top_14 &lt;chr&gt;, top_13 &lt;chr&gt;, top_12 &lt;chr&gt;,\n#   top_11 &lt;chr&gt;, top_11_2 &lt;chr&gt;, wildcard &lt;chr&gt;, comeback &lt;lgl&gt;, top_10 &lt;chr&gt;,\n#   top_9 &lt;chr&gt;, top_9_2 &lt;chr&gt;, top_8 &lt;chr&gt;, top_8_2 &lt;chr&gt;, top_7 &lt;chr&gt;,\n#   top_7_2 &lt;chr&gt;, top_6 &lt;chr&gt;, top_6_2 &lt;chr&gt;, top_5 &lt;chr&gt;, top_5_2 &lt;chr&gt;, …\n\nhead(finalists)\n\n# A tibble: 6 × 7\n  contestant   birthday birthplace hometown description season contestant_gender\n  &lt;chr&gt;        &lt;chr&gt;    &lt;chr&gt;      &lt;chr&gt;    &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;            \n1 Kelly Clark… 24-Apr-… Fort Wort… Burleso… \"She perfo…      1 Female           \n2 Justin Guar… 28-Oct-… Columbus,… Doylest… \"He perfor…      1 Male             \n3 Nikki McKib… 28-Sep-… Grand Pra… &lt;NA&gt;     \"She had p…      1 Female           \n4 Tamyra Gray  26-Jul-… Takoma Pa… Atlanta… \"She had a…      1 Female           \n5 R. J. Helton 17-May-… Pasadena,… Cumming… \"J. Helton…      1 Male             \n6 Christina C… 21-Jun-… Brooklyn,… &lt;NA&gt;     \".Christin…      1 Female           \n\nhead(seasons)\n\n# A tibble: 6 × 12\n  season winner     runner_up original_release original_network hosted_by judges\n   &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt; \n1      1 Kelly Cla… Justin G… June 11 (2002-0… Fox              Ryan Sea… Paula…\n2      2 Ruben Stu… Clay Aik… January 21 (200… Fox              Ryan Sea… Paula…\n3      3 Fantasia … Diana De… January 19 (200… Fox              Ryan Sea… Paula…\n4      4 Carrie Un… Bo Bice   January 18 (200… Fox              Ryan Sea… Paula…\n5      5 Taylor Hi… Katharin… January 17 (200… Fox              Ryan Sea… Paula…\n6      6 Jordin Sp… Blake Le… January 16 (200… Fox              Ryan Sea… Paula…\n# ℹ 5 more variables: no_of_episodes &lt;dbl&gt;, finals_venue &lt;chr&gt;, mentor &lt;chr&gt;,\n#   winner_gender &lt;chr&gt;, runner_up_winner &lt;chr&gt;\n\n# In case these files need to be read.\n#songs &lt;- read_csv(here::here(\"data\", \"songs.csv\"))\n#auditions &lt;- read_csv(here::here(\"data\", \"auditions.csv\"))\n#ratings &lt;- read_csv(here::here(\"data\", \"ratings.csv\"))"
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#eliminations-analysis",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#eliminations-analysis",
    "title": "Tidy Tuesday Exercise",
    "section": "Eliminations Analysis",
    "text": "Eliminations Analysis\n\n# Calculate gender ratios for eliminations by season\neliminations_gender_ratio &lt;- eliminations %&gt;%\n  group_by(season) %&gt;%\n  summarize(\n    male_eliminations = sum(gender == \"Male\", na.rm = TRUE),\n    female_eliminations = sum(gender == \"Female\", na.rm = TRUE)\n  ) %&gt;%\n  mutate(gender_ratio = male_eliminations / (female_eliminations + 1))  # Adding 1 to avoid division by zero\n\n# Display the gender ratio\nprint(eliminations_gender_ratio)\n\n# A tibble: 18 × 4\n   season male_eliminations female_eliminations gender_ratio\n    &lt;dbl&gt;             &lt;int&gt;               &lt;int&gt;        &lt;dbl&gt;\n 1      1                14                  16        0.824\n 2      2                15                  21        0.682\n 3      3                12                  20        0.571\n 4      4                12                  12        0.923\n 5      5                12                  12        0.923\n 6      6                12                  12        0.923\n 7      7                12                  12        0.923\n 8      8                18                  18        0.947\n 9      9                12                  12        0.923\n10     10                12                  12        0.923\n11     11                13                  12        1    \n12     12                10                  10        0.909\n13     13                10                  10        0.909\n14     14                12                  12        0.923\n15     15                11                  13        0.786\n16     16                12                  12        0.923\n17     17                11                   9        1.1  \n18     18                 9                  11        0.75 \n\n# Plot the number of male and female eliminations over seasons\nggplot(eliminations_gender_ratio, aes(x = season)) +\n  geom_line(aes(y = male_eliminations, color = \"Male\"), size = 1.2) +  # Make lines bolder\n  geom_line(aes(y = female_eliminations, color = \"Female\"), size = 1.2) +  # Make lines bolder\n  geom_point(aes(y = male_eliminations, color = \"Male\"), size = 3) +  # Adjust point size\n  geom_point(aes(y = female_eliminations, color = \"Female\"), size = 3) +  # Adjust point size\n  labs(\n    title = \"Number of Male and Female Eliminations Over Seasons\",\n    x = \"Season\",\n    y = \"Number of Eliminations\",\n    color = \"Gender\"  # Customize legend title\n  ) +\n  scale_color_manual(values = c(\"Male\" = \"blue\", \"Female\" = \"pink\")) +\n  scale_x_continuous(limits = c(0, 20)) +  # Set x-axis limits\n  scale_y_continuous(limits = c(0, 25)) +  # Set y-axis limits\n  theme_minimal(base_size = 14) +  # Use minimal theme with larger base size\n  theme(\n    legend.position = \"top\",  # Position the legend at the top\n    legend.title = element_text(face = \"bold\"),  # Bold the legend title\n    legend.text = element_text(size = 12),  # Adjust legend text size\n    axis.title.x = element_text(face = \"bold\"),  # Bold x-axis title\n    axis.title.y = element_text(face = \"bold\"),  # Bold y-axis title\n    axis.text = element_text(size = 12),  # Adjust axis text size\n    panel.grid.major = element_line(size = 0.5, linetype = \"solid\"),  # Adjust grid line size\n    panel.grid.minor = element_line(size = 0.25, linetype = \"dashed\")  # Adjust grid line size\n  )\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\nConclusion:\nThe elimination data indicates a generally balanced gender ratio across most seasons, with some variability. This suggests that, in terms of eliminations, there is no strong evidence that one gender is systematically favored or disadvantaged."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#winners",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#winners",
    "title": "Tidy Tuesday Exercise",
    "section": "Winners",
    "text": "Winners\n\n# Aggregate the count of male and female winners\nwinner_counts &lt;- seasons %&gt;%\n  group_by(winner_gender) %&gt;%\n  summarize(count = n(), .groups = 'drop')\n\n# Print the winner counts to verify\nprint(winner_counts)\n\n# A tibble: 2 × 2\n  winner_gender count\n  &lt;chr&gt;         &lt;int&gt;\n1 Female            7\n2 Male             11\n\n# Create a bar graph of the number of winners by gender\nggplot(winner_counts, aes(x = winner_gender, y = count, fill = winner_gender)) +\n  geom_bar(stat = \"identity\") +\n  labs(\n    title = \"American Idol: Male & Female Winners\",\n    x = \"Gender\",\n    y = \"# of winner\",\n    fill = \"Gender\"  # Change legend title to \"Gender\"\n  ) +\n  scale_fill_manual(values = c(\"Male\" = \"blue\", \"Female\" = \"pink\")) +\n  theme_minimal(base_size = 14) +  # Use minimal theme with larger base size\n  theme(\n    legend.position = \"top\",  # Position the legend at the top\n    legend.title = element_text(face = \"bold\"),  # Bold the legend title\n    legend.text = element_text(size = 12),  # Adjust legend text size\n    axis.title.x = element_text(face = \"bold\"),  # Bold x-axis title\n    axis.title.y = element_text(face = \"bold\"),  # Bold y-axis title\n    axis.text = element_text(size = 12)  # Adjust axis text size\n  )\n\n\n\n\nConclusion:\nThe American Idol data reveals that out of the 18 seasons analyzed, 7 winners were female and 11 winners were male. This suggests that males have historically been more likely to win the competition compared to females."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#contestants",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#contestants",
    "title": "Tidy Tuesday Exercise",
    "section": "Contestants",
    "text": "Contestants\n\n# Count the number of contestants by season and gender\ncontestants_by_gender &lt;- finalists %&gt;%\n  group_by(season, contestant_gender) %&gt;%\n  summarize(count = n(), .groups = 'drop')\n\n# Display the summarized data\nprint(contestants_by_gender)\n\n# A tibble: 37 × 3\n   season contestant_gender count\n    &lt;dbl&gt; &lt;chr&gt;             &lt;int&gt;\n 1      1 Female                5\n 2      1 Male                  5\n 3      2 Female                6\n 4      2 Male                  6\n 5      3 Female                8\n 6      3 Male                  4\n 7      4 Female                6\n 8      4 Male                  6\n 9      5 Female                5\n10      5 Male                  3\n# ℹ 27 more rows\n\n# Create a scatter plot of the number of contestants by gender and season\nggplot(contestants_by_gender, aes(x = season, y = count, color = contestant_gender, shape = contestant_gender)) +\n  geom_point(size = 3) +\n  labs(title = \"American Idol Contestants: Gender\",\n       x = \"Season\",\n       y = \"Number of Contestants\",\n       color = \"Gender\",\n       shape = \"Gender\") +\n  scale_color_manual(values = c(\"Male\" = \"blue\", \"Female\" = \"red\")) +\n  theme_minimal()\n\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\nObservations:\n\nAcross the 17 seasons, there were several seasons with a balanced number of male and female contestants.\nSome seasons showed a slight imbalance, such as Season 3 with more females (8) than males (4) and Season 8 with more males (7) than females (3)."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#model-fitting",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#model-fitting",
    "title": "Tidy Tuesday Exercise",
    "section": "Model Fitting",
    "text": "Model Fitting\n\n# Load necessary libraries\nlibrary(tidymodels)\nlibrary(dplyr)\n\n# Prepare data for modeling\nseasons_clean &lt;- seasons %&gt;%\n  mutate(\n    winner_gender = as.factor(winner_gender),\n    original_network = as.factor(original_network),\n    hosted_by = as.factor(hosted_by),\n    judges = as.factor(judges),\n    finals_venue = as.factor(finals_venue),\n    mentor = as.factor(mentor))\n\n\n# Split data into training and testing sets\nset.seed(123)  # For reproducibility\ndata_split &lt;- initial_split(seasons_clean, prop = 0.8, strata = winner_gender)\ntrain_data &lt;- training(data_split)\ntest_data &lt;- testing(data_split)\n\n# Create a recipe for preprocessing\nrecipe &lt;- recipe(winner_gender ~ season + original_network + hosted_by + judges + no_of_episodes + finals_venue + mentor, data = train_data) %&gt;%\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%  # Convert categorical variables to dummy variables\n  step_zv(all_predictors()) %&gt;%  # Remove zero variance predictors\n  step_impute_median(all_predictors()) %&gt;%  # Impute missing values\n  step_scale(all_predictors()) %&gt;%  # Scale predictors\n  step_center(all_predictors())     # Center predictors\n\n\n# Define model specifications\nlog_reg_spec &lt;- logistic_reg() %&gt;%\n  set_engine(\"glm\")\n\nrf_spec &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"classification\")\n\nsvm_spec &lt;- svm_rbf() %&gt;%\n  set_engine(\"kernlab\") %&gt;%\n  set_mode(\"classification\")\n\n\n# Create workflows\nlog_reg_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(log_reg_spec)\n\nrf_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(rf_spec)\n\nsvm_workflow &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(svm_spec)\n\n\n# Fit models with cross-validation\ncv_folds &lt;- vfold_cv(train_data, v = 5, strata = winner_gender)\n\nlog_reg_fit &lt;- log_reg_workflow %&gt;%\n  fit_resamples(cv_folds)\n\n→ A | warning: ! There are new levels in a factor: `NA`.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: ! There are new levels in a factor: `NA`., prediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases\n\n\nThere were issues with some computations   A: x1\n→ C | warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x5   B: x5   C: x2\n\nrf_fit &lt;- rf_workflow %&gt;%\n  fit_resamples(cv_folds)\n\nWarning: package 'ranger' was built under R version 4.3.3\n\n\n→ A | warning: ! There are new levels in a factor: `NA`.\nThere were issues with some computations   A: x3\nThere were issues with some computations   A: x10\n\nsvm_fit &lt;- svm_workflow %&gt;%\n  fit_resamples(cv_folds)\n\n→ A | warning: ! There are new levels in a factor: `NA`.\nThere were issues with some computations   A: x2\nThere were issues with some computations   A: x10\n\n# Collect and print metrics\nlog_reg_metrics &lt;- log_reg_fit %&gt;% collect_metrics()\nrf_metrics &lt;- rf_fit %&gt;% collect_metrics()\nsvm_metrics &lt;- svm_fit %&gt;% collect_metrics()\n\n\nprint(log_reg_metrics)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.6       5   0.113 Preprocessor1_Model1\n2 brier_class binary     0.374     5   0.108 Preprocessor1_Model1\n3 roc_auc     binary     0.55      5   0.2   Preprocessor1_Model1\n\n\n\nprint(rf_metrics)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.6       5 0.0408  Preprocessor1_Model1\n2 brier_class binary     0.229     5 0.00545 Preprocessor1_Model1\n3 roc_auc     binary     0.7       5 0.122   Preprocessor1_Model1\n\n\n\nprint(svm_metrics)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.533     5  0.122  Preprocessor1_Model1\n2 brier_class binary     0.315     5  0.0161 Preprocessor1_Model1\n3 roc_auc     binary     0.1       5  0.1    Preprocessor1_Model1\n\n\n\n# Select the best model (e.g., Random Forest)\nfinal_rf_model &lt;- rf_workflow %&gt;%\n  fit(train_data)\n\nWarning: ! There are new levels in a factor: `NA`.\n! There are new levels in a factor: `NA`.\n\n# Predict on the test set\npredictions &lt;- predict(final_rf_model, test_data) %&gt;%\n  bind_cols(test_data)  # Merge predictions with test_data\n\nWarning: ! There are new levels in a factor: `NA`.\n\n# Rename the prediction column to '.pred_class' for consistency\npredictions &lt;- predictions %&gt;%\n  rename(.pred_class = .pred_class)\n\n\n# Evaluate predictions\nmetrics_test &lt;- metrics(predictions, truth = winner_gender, estimate = .pred_class)\nprint(metrics_test)\n\n# A tibble: 2 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.8  \n2 kap      binary         0.545\n\n\nConclusion:\nRandom Forest (RF):\n\nAccuracy: 60% is the same as Logistic Regression, but RF has a lower Brier score (better) and a higher ROC AUC, indicating it has better performance in distinguishing between classes compared to Logistic Regression.\nBrier Class Score: The lower score means RF provides better probabilistic predictions than Logistic Regression.\nROC AUC: RF’s higher ROC AUC (0.70) shows better overall performance in class separation.\n\nLogistic Regression (LogReg):\n\nAccuracy: Matches Random Forest, but with a higher Brier score and lower ROC AUC.\nBrier Class Score: Higher than RF, indicating less reliable probability estimates.\nROC AUC: Lower than RF, showing poorer performance in distinguishing between classes.\n\nSupport Vector Machine (SVM):\n\nAccuracy: Lowest among the models at 53.33%.\nBrier Class Score: Higher than RF and LogReg, indicating less reliable probability estimates.\nROC AUC: Very low at 0.10, suggesting that SVM performs poorly in distinguishing between classes.\n\nIn other words the best model is the Random Forest which seems to be the most effective model overall, with the highest ROC, AUC and lower Brier score, showing strong performance on the test set with an accuracy of 80% and a moderate Kappa score."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html#summary",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html#summary",
    "title": "Tidy Tuesday Exercise",
    "section": "Summary:",
    "text": "Summary:\nThis analysis has shown that males have historically been more likely to win American Idol, with Random Forest emerging as the most accurate model for predicting winners based on the available data. The gender ratio among eliminations and finalists does not strongly favor either gender, but the winning trend shows a slight male advantage.\nFuture Work\nFuture research could delve into other factors such as judging patterns, audience reactions, and additional contestant characteristics to further understand their impact on the likelihood of winning."
  }
]